{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the path to the training and test directories on your device here\n",
    "TRAINING_PATH = \"mediaeval-2015-trainingset.txt\"\n",
    "TESTING_PATH = \"mediaeval-2015-testset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_training = pd.read_csv(TRAINING_PATH, delimiter = \"\\t\")\n",
    "original_testing = pd.read_csv(TESTING_PATH, delimiter = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns apart from the text and the label as none of the other data appears to be useful\n",
    "original_training = original_training.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "#Do the same for the testing set\n",
    "original_testing = original_testing.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column to store the language, initially empty before langdetect populates it\n",
    "original_training[\"lang\"] = np.nan\n",
    "original_testing[\"lang\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_training = original_training.sample(frac = 1)\n",
    "original_testing = original_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Responsible for parsing tweets\n",
    "class TweetHandler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        snowball_langs = list(SnowballStemmer.languages)\n",
    "        #some languages are supported by stemming but NOT supported by language specific tokenizing,\n",
    "        #only the tokens that are in this set are supported by language specific tokenizing\n",
    "        self.tokenizer_langs = {\"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"it\", \"pt\", \"ru\", \"es\", \"sv\"}\n",
    "        langdetect_langs = [\"ar\", \"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"hu\", \"it\", \"no\", \"illegal\", \"pt\", \"ro\", \"ru\", \"es\", \"sv\"]\n",
    "        #a dictionary to map the corresponding snowball and langdetect properties\n",
    "        self.lang_dict = dict(zip(langdetect_langs, snowball_langs))\n",
    "        #declare some custom stop words\n",
    "        self.custom_stops = [\"http\",\"nhttp\",\"https\"]\n",
    "\n",
    "    #takes a tweet, detects its language, removes any stop words in the language, tokenizes and stems\n",
    "    #specific to the detected language and returns the simplified tokens paired with the language\n",
    "    def parse_tweet(self, tweet):\n",
    "        \n",
    "        try:\n",
    "            lang_prediction = detect(tweet)\n",
    "            #the nltk name for the predicted language\n",
    "            nltkprop = self.lang_dict[lang_prediction]\n",
    "        except:\n",
    "            #assume english stopwords and stemming if the language cannot be detected\n",
    "            lang_prediction = \"unknown\"\n",
    "            nltkprop = \"english\"\n",
    "            \n",
    "        # if the language is not supported by the tokenizer (including unkown) then assume tokenizing in English, however stemming\n",
    "        # and stopwords may still be supported in the language that does not support language specific tokenization\n",
    "        # e.g. arabic, hungarian, romanian so tokenize with the english\n",
    "        # version of the algorithm if this is the case and use the stemming and stopwords specific to \n",
    "        # the language if this is available even if the tokenization algorithm isnt\n",
    "        # use a python ternary expression to do this\n",
    "        tokens = word_tokenize(tweet, language = nltkprop if lang_prediction in self.tokenizer_langs else \"english\")\n",
    "        \n",
    "        #stop words specific to the language\n",
    "        stop_words = set(stopwords.words(nltkprop))\n",
    "        \n",
    "        #stemming algorithm specific to the language detected\n",
    "        stemmer = SnowballStemmer(nltkprop)\n",
    "        \n",
    "        # store all tokens to be output as a concatenated string here so that this string\n",
    "        # can later be fed to a CountVectorizer or TfIDFVectorizer , filter out any unwanted tokens \n",
    "        # and don't add them \n",
    "        filtered_tokens = \"\"\n",
    "        \n",
    "        for tok in tokens:\n",
    "            \n",
    "            #remove any hashtags\n",
    "            if tok[0] == '#':\n",
    "                tok = tok[1:]\n",
    "                \n",
    "            #discard non alphanumeric strings containing symbols or pure digits, or stop words\n",
    "            if (not tok.isalnum()) or tok.isdigit() or (tok in stop_words) or tok in self.custom_stops:\n",
    "                continue;\n",
    "            \n",
    "            #carry out stemming specific to the language detected\n",
    "            filtered_tokens += \" \" + stemmer.stem(tok)\n",
    "        \n",
    "        return filtered_tokens, lang_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataset from a dataset of tweets into a dataset of labelled tokens in concatenated\n",
    "# string form, along with the detected language\n",
    "\n",
    "def transform_data(arg):\n",
    "\n",
    "    #copy the argument given so we don't change the original instance and can keep it in memory and reuse it \n",
    "    #if necessary\n",
    "    dataset = deepcopy(arg)\n",
    "    \n",
    "    th = TweetHandler()\n",
    "    num_rows = dataset.label.size\n",
    "    \n",
    "    #the tweet text will be transformed into tokens so rename the column appropriately\n",
    "    dataset = dataset.rename(columns = {\"tweetText\" : \"tokens\"})\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "\n",
    "        tweet = dataset.tokens[i]\n",
    "        label = dataset.label[i]\n",
    "\n",
    "        #disregard the humour information for now, map humor and fake to a single class\n",
    "        if (\"humor\" in label) or (\"fake\" in label):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        tokens, lang = th.parse_tweet(tweet)\n",
    "        \n",
    "        #replace the row with the simplified tokens, the mapped labels and the detected language\n",
    "        dataset.loc[i] = tokens, label, lang\n",
    "    \n",
    "    #make sure the label column is converted into a column of integers and not objects\n",
    "    dataset.label = dataset.label.astype(\"int\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data and populate language column\n",
    "simplified_training = transform_data(original_training)\n",
    "simplified_testing = transform_data(original_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>cafeinoman acojona sandy</td>\n",
       "      <td>1</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13756</th>\n",
       "      <td>gambar ini bukan mh370 ini adalah gambar dari...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>better pic porch fish shark sandi</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647</th>\n",
       "      <td>sochi serv all food in ass olymp how putin ass</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7200</th>\n",
       "      <td>dope hurricanesandi</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6450</th>\n",
       "      <td>y asi pas tiburon call wildwood pas huracan s...</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6484</th>\n",
       "      <td>huracan sandy new york dios cuid tod person</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6411</th>\n",
       "      <td>sandi</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11302</th>\n",
       "      <td>rememb presid bush vacat hurrican katrina thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8904</th>\n",
       "      <td>may god us hurrican sandi</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14277 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens  label     lang\n",
       "3446                            cafeinoman acojona sandy      1       pt\n",
       "13756   gambar ini bukan mh370 ini adalah gambar dari...      1  unknown\n",
       "1583                   better pic porch fish shark sandi      1       en\n",
       "13647     sochi serv all food in ass olymp how putin ass      1       en\n",
       "7200                                 dope hurricanesandi      1       en\n",
       "...                                                  ...    ...      ...\n",
       "6450    y asi pas tiburon call wildwood pas huracan s...      1       es\n",
       "6484         huracan sandy new york dios cuid tod person      1       es\n",
       "6411                                               sandi      1       en\n",
       "11302   rememb presid bush vacat hurrican katrina thi...      0       en\n",
       "8904                           may god us hurrican sandi      0       en\n",
       "\n",
       "[14277 rows x 3 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_training = simplified_training.sample(frac = 1)\n",
    "simplified_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>syria syrian hero boy rescu girl shootout</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>nepal histor dharahara tower collaps massiv e...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>syria syrian hero boy rescu girl shootout see...</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>nepal amp histor dharahara tower collaps mass...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>nepal histor dharahara tower collaps massiv e...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>syria syrian hero boy rescu girl shootout مجه...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>syrian hero boy rescu girl while under sniper...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>syria syrian hero boy rescu girl shootout الط...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>awwww rt syrian boy appear brave sniper fire ...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>ghost boot behind girl spot viral photo</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3755 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  label     lang\n",
       "3314          syria syrian hero boy rescu girl shootout      1       en\n",
       "1716   nepal histor dharahara tower collaps massiv e...      0       en\n",
       "2398   syria syrian hero boy rescu girl shootout see...      1  unknown\n",
       "1303   nepal amp histor dharahara tower collaps mass...      0       en\n",
       "1636   nepal histor dharahara tower collaps massiv e...      0       en\n",
       "...                                                 ...    ...      ...\n",
       "3197   syria syrian hero boy rescu girl shootout مجه...      1       en\n",
       "2583   syrian hero boy rescu girl while under sniper...      1       en\n",
       "2713   syria syrian hero boy rescu girl shootout الط...      1       en\n",
       "2957   awwww rt syrian boy appear brave sniper fire ...      1       en\n",
       "492             ghost boot behind girl spot viral photo      1       en\n",
       "\n",
       "[3755 rows x 3 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_testing = simplified_testing.sample(frac = 1)\n",
    "simplified_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# TESTING STUFF\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('sgd',\n",
       "                              Pipeline(steps=[('cv',\n",
       "                                               CountVectorizer(max_features=23000,\n",
       "                                                               ngram_range=(1,\n",
       "                                                                            7))),\n",
       "                                              ('sgd',\n",
       "                                               SGDClassifier(l1_ratio=0.6,\n",
       "                                                             n_jobs=-1,\n",
       "                                                             penalty='elasticnet',\n",
       "                                                             random_state=10))])),\n",
       "                             ('svc',\n",
       "                              Pipeline(steps=[('cv',\n",
       "                                               CountVectorizer(max_features=3000)),\n",
       "                                              ('svc', LinearSVC(C=1))])),\n",
       "                             ('mnb',\n",
       "                              Pipeline(steps=[('tfidf',\n",
       "                                               TfidfVectorizer(ngram_range=(1,\n",
       "                                                                            9))),\n",
       "                                              ('mnb',\n",
       "                                               MultinomialNB(alpha=0.25))])),\n",
       "                             ('rf',\n",
       "                              Pipeline(steps=[('cv',\n",
       "                                               CountVectorizer(max_features=18000)),\n",
       "                                              ('rf',\n",
       "                                               RandomForestClassifier(n_estimators=800,\n",
       "                                                                      n_jobs=-1,\n",
       "                                                                      random_state=10))]))],\n",
       "                 n_jobs=-1)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,7), max_features = 23000)),\n",
    "    (\"sgd\", SGDClassifier(alpha = 0.0001, l1_ratio = 0.6, penalty = \"elasticnet\", random_state = 10, n_jobs = -1))\n",
    "])\n",
    "\n",
    "svc_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,1), max_features = 3000)),\n",
    "    (\"svc\", LinearSVC(C = 1)),\n",
    "])\n",
    "\n",
    "mnb_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range = (1,9))),\n",
    "    (\"mnb\", MultinomialNB(alpha = 0.25))\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,1), max_features = 18000)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators = 800, max_depth = None, max_features = \"auto\", random_state = 10, n_jobs = -1))\n",
    "])\n",
    "\n",
    "vcf = VotingClassifier([\n",
    "    (\"sgd\", sgd_pipe),\n",
    "    (\"svc\", svc_pipe),\n",
    "    (\"mnb\", mnb_pipe),\n",
    "    (\"rf\", rf_pipe)\n",
    "], voting = \"hard\", n_jobs = -1)\n",
    "\n",
    "vcf.fit(simplified_training.tokens, simplified_training.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = simplified_testing.label\n",
    "predictions = vcf.predict(simplified_testing.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(true, predictions):\n",
    "    \n",
    "    print(\"-------------------- REPORT --------------------\\n\")\n",
    "    \n",
    "    #Format the scores to 2 decimal places\n",
    "    print(\"F1 score:\", \"%0.2f\" % f1_score(true, predictions))\n",
    "    print(\"\\nPrecision score:\", \"%0.2f\" % precision_score(true, predictions))\n",
    "    print(\"\\nRecall score:\", \"%0.2f\" % recall_score(true, predictions))\n",
    "        \n",
    "    print(\"\\nConfusion matrix:\\n\\n\", confusion_matrix(true,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      1209\n",
      "           1       0.89      0.83      0.86      2546\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.79      0.81      0.80      3755\n",
      "weighted avg       0.82      0.82      0.82      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- REPORT --------------------\n",
      "\n",
      "F1 score: 0.80\n",
      "\n",
      "Precision score: 0.68\n",
      "\n",
      "Recall score: 0.97\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      " [[  35 1174]\n",
      " [  85 2461]]\n"
     ]
    }
   ],
   "source": [
    "knn_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"knn\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "knn_pipe.fit(simplified_training.tokens, simplified_training.label)\n",
    "\n",
    "true = simplified_testing.label\n",
    "predictions = knn_pipe.predict(simplified_testing.tokens)\n",
    "\n",
    "report(true, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 5000)\n",
    "tokenizer.fit_on_texts(simplified_training.tokens)\n",
    "\n",
    "raw_train = tokenizer.texts_to_sequences(simplified_training.tokens)\n",
    "raw_test = tokenizer.texts_to_sequences(simplified_testing.tokens)\n",
    "\n",
    "padded_train = pad_sequences(raw_train, padding = \"post\", maxlen = 24)\n",
    "padded_test = pad_sequences(raw_train, padding = \"post\", maxlen = 24)\n",
    "\n",
    "#pair each instance with it's label\n",
    "unsplitwlabels = list(zip(padded_train, simplified_training.label))\n",
    "testwlabels = list(zip(padded_test, simplified_testing.label))\n",
    "\n",
    "#shuffle the training data before splitting it into a validation set\n",
    "shuffle(unsplitwlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and validation\n",
    "\n",
    "#take the first n elements of the list\n",
    "trainwlabels = unsplitwlabels[:12000]\n",
    "\n",
    "#take the last n elements of the list\n",
    "validationwlabels = unsplitwlabels[12000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11836"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 24, 5)             59180     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_9 (Average (None, 1, 5)              0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 59,216\n",
      "Trainable params: 59,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "#word2vec library\n",
    "model.add(layers.Embedding(input_dim = vocab_size, output_dim = 5, input_length = 24))\n",
    "\n",
    "model.add(layers.AveragePooling1D(24))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(5, activation = \"relu\"))\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(list(np.array(x[0]).astype(int) for x in trainwlabels))\n",
    "train_label = np.array(list(np.array(x[1]).astype(int) for x in trainwlabels))\n",
    "validation_data = np.array(list(np.array(x[0]).astype(int) for x in validationwlabels))\n",
    "validation_label = np.array(list(np.array(x[1]).astype(int) for x in validationwlabels))\n",
    "test_data = np.array(list(np.array(x[0]).astype(int) for x in testwlabels))\n",
    "test_label = np.array(list(np.array(x[1]).astype(int) for x in testwlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.6481 - accuracy: 0.6496 - val_loss: 0.5621 - val_accuracy: 0.6785\n",
      "Epoch 2/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.5524 - accuracy: 0.6781 - val_loss: 0.4550 - val_accuracy: 0.8195\n",
      "Epoch 3/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4430 - accuracy: 0.8189 - val_loss: 0.3774 - val_accuracy: 0.8740\n",
      "Epoch 4/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3614 - accuracy: 0.8748 - val_loss: 0.3289 - val_accuracy: 0.8946\n",
      "Epoch 5/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3059 - accuracy: 0.9045 - val_loss: 0.2985 - val_accuracy: 0.9065\n",
      "Epoch 6/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2684 - accuracy: 0.9177 - val_loss: 0.2781 - val_accuracy: 0.9113\n",
      "Epoch 7/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2379 - accuracy: 0.9282 - val_loss: 0.2613 - val_accuracy: 0.9122\n",
      "Epoch 8/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9324 - val_loss: 0.2514 - val_accuracy: 0.9126\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9379 - val_loss: 0.2464 - val_accuracy: 0.9148\n",
      "Epoch 10/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1866 - accuracy: 0.9418 - val_loss: 0.2424 - val_accuracy: 0.9157\n",
      "Epoch 11/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1754 - accuracy: 0.9422 - val_loss: 0.2412 - val_accuracy: 0.9161\n",
      "Epoch 12/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9459 - val_loss: 0.2411 - val_accuracy: 0.9152\n",
      "Epoch 13/100\n",
      "375/375 [==============================] - 0s 994us/step - loss: 0.1487 - accuracy: 0.9503 - val_loss: 0.2420 - val_accuracy: 0.9130\n",
      "Epoch 14/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9520 - val_loss: 0.2457 - val_accuracy: 0.9144\n",
      "Epoch 15/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9545 - val_loss: 0.2519 - val_accuracy: 0.9135\n",
      "Epoch 16/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9524 - val_loss: 0.2524 - val_accuracy: 0.9122\n",
      "Epoch 17/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9569 - val_loss: 0.2558 - val_accuracy: 0.9130\n",
      "Epoch 18/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9544 - val_loss: 0.2610 - val_accuracy: 0.9130\n",
      "Epoch 19/100\n",
      "375/375 [==============================] - 0s 992us/step - loss: 0.1209 - accuracy: 0.9592 - val_loss: 0.2671 - val_accuracy: 0.9122\n",
      "Epoch 20/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9573 - val_loss: 0.2705 - val_accuracy: 0.9091\n",
      "Epoch 21/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9563 - val_loss: 0.2770 - val_accuracy: 0.9038\n",
      "Epoch 22/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9610 - val_loss: 0.2818 - val_accuracy: 0.9025\n",
      "Epoch 23/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9609 - val_loss: 0.2870 - val_accuracy: 0.9082\n",
      "Epoch 24/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9593 - val_loss: 0.2941 - val_accuracy: 0.9078\n",
      "Epoch 25/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9625 - val_loss: 0.2985 - val_accuracy: 0.9078\n",
      "Epoch 26/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9641 - val_loss: 0.3061 - val_accuracy: 0.9069\n",
      "Epoch 27/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9612 - val_loss: 0.3093 - val_accuracy: 0.9047\n",
      "Epoch 28/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9667 - val_loss: 0.3166 - val_accuracy: 0.9065\n",
      "Epoch 29/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9624 - val_loss: 0.3205 - val_accuracy: 0.9034\n",
      "Epoch 30/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9647 - val_loss: 0.3265 - val_accuracy: 0.9021\n",
      "Epoch 31/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9667 - val_loss: 0.3327 - val_accuracy: 0.8990\n",
      "Epoch 32/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.9654 - val_loss: 0.3384 - val_accuracy: 0.9007\n",
      "Epoch 33/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9644 - val_loss: 0.3437 - val_accuracy: 0.9016\n",
      "Epoch 34/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9631 - val_loss: 0.3497 - val_accuracy: 0.9021\n",
      "Epoch 35/100\n",
      "375/375 [==============================] - 0s 987us/step - loss: 0.0958 - accuracy: 0.9641 - val_loss: 0.3550 - val_accuracy: 0.9021\n",
      "Epoch 36/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9661 - val_loss: 0.3621 - val_accuracy: 0.9043\n",
      "Epoch 37/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9667 - val_loss: 0.3649 - val_accuracy: 0.8981\n",
      "Epoch 38/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9668 - val_loss: 0.3716 - val_accuracy: 0.9016\n",
      "Epoch 39/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9677 - val_loss: 0.3767 - val_accuracy: 0.8946\n",
      "Epoch 40/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9667 - val_loss: 0.3934 - val_accuracy: 0.9051\n",
      "Epoch 41/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9673 - val_loss: 0.3879 - val_accuracy: 0.9003\n",
      "Epoch 42/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9669 - val_loss: 0.3923 - val_accuracy: 0.8968\n",
      "Epoch 43/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.9663 - val_loss: 0.4062 - val_accuracy: 0.9034\n",
      "Epoch 44/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9702 - val_loss: 0.4039 - val_accuracy: 0.8986\n",
      "Epoch 45/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9657 - val_loss: 0.4088 - val_accuracy: 0.8999\n",
      "Epoch 46/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9639 - val_loss: 0.4120 - val_accuracy: 0.8915\n",
      "Epoch 47/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9680 - val_loss: 0.4202 - val_accuracy: 0.9007\n",
      "Epoch 48/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9690 - val_loss: 0.4221 - val_accuracy: 0.8955\n",
      "Epoch 49/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9641 - val_loss: 0.4264 - val_accuracy: 0.8933\n",
      "Epoch 50/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9672 - val_loss: 0.4307 - val_accuracy: 0.8959\n",
      "Epoch 51/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9683 - val_loss: 0.4349 - val_accuracy: 0.8964\n",
      "Epoch 52/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9674 - val_loss: 0.4401 - val_accuracy: 0.8911\n",
      "Epoch 53/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9671 - val_loss: 0.4478 - val_accuracy: 0.8981\n",
      "Epoch 54/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9660 - val_loss: 0.4480 - val_accuracy: 0.8933\n",
      "Epoch 55/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.9648 - val_loss: 0.4513 - val_accuracy: 0.8937\n",
      "Epoch 56/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9675 - val_loss: 0.4567 - val_accuracy: 0.8906\n",
      "Epoch 57/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9681 - val_loss: 0.4631 - val_accuracy: 0.8884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.9675 - val_loss: 0.4653 - val_accuracy: 0.8898\n",
      "Epoch 59/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9708 - val_loss: 0.4693 - val_accuracy: 0.8924\n",
      "Epoch 60/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9694 - val_loss: 0.4718 - val_accuracy: 0.8911\n",
      "Epoch 61/100\n",
      "375/375 [==============================] - 0s 989us/step - loss: 0.0822 - accuracy: 0.9687 - val_loss: 0.4767 - val_accuracy: 0.8893\n",
      "Epoch 62/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9689 - val_loss: 0.4897 - val_accuracy: 0.8977\n",
      "Epoch 63/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9689 - val_loss: 0.4856 - val_accuracy: 0.8937\n",
      "Epoch 64/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9659 - val_loss: 0.4954 - val_accuracy: 0.8977\n",
      "Epoch 65/100\n",
      "375/375 [==============================] - 0s 995us/step - loss: 0.0766 - accuracy: 0.9702 - val_loss: 0.4923 - val_accuracy: 0.8933\n",
      "Epoch 66/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9659 - val_loss: 0.4977 - val_accuracy: 0.8950\n",
      "Epoch 67/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9665 - val_loss: 0.5021 - val_accuracy: 0.8942\n",
      "Epoch 68/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9689 - val_loss: 0.5042 - val_accuracy: 0.8876\n",
      "Epoch 69/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9665 - val_loss: 0.5162 - val_accuracy: 0.8990\n",
      "Epoch 70/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9691 - val_loss: 0.5178 - val_accuracy: 0.8981\n",
      "Epoch 71/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9681 - val_loss: 0.5205 - val_accuracy: 0.8959\n",
      "Epoch 72/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.9687 - val_loss: 0.5188 - val_accuracy: 0.8924\n",
      "Epoch 73/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9672 - val_loss: 0.5213 - val_accuracy: 0.8924\n",
      "Epoch 74/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.9666 - val_loss: 0.5252 - val_accuracy: 0.8906\n",
      "Epoch 75/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9682 - val_loss: 0.5299 - val_accuracy: 0.8924\n",
      "Epoch 76/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.9665 - val_loss: 0.5406 - val_accuracy: 0.8972\n",
      "Epoch 77/100\n",
      "375/375 [==============================] - 0s 998us/step - loss: 0.0867 - accuracy: 0.9659 - val_loss: 0.5352 - val_accuracy: 0.8911\n",
      "Epoch 78/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9649 - val_loss: 0.5456 - val_accuracy: 0.8950\n",
      "Epoch 79/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9664 - val_loss: 0.5421 - val_accuracy: 0.8898\n",
      "Epoch 80/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9673 - val_loss: 0.5497 - val_accuracy: 0.8924\n",
      "Epoch 81/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9654 - val_loss: 0.5493 - val_accuracy: 0.8911\n",
      "Epoch 82/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9656 - val_loss: 0.5500 - val_accuracy: 0.8884\n",
      "Epoch 83/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9659 - val_loss: 0.5538 - val_accuracy: 0.8876\n",
      "Epoch 84/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9700 - val_loss: 0.5607 - val_accuracy: 0.8928\n",
      "Epoch 85/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9702 - val_loss: 0.5585 - val_accuracy: 0.8898\n",
      "Epoch 86/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9685 - val_loss: 0.5613 - val_accuracy: 0.8893\n",
      "Epoch 87/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9693 - val_loss: 0.5712 - val_accuracy: 0.8937\n",
      "Epoch 88/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9678 - val_loss: 0.5662 - val_accuracy: 0.8893\n",
      "Epoch 89/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9696 - val_loss: 0.5742 - val_accuracy: 0.8933\n",
      "Epoch 90/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9692 - val_loss: 0.5767 - val_accuracy: 0.8924\n",
      "Epoch 91/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9667 - val_loss: 0.5767 - val_accuracy: 0.8915\n",
      "Epoch 92/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.9700 - val_loss: 0.5945 - val_accuracy: 0.8950\n",
      "Epoch 93/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9694 - val_loss: 0.5792 - val_accuracy: 0.8884\n",
      "Epoch 94/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9697 - val_loss: 0.5813 - val_accuracy: 0.8880\n",
      "Epoch 95/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.9690 - val_loss: 0.5912 - val_accuracy: 0.8933\n",
      "Epoch 96/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9670 - val_loss: 0.5902 - val_accuracy: 0.8915\n",
      "Epoch 97/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.9689 - val_loss: 0.5889 - val_accuracy: 0.8920\n",
      "Epoch 98/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9706 - val_loss: 0.6053 - val_accuracy: 0.8946\n",
      "Epoch 99/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9668 - val_loss: 0.5941 - val_accuracy: 0.8898\n",
      "Epoch 100/100\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9693 - val_loss: 0.6022 - val_accuracy: 0.8928\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data, train_label,\n",
    "    epochs = 100, \n",
    "    verbose = False, #True, \n",
    "    validation_data = (validation_data, validation_label)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains the model and returns it for use with sklearn\n",
    "def forskl():\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words = 5000)\n",
    "    tokenizer.fit_on_texts(simplified_training.tokens)\n",
    "\n",
    "    raw_train = tokenizer.texts_to_sequences(simplified_training.tokens)\n",
    "    raw_test = tokenizer.texts_to_sequences(simplified_testing.tokens)\n",
    "\n",
    "    padded_train = pad_sequences(raw_train, padding = \"post\", maxlen = 24)\n",
    "    padded_test = pad_sequences(raw_train, padding = \"post\", maxlen = 24)\n",
    "\n",
    "    #pair each instance with it's label\n",
    "    unsplitwlabels = list(zip(padded_train, simplified_training.label))\n",
    "    testwlabels = list(zip(padded_test, simplified_testing.label))\n",
    "\n",
    "    #shuffle the training data before splitting it into a validation set\n",
    "    shuffle(unsplitwlabels)\n",
    "    \n",
    "    train_data = np.array(list(np.array(x[0]).astype(int) for x in trainwlabels))\n",
    "    train_label = np.array(list(np.array(x[1]).astype(int) for x in trainwlabels))\n",
    "    validation_data = np.array(list(np.array(x[0]).astype(int) for x in validationwlabels))\n",
    "    validation_label = np.array(list(np.array(x[1]).astype(int) for x in validationwlabels))\n",
    "    test_data = np.array(list(np.array(x[0]).astype(int) for x in testwlabels))\n",
    "    test_label = np.array(list(np.array(x[1]).astype(int) for x in testwlabels))\n",
    "    \n",
    "    model = build_model()\n",
    "    model.fit(\n",
    "        train_data, train_label,\n",
    "        epochs = 100, \n",
    "        verbose = False, #True, \n",
    "        validation_data = (validation_data, validation_label)\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3755, 1)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3755"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(train_data)\n",
    "predictions = (predictions > 0.5).astype('int').reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66275"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions > 0.5)/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96      4189\n",
      "           1       0.96      0.99      0.98      7811\n",
      "\n",
      "    accuracy                           0.97     12000\n",
      "   macro avg       0.97      0.96      0.97     12000\n",
      "weighted avg       0.97      0.97      0.97     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_label, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)\n",
    "predictions = (predictions > 0.5).astype('int').reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.02      0.04      1209\n",
      "           1       0.68      0.98      0.80      2546\n",
      "\n",
      "    accuracy                           0.67      3755\n",
      "   macro avg       0.52      0.50      0.42      3755\n",
      "weighted avg       0.58      0.67      0.56      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_label, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
