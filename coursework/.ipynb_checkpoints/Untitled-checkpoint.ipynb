{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"mediaeval-2015-trainingset.txt\", delimiter = \"\\t\")\n",
    "test_data = pd.read_csv(\"mediaeval-2015-testset.txt\", delimiter = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¿Se acuerdan de la película: “El día después d...</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda i...</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Huracán Sandy, me recuerda a...</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14272</th>\n",
       "      <td>443231991593304064</td>\n",
       "      <td>@BobombDom *slaps TweetDeck with the PigFish h...</td>\n",
       "      <td>2179310905</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Da_Vault_Hunter</td>\n",
       "      <td>Tue Mar 11 03: 48: 36 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14273</th>\n",
       "      <td>443086239127076865</td>\n",
       "      <td>New Species of Fish found in Brazil or just Re...</td>\n",
       "      <td>254843101</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>DjSituation_RC</td>\n",
       "      <td>Mon Mar 10 18: 09: 26 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14274</th>\n",
       "      <td>442978105238753280</td>\n",
       "      <td>What do we call this? #pigFISH http: \\/\\/t.co\\...</td>\n",
       "      <td>2367553228</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Vivo1Vuyo</td>\n",
       "      <td>Mon Mar 10 10: 59: 45 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14275</th>\n",
       "      <td>442753479782989824</td>\n",
       "      <td>Pigfish ? E dopo il pescecane c'è il pesce mai...</td>\n",
       "      <td>603120231</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>CosimoTarta</td>\n",
       "      <td>Sun Mar 09 20: 07: 10 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14276</th>\n",
       "      <td>442700377860104192</td>\n",
       "      <td>For those who can't decide between fish or mea...</td>\n",
       "      <td>25086784</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>johnszim</td>\n",
       "      <td>Sun Mar 09 16: 36: 09 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6742 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweetId                                          tweetText  \\\n",
       "0      263046056240115712  ¿Se acuerdan de la película: “El día después d...   \n",
       "1      262995061304852481  @milenagimon: Miren a Sandy en NY!  Tremenda i...   \n",
       "2      262979898002534400  Buena la foto del Huracán Sandy, me recuerda a...   \n",
       "3      262996108400271360     Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4      263018881839411200  My fave place in the world #nyc #hurricane #sa...   \n",
       "...                   ...                                                ...   \n",
       "14272  443231991593304064  @BobombDom *slaps TweetDeck with the PigFish h...   \n",
       "14273  443086239127076865  New Species of Fish found in Brazil or just Re...   \n",
       "14274  442978105238753280  What do we call this? #pigFISH http: \\/\\/t.co\\...   \n",
       "14275  442753479782989824  Pigfish ? E dopo il pescecane c'è il pesce mai...   \n",
       "14276  442700377860104192  For those who can't decide between fish or mea...   \n",
       "\n",
       "           userId      imageId(s)         username  \\\n",
       "0        21226711  sandyA_fake_46          iAnnieM   \n",
       "1       192378571  sandyA_fake_09   CarlosVerareal   \n",
       "2       132303095  sandyA_fake_09      LucasPalape   \n",
       "3       241995902  sandyA_fake_29      Haaaaarryyy   \n",
       "4       250315890  sandyA_fake_15   princess__natt   \n",
       "...           ...             ...              ...   \n",
       "14272  2179310905      pigFish_01  Da_Vault_Hunter   \n",
       "14273   254843101      pigFish_01   DjSituation_RC   \n",
       "14274  2367553228      pigFish_01        Vivo1Vuyo   \n",
       "14275   603120231      pigFish_01      CosimoTarta   \n",
       "14276    25086784      pigFish_01         johnszim   \n",
       "\n",
       "                              timestamp label  \n",
       "0        Mon Oct 29 22:34:01 +0000 2012  fake  \n",
       "1        Mon Oct 29 19:11:23 +0000 2012  fake  \n",
       "2        Mon Oct 29 18:11:08 +0000 2012  fake  \n",
       "3        Mon Oct 29 19:15:33 +0000 2012  fake  \n",
       "4        Mon Oct 29 20:46:02 +0000 2012  fake  \n",
       "...                                 ...   ...  \n",
       "14272  Tue Mar 11 03: 48: 36 +0000 2014  fake  \n",
       "14273  Mon Mar 10 18: 09: 26 +0000 2014  fake  \n",
       "14274  Mon Mar 10 10: 59: 45 +0000 2014  fake  \n",
       "14275  Sun Mar 09 20: 07: 10 +0000 2014  fake  \n",
       "14276  Sun Mar 09 16: 36: 09 +0000 2014  fake  \n",
       "\n",
       "[6742 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[training_data[\"label\"] == \"fake\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a tweet, return a tuple of a set of words of the hashtags and a set of normal words\n",
    "def parse_tweet(tweet):\n",
    "    \n",
    "    #remove any hashtags \n",
    "    tweet = tweet.replace(\"#\", \"\")\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for tok in nltk.word_tokenize(tweet):\n",
    "        \n",
    "        \n",
    "    \n",
    "    return nltk.word_tokenize(tweet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scary', 'shit', 'hurricane', 'NY', 'http', ':', '//t.co/e4JLBUfH']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_tweet(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/georgegarrington/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "sentence = \"The quick brown fox, jumped over the lazy dog\"\n",
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any stop words from the tokens. Stop words are assumed to all be lowercase\n",
    "def filter_tokens(tokens, stop_words):\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for tok in tokens:\n",
    "        lc = tok.lower()\n",
    "        if(lc not in stop_words):\n",
    "            filtered_tokens.append(lc)\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estad', 'o', 'habida', 'hayan', 'seamos', 'eres', 'estaríais', 'yo', 'tuya', 'fuesen', 'porque', 'había', 'habré', 'estuvieron', 'estará', 'seré', 'unos', 'nuestro', 'fuésemos', 'hubiese', 'estéis', 'tenida', 'hubiésemos', 'estarías', 'seríais', 'algunas', 'las', 'son', 'ese', 'tú', 'mío', 'tanto', 'estos', 'habréis', 'estas', 'habrá', 'estaríamos', 'un', 'seréis', 'tengáis', 'estuviese', 'mucho', 'y', 'vosotros', 'es', 'habías', 'estuvo', 'esta', 'somos', 'nos', 'estuvisteis', 'entre', 'hayáis', 'tuvieras', 'hubieseis', 'hubo', 'suyo', 'tenga', 'tendrá', 'vuestra', 'sus', 'he', 'e', 'era', 'todos', 'tendré', 'mi', 'tendríais', 'habríamos', 'siente', 'suyas', 'estuviera', 'habéis', 'míos', 'fueron', 'les', 'habremos', 'tuviéramos', 'eran', 'todo', 'se', 'fueran', 'serían', 'contra', 'muy', 'hubieran', 'habrías', 'os', 'mías', 'no', 'ellas', 'nuestros', 'tuvierais', 'tenías', 'estuvieras', 'fuiste', 'hayamos', 'estás', 'hay', 'tendría', 'otro', 'fueses', 'algo', 'estaréis', 'eso', 'están', 'estuviéramos', 'hubieses', 'tenidas', 'hube', 'con', 'será', 'seáis', 'muchos', 'sentida', 'habrás', 'durante', 'vosotras', 'erais', 'estaremos', 'suyos', 'nosotros', 'seríamos', 'ti', 'fuéramos', 'eras', 'éramos', 'sentido', 'esa', 'tenemos', 'tendrían', 'fuimos', 'teníamos', 'sobre', 'los', 'hubiera', 'su', 'antes', 'tendrán', 'le', 'tuyas', 'tu', 'serías', 'me', 'habríais', 'estada', 'fui', 'estaría', 'esto', 'nuestra', 'estados', 'mis', 'seas', 'teníais', 'tuvo', 'quien', 'estés', 'hubiéramos', 'tienes', 'estemos', 'otros', 'tenían', 'estuve', 'sintiendo', 'estarían', 'poco', 'sentidas', 'ellos', 'hasta', 'estarán', 'seremos', 'tendrías', 'habíais', 'ya', 'tuyo', 'pero', 'una', 'estado', 'estadas', 'estuviste', 'estaré', 'tendrás', 'hubimos', 'estando', 'uno', 'soy', 'tendríamos', 'sois', 'tuviesen', 'suya', 'tuyos', 'mía', 'también', 'serán', 'esos', 'tuviera', 'sin', 'han', 'cual', 'estuvimos', 'de', 'ella', 'cuando', 'tiene', 'estuvieseis', 'tuvieron', 'al', 'sería', 'estuvieses', 'vuestro', 'ante', 'sentid', 'hubierais', 'tuvieses', 'esas', 'nuestras', 'tuviese', 'por', 'habidas', 'fueseis', 'hubieron', 'donde', 'hubiesen', 'habíamos', 'estuviesen', 'estaba', 'que', 'tendremos', 'tus', 'teniendo', 'este', 'estarás', 'lo', 'habían', 'habidos', 'sí', 'otra', 'del', 'ha', 'hayas', 'vuestras', 'estáis', 'tenidos', 'estamos', 'habrán', 'tuvieran', 'nosotras', 'estuviésemos', 'tuviste', 'haya', 'está', 'fue', 'has', 'hubisteis', 'tengan', 'como', 'hemos', 'algunos', 'tuvieseis', 'serás', 'más', 'fuese', 'sea', 'habrían', 'tuve', 'desde', 'tuvimos', 'sentidos', 'fuerais', 'tuvisteis', 'para', 'estoy', 'fuera', 'tengamos', 'habido', 'qué', 'nada', 'hubieras', 'ni', 'estén', 'vuestros', 'tened', 'el', 'tienen', 'él', 'te', 'esté', 'sean', 'a', 'estabais', 'habiendo', 'tenéis', 'otras', 'estabas', 'estaban', 'hubiste', 'mí', 'habría', 'la', 'tengo', 'tendréis', 'tenido', 'estábamos', 'en', 'quienes', 'tengas', 'tuviésemos', 'fueras', 'estuvieran', 'fuisteis', 'estar', 'tenía', 'estuvierais'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/georgegarrington/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "#how to get stopwords in a language\n",
    "stop_words_english=set(stopwords.words(\"english\"))\n",
    "stop_words_spanish=set(stopwords.words(\"spanish\"))\n",
    "print(stop_words_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#stem the words into a single form\n",
    "def stem(unstemmed):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = []\n",
    "    \n",
    "    for word in stemmed:\n",
    "        stemmed.append(ps.stem(word))\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetTokenizer:\n",
    "    \n",
    "    #by default the stop words are english and spanish\n",
    "    stop_words = set(stopwords.words(\"english\") + stopwords.words(\"spanish\"))\n",
    "    \n",
    "    def tokenize(self, tweet):\n",
    "        \n",
    "        filtered_tokens = filter_tokens(tweet, stop_words)\n",
    "        print(\"the filtered tokens are: \", filtered_tokens)\n",
    "        output = []\n",
    "        \n",
    "        for tok in filtered_tokens:\n",
    "        \n",
    "            output.append(tok)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "        #return map(stem, filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scary', 'shit', '#', 'hurricane', '#', 'NY', 'http', ':', '//t.co/e4JLBUfH']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TweetTokenizer()\n",
    "string = training_data.tweetText[3]\n",
    "nltk.word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original is: Scary shit #hurricane #NY http://t.co/e4JLBUfH\n",
      "The tokens are:  ['scary', 'shit', '#hurricane', '#ny', 'http://t.co/e4JLBUfH']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tzr = TweetTokenizer(preserve_case = False, reduce_len = True, strip_handles = True)\n",
    "tokens = tzr.tokenize(string)\n",
    "\n",
    "#given an nltk tokenized tweet, a list of stop words and a stemmer, stem the tokens, \n",
    "#remove prefix hashtags from tokens and also discard any link tokens or stop words\n",
    "def simplify_tweet(tokenized_tweet, stop_words, ps):\n",
    "\n",
    "    output = set()\n",
    "    for tok in tokenized_tweet:\n",
    "        \n",
    "        #discard any links\n",
    "        if(tok[:4] == \"http\" or tok in stop_words):\n",
    "            continue;\n",
    "        \n",
    "        #If there is a prefix hashtag remove it\n",
    "        if(tok[0] == '#'):\n",
    "            tok = tok[1:]\n",
    "        \n",
    "        #stem the token\n",
    "        output.add(ps.stem(tok))\n",
    "\n",
    "    return output\n",
    "\n",
    "print(\"The original is: \" + string)\n",
    "print(\"The tokens are: \", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My test is: {'hurrican', 'scari', 'ny', 'shit'}\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "english_stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "test = simplify_tweet(tokens, english_stop_words, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ¿Se acuerdan de la película: “El día después d...\n",
       "1        @milenagimon: Miren a Sandy en NY!  Tremenda i...\n",
       "2        Buena la foto del Huracán Sandy, me recuerda a...\n",
       "3           Scary shit #hurricane #NY http://t.co/e4JLBUfH\n",
       "4        My fave place in the world #nyc #hurricane #sa...\n",
       "                               ...                        \n",
       "14272    @BobombDom *slaps TweetDeck with the PigFish h...\n",
       "14273    New Species of Fish found in Brazil or just Re...\n",
       "14274    What do we call this? #pigFISH http: \\/\\/t.co\\...\n",
       "14275    Pigfish ? E dopo il pescecane c'è il pesce mai...\n",
       "14276    For those who can't decide between fish or mea...\n",
       "Name: tweetText, Length: 14277, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.tweetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-115-dd9e090bdbcc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-115-dd9e090bdbcc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    modified = del training_data[\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\", \"label\"]\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "modified = del training_data[\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Se acuerdan de la película: “El día después d...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda i...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Huracán Sandy, me recuerda a...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14272</th>\n",
       "      <td>@BobombDom *slaps TweetDeck with the PigFish h...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14273</th>\n",
       "      <td>New Species of Fish found in Brazil or just Re...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14274</th>\n",
       "      <td>What do we call this? #pigFISH http: \\/\\/t.co\\...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14275</th>\n",
       "      <td>Pigfish ? E dopo il pescecane c'è il pesce mai...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14276</th>\n",
       "      <td>For those who can't decide between fish or mea...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14277 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweetText label\n",
       "0      ¿Se acuerdan de la película: “El día después d...  fake\n",
       "1      @milenagimon: Miren a Sandy en NY!  Tremenda i...  fake\n",
       "2      Buena la foto del Huracán Sandy, me recuerda a...  fake\n",
       "3         Scary shit #hurricane #NY http://t.co/e4JLBUfH  fake\n",
       "4      My fave place in the world #nyc #hurricane #sa...  fake\n",
       "...                                                  ...   ...\n",
       "14272  @BobombDom *slaps TweetDeck with the PigFish h...  fake\n",
       "14273  New Species of Fish found in Brazil or just Re...  fake\n",
       "14274  What do we call this? #pigFISH http: \\/\\/t.co\\...  fake\n",
       "14275  Pigfish ? E dopo il pescecane c'è il pesce mai...  fake\n",
       "14276  For those who can't decide between fish or mea...  fake\n",
       "\n",
       "[14277 rows x 2 columns]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified = training_data.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 1 USING STEMMING\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "#illegal as this is the key corresponding to porter stemmer I think which we are not using\n",
    "snowball_langs = list(SnowballStemmer.languages)\n",
    "langdetect_langs = [\"ar\", \"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"hu\", \"it\", \"no\", \"illegal\", \"pt\", \"ro\", \"ru\", \"es\", \"sv\"]\n",
    "lang_dict = dict(zip(langdetect_langs, snowball_langs))\n",
    "\n",
    "#given an nltk tokenized tweet, a list of stop words and a stemmer, stem the tokens, \n",
    "#remove prefix hashtags from tokens and also discard any link tokens or stop words\n",
    "#return the simplified tweet paired with the estimated language\n",
    "def handle_tweet(tweet):\n",
    "    \n",
    "    #tokenizer the tweet with a TWEET TOKENIZER\n",
    "    tokens = tzr.tokenize(tweet)\n",
    "    \n",
    "    #Keep track of the number of tokens (possibly) detected in each language. Assume that\n",
    "    #the tweet is in the language with the most tokens seen\n",
    "    langs_seen = {}\n",
    "    \n",
    "    filtered_toks = set()\n",
    "    \n",
    "    #remove punctuation and digits\n",
    "    for tok in tokens:\n",
    "    \n",
    "        #If there is a prefix hashtag remove it\n",
    "        if tok[0] == '#':\n",
    "            tok = tok[1:]\n",
    "        \n",
    "        #discard non alphanumeric strings and digits\n",
    "        if not tok.isalnum() or tok.isdigit():\n",
    "            continue;\n",
    "            \n",
    "        filtered_toks.add(tok)\n",
    "    \n",
    "    #first determine the language\n",
    "    for tok in filtered_toks:\n",
    "        \n",
    "        #if the token language cannot be detectected then label as unknown\n",
    "        try:\n",
    "            #print(\"The token is:\",tok)\n",
    "            guess = detect(tok)\n",
    "            #print(\"My guess is:\",guess)\n",
    "        except:\n",
    "            guess = \"unknown\"\n",
    "            \n",
    "        if guess in langs_seen:\n",
    "            langs_seen[guess] += 1\n",
    "        else:\n",
    "            langs_seen[guess] = 1\n",
    "        \n",
    "    #the most common language guess seen\n",
    "    predicted_language = max(langs_seen, key=langs_seen.get)\n",
    "    print(\"The original tweet was:\", tweet,\"\\n\")\n",
    "    #print(\"The tokens were:\", filtered_toks)\n",
    "    print(\"The language guesses were:\",langs_seen)\n",
    "    print(\"I predicted the language:\",predicted_language,\"\\n\")\n",
    "    output = set()\n",
    "    \n",
    "    #print(\"Does snowball stemmer support it?\")\n",
    "    \n",
    "    #only stem the tokens if snowball stemmer supports the language\n",
    "    if predicted_language in lang_dict.keys():\n",
    "    \n",
    "        #print(\"Yes it does\")\n",
    "    \n",
    "        stop_words = stopwords.words(lang_dict[predicted_language])\n",
    "        stemmer = SnowballStemmer(lang_dict[predicted_language])\n",
    "    \n",
    "        #now remove stopwords, links, hashtags and stem the words\n",
    "        for tok in filtered_toks:\n",
    "            \n",
    "            #discard any non alphanumeric tokens or stopwords\n",
    "            if(tok in stop_words):\n",
    "                continue;\n",
    "\n",
    "            #stem the token with it's language specific stemmer\n",
    "            output.add(stemmer.stem(tok))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"(Unsupported by snowball stem)\")\n",
    "        return None\n",
    "        #output = filtered_toks\n",
    "        #print(\"No it doesn't\")\n",
    "            \n",
    "    # LOOK ABOVE AND BELOW ME, should change this so you stem the language with the stemmer \n",
    "    # for the language it is detected in\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INDEX 0 ----\n",
      "\n",
      "\n",
      "The original tweet was: ¿Se acuerdan de la película: “El día después de mañana”? Me recuerda a lo que está pasando con el huracán #Sandy. http://t.co/JQQeRPwN \n",
      "\n",
      "The language guesses were: {'es': 10, 'fr': 1, 'tl': 3, 'ca': 1, 'id': 2, 'vi': 1, 'it': 1}\n",
      "I predicted the language: es \n",
      "\n",
      "result: {'pas', 'dia', 'mañan', 'sandy', 'acuerd', 'huracan', 'recuerd', 'pelicul', 'despues'}\n",
      "\n",
      "\n",
      "\n",
      "---- INDEX 1 ----\n",
      "\n",
      "\n",
      "The original tweet was: @milenagimon: Miren a Sandy en NY!  Tremenda imagen del huracán. Parece el \"Día de la Independencia 2\" http://t.co/41jUweux REAL! RT. \n",
      "\n",
      "The language guesses were: {'ro': 2, 'es': 4, 'ca': 1, 'tl': 2, 'fr': 2, 'nl': 2, 'de': 1, 'vi': 1, 'sw': 1, 'tr': 1}\n",
      "I predicted the language: es \n",
      "\n",
      "result: {'real', 'parec', 'mir', 'tremend', 'sandy', 'rt', 'huracan', 'independent', 'imag', 'dia', 'ny'}\n",
      "\n",
      "\n",
      "\n",
      "---- INDEX 2 ----\n",
      "\n",
      "\n",
      "The original tweet was: Buena la foto del Huracán Sandy, me recuerda a la película Día de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ \n",
      "\n",
      "The language guesses were: {'es': 5, 'ca': 1, 'tl': 2, 'nl': 1, 'cy': 1, 'id': 2, 'vi': 1, 'no': 1}\n",
      "I predicted the language: es \n",
      "\n",
      "result: {'fot', 'buen', 'sandy', 'huracan', 'independent', 'id4', 'dia', 'recuerd', 'pelicul'}\n",
      "\n",
      "\n",
      "\n",
      "---- INDEX 3 ----\n",
      "\n",
      "\n",
      "The original tweet was: Scary shit #hurricane #NY http://t.co/e4JLBUfH \n",
      "\n",
      "The language guesses were: {'en': 2, 'sw': 1, 'sq': 1}\n",
      "I predicted the language: en \n",
      "\n",
      "result: {'hurrican', 'scari', 'ny', 'shit'}\n",
      "\n",
      "\n",
      "\n",
      "---- INDEX 4 ----\n",
      "\n",
      "\n",
      "The original tweet was: My fave place in the world #nyc #hurricane #sandy #statueofliberty 🗽 http://t.co/Ex61doZk \n",
      "\n",
      "The language guesses were: {'no': 1, 'es': 1, 'tl': 1, 'af': 2, 'fi': 1, 'pl': 1, 'en': 3}\n",
      "I predicted the language: en \n",
      "\n",
      "result: {'fave', 'place', 'world', 'nyc', 'hurrican', 'statueofliberti', 'sandi'}\n",
      "\n",
      "\n",
      "\n",
      "---- INDEX 5 ----\n",
      "\n",
      "\n",
      "The original tweet was: 42nd #time #square #NYC #subway #hurricane http://t.co/daX5YY7X \n",
      "\n",
      "The language guesses were: {'so': 1, 'sq': 2, 'pl': 1, 'ca': 1, 'en': 1}\n",
      "I predicted the language: sq \n",
      "\n",
      "(Unsupported by snowball stem)\n",
      "result: None\n",
      "\n",
      "\n",
      "\n",
      "---- INDEX 6 ----\n",
      "\n",
      "\n",
      "The original tweet was: Just in time for #halloween a photo of #hurricane #sandy #frankenstorm http://t.co/xquKB4VN \n",
      "\n",
      "The language guesses were: {'da': 2, 'tl': 2, 'af': 1, 'cs': 1, 'en': 3, 'sq': 1, 'et': 1}\n",
      "I predicted the language: en \n",
      "\n",
      "result: {'frankenstorm', 'halloween', 'photo', 'time', 'hurrican', 'sandi'}\n",
      "\n",
      "\n",
      "\n",
      "---- INDEX 7 ----\n",
      "\n",
      "\n",
      "The original tweet was: Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast http://t.co/c4sceiMt \n",
      "\n",
      "The language guesses were: {'da': 1, 'en': 8, 'pl': 1, 'tl': 1, 'fr': 1, 'it': 1, 'cy': 1, 'ro': 1, 'fi': 1}\n",
      "I predicted the language: en \n",
      "\n",
      "result: {'crazi', 'famili', 'hurrican', 'east', 'pic', 'prayer', 'go', 'friend', 'coast', 'sandi'}\n",
      "\n",
      "\n",
      "\n",
      "---- INDEX 8 ----\n",
      "\n",
      "\n",
      "The original tweet was: #sandy #newyork #hurricane #statueofliberty #USA http://t.co/iQfEbO1E \n",
      "\n",
      "The language guesses were: {'tl': 1, 'cy': 1, 'fi': 1, 'en': 2}\n",
      "I predicted the language: en \n",
      "\n",
      "result: {'newyork', 'usa', 'hurrican', 'statueofliberti', 'sandi'}\n",
      "\n",
      "\n",
      "\n",
      "---- INDEX 9 ----\n",
      "\n",
      "\n",
      "The original tweet was: #nyc #hurricane http://t.co/Gv3QxZlq \n",
      "\n",
      "The language guesses were: {'pl': 1, 'en': 1}\n",
      "I predicted the language: pl \n",
      "\n",
      "(Unsupported by snowball stem)\n",
      "result: None\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    print(\"---- INDEX\",i,\"----\\n\\n\")\n",
    "    result = handle_tweet(modified[\"tweetText\"][i])\n",
    "    print(\"result:\",result)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION 2 TRY AND DETECT THE LANGUAGE WITHOUT STEMMING\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "\n",
    "snowball_langs = list(SnowballStemmer.languages)\n",
    "langdetect_langs = [\"ar\", \"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"hu\", \"it\", \"no\", \"illegal\", \"pt\", \"ro\", \"ru\", \"es\", \"sv\"]\n",
    "lang_dict = dict(zip(langdetect_langs, snowball_langs))\n",
    "\n",
    "\n",
    "\n",
    "#given an nltk tokenized tweet, a list of stop words and a stemmer, stem the tokens, \n",
    "#remove prefix hashtags from tokens and also discard any link tokens or stop words\n",
    "#return the simplified tweet paired with the estimated language\n",
    "def handle_tweet(tweet):\n",
    "    \n",
    "    #tokenizer the tweet with a TWEET TOKENIZER\n",
    "    print(\"Going to tokenize the tweet:\",tweet)\n",
    "    tokens = tzr.tokenize(tweet)\n",
    "    \n",
    "    #Keep track of the number of tokens (possibly) detected in each language. Assume that\n",
    "    #the tweet is in the language with the most tokens seen\n",
    "    langs_seen = {}\n",
    "    \n",
    "    filtered_toks = set()\n",
    "    \n",
    "    #remove punctuation and digits\n",
    "    for tok in tokens:\n",
    "    \n",
    "        #If there is a prefix hashtag remove it\n",
    "        if tok[0] == '#':\n",
    "            tok = tok[1:]\n",
    "        \n",
    "        #discard non alphanumeric strings and digits\n",
    "        if not tok.isalnum() or tok.isdigit():\n",
    "            continue;\n",
    "            \n",
    "        filtered_toks.add(tok)\n",
    "    \n",
    "    #first determine the language\n",
    "    for tok in filtered_toks:\n",
    "        \n",
    "        #if the token language cannot be detectected then label as unknown\n",
    "        try:\n",
    "            #print(\"The token is:\",tok)\n",
    "            guess = detect(tok)\n",
    "            #print(\"My guess is:\",guess)\n",
    "        except:\n",
    "            guess = \"unknown\"\n",
    "            \n",
    "        if guess in langs_seen:\n",
    "            langs_seen[guess] += 1\n",
    "        else:\n",
    "            langs_seen[guess] = 1\n",
    "        \n",
    "    #the most common language guess seen\n",
    "    predicted_language = max(langs_seen, key=langs_seen.get)\n",
    "    print(\"The original tweet was:\", tweet,\"\\n\")\n",
    "    #print(\"The tokens were:\", filtered_toks)\n",
    "    print(\"The language guesses were:\",langs_seen)\n",
    "    print(\"I predicted the language:\",predicted_language,\"\\n\")\n",
    "    output = set()\n",
    "    \n",
    "    #print(\"Does snowball stemmer support it?\")\n",
    "    \n",
    "    #only remove stop words if we have predicted a supported language\n",
    "    if predicted_language in lang_dict.keys():\n",
    "    \n",
    "        #print(\"Yes it does\")\n",
    "    \n",
    "        stop_words = stopwords.words(lang_dict[predicted_language])\n",
    "    \n",
    "        #now remove stopwords, links, hashtags and stem the words\n",
    "        for tok in filtered_toks:\n",
    "            \n",
    "            #discard any non alphanumeric tokens or stopwords\n",
    "            if(tok in stop_words):\n",
    "                continue;\n",
    "\n",
    "            #stem the token with it's language specific stemmer\n",
    "            output.add(tok)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"Returning the values:\",filtered_toks,predicted_language)\n",
    "        return filtered_toks\n",
    "        #output = filtered_toks\n",
    "        #print(\"No it doesn't\")\n",
    "            \n",
    "    # LOOK ABOVE AND BELOW ME, should change this so you stem the language with the stemmer \n",
    "    # for the language it is detected in\n",
    "        \n",
    "    print(\"Returning the values:\",output,predicted_language)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweetText    wow! #hurricane #Sandy over lady liberty  http...\n",
       "label                                                     fake\n",
       "Name: 40, dtype: object"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified.loc[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modified' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c5e50d59102b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodified2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodified\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmyMethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modified' is not defined"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "modified2 = copy.deepcopy(modified)\n",
    "\n",
    "def myMethod():\n",
    "\n",
    "    for i in range(modified2.size):\n",
    "\n",
    "        print(\"---- INDEX\",i,\"----\\n\\n\")\n",
    "        tokens = handle_tweet(modified2[\"tweetText\"][i])\n",
    "        \n",
    "        modified2.at[i] = tokens, modified2[\"label\"][i]\n",
    "        print(\"the old value of the row is: \", modified[\"tweetText\"][i])\n",
    "        print(\"the new value of the row is: \", modified2[\"tweetText\"][i])\n",
    "        print(\"result:\",result)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "myMethod()\n",
    "        \n",
    "modified2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42nd #time #square #NYC #subway #hurricane http://t.co/daX5YY7X'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified[\"tweetText\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42nd #time #square #NYC #subway #hurricane http://t.co/daX5YY7X'"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified2[\"tweetText\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14277, 2)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
