{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "#make the columns as wide as possible so we can see all the text\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  263046056240115712   \n",
       "1  262995061304852481   \n",
       "2  262979898002534400   \n",
       "3  262996108400271360   \n",
       "4  263018881839411200   \n",
       "\n",
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "\n",
       "      userId      imageId(s)        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
       "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_training = pd.read_csv(\"mediaeval-2015-trainingset.txt\", delimiter = \"\\t\")\n",
    "original_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS.... http://t.co/je2hcFpVfN</td>\n",
       "      <td>70824972</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>peay_s</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse from ISS.... http://t.co/oqwtTL0ThS</td>\n",
       "      <td>344707006</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>JaredUcanChange</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>‚Äú@Shyman33: Eclipse from ISS.... http://t.co/C0VfboScRj‚Äù Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!</td>\n",
       "      <td>224839607</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>tpjp1231</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>Eclipse from ISS.... http://t.co/En87OtvsU6</td>\n",
       "      <td>134543073</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Shyman33</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose... http://t.co/yNBN7c4O51\\n\\nLa cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>1150728872</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Epimethee_</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  578854927457349632   \n",
       "1  578874632670953472   \n",
       "2  578891261353984000   \n",
       "3  578846612312748032   \n",
       "4  578975333841551360   \n",
       "\n",
       "                                                                                                             tweetText  \\\n",
       "0                                                    kereeen RT @Shyman33: Eclipse from ISS.... http://t.co/je2hcFpVfN   \n",
       "1                                      Absolutely beautiful! RT @Shyman33: Eclipse from ISS.... http://t.co/oqwtTL0ThS   \n",
       "2                                 ‚Äú@Shyman33: Eclipse from ISS.... http://t.co/C0VfboScRj‚Äù Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!   \n",
       "3                                                                          Eclipse from ISS.... http://t.co/En87OtvsU6   \n",
       "4  @ebonfigli: √âclipse vue de l'ISS... Autre chose... http://t.co/yNBN7c4O51\\n\\nLa cr√©ation divine n'a pas de limite üòç   \n",
       "\n",
       "       userId   imageId(s)         username                       timestamp  \\\n",
       "0    70824972  eclipse_01            peay_s  Fri Mar 20 09:45:43 +0000 2015   \n",
       "1   344707006  eclipse_01   JaredUcanChange  Fri Mar 20 11:04:02 +0000 2015   \n",
       "2   224839607  eclipse_01          tpjp1231  Fri Mar 20 12:10:06 +0000 2015   \n",
       "3   134543073  eclipse_01          Shyman33  Fri Mar 20 09:12:41 +0000 2015   \n",
       "4  1150728872   eclipse_01       Epimethee_  Fri Mar 20 17:44:11 +0000 2015   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeat the same process for the testing dataset\n",
    "original_testing = pd.read_csv(\"mediaeval-2015-testset.txt\", delimiter = \"\\t\")\n",
    "original_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake     47.222806\n",
       "real     34.468025\n",
       "humor    18.309169\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 1\n",
    "\n",
    "#we can see that the dataset is skewed towards fake and humor tweets\n",
    "original_training.label.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake     6742\n",
       "real     4921\n",
       "humor    2614\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 2\n",
    "\n",
    "#(6742 + 2614) - 4921 = 4435 additional real entries needed to make the dataset balanced\n",
    "original_training.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14277 entries, 0 to 14276\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweetId     14277 non-null  int64 \n",
      " 1   tweetText   14277 non-null  object\n",
      " 2   userId      14277 non-null  int64 \n",
      " 3   imageId(s)  14277 non-null  object\n",
      " 4   username    14277 non-null  object\n",
      " 5   timestamp   14277 non-null  object\n",
      " 6   label       14277 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 780.9+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3755 entries, 0 to 3754\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweetId     3755 non-null   int64 \n",
      " 1   tweetText   3755 non-null   object\n",
      " 2   userId      3755 non-null   int64 \n",
      " 3   imageId(s)  3755 non-null   object\n",
      " 4   username    3755 non-null   object\n",
      " 5   timestamp   3755 non-null   object\n",
      " 6   label       3755 non-null   object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 205.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# BOOKMARK 3\n",
    "\n",
    "#there are no non null values to being with we can see\n",
    "original_training.info()\n",
    "print(\"\\n\")\n",
    "original_testing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 4\n",
    "\n",
    "#drop all columns apart from the text and the label as none of the other data appears to be useful\n",
    "original_training = original_training.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "\n",
    "#Do the same for the testing set\n",
    "original_testing = original_testing.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "\n",
    "original_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane http://t.co/daX5YY7X</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Just in time for #halloween a photo of #hurricane #sandy #frankenstorm http://t.co/xquKB4VN</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast http://t.co/c4sceiMt</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #USA http://t.co/iQfEbO1E</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#nyc #hurricane http://t.co/Gv3QxZlq</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "5                                                                         42nd #time #square #NYC #subway #hurricane http://t.co/daX5YY7X   \n",
       "6                                             Just in time for #halloween a photo of #hurricane #sandy #frankenstorm http://t.co/xquKB4VN   \n",
       "7                              Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast http://t.co/c4sceiMt   \n",
       "8                                                                   #sandy #newyork #hurricane #statueofliberty #USA http://t.co/iQfEbO1E   \n",
       "9                                                                                                    #nyc #hurricane http://t.co/Gv3QxZlq   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  \n",
       "5  fake  \n",
       "6  fake  \n",
       "7  fake  \n",
       "8  fake  \n",
       "9  fake  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 5 \n",
    "\n",
    "#we can see that not all the posts are in English\n",
    "original_training[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "\n",
       "  label  lang  \n",
       "0  fake   NaN  \n",
       "1  fake   NaN  \n",
       "2  fake   NaN  \n",
       "3  fake   NaN  \n",
       "4  fake   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 6\n",
    "\n",
    "#add a column to store the language, initially empty before langdetect populates it\n",
    "original_training[\"lang\"] = np.nan\n",
    "original_testing[\"lang\"] = np.nan\n",
    "original_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\George\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\George\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import langdetect as l\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import nltk.stem as st\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the languages supported by the stemming algorithm\n",
    "st.SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOKMARK 7 TweetHandler class\n",
    "\n",
    "#responsible for parsing tweets\n",
    "class TweetHandler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        snowball_langs = list(st.SnowballStemmer.languages)\n",
    "        #some languages are supported by stemming but NOT supported by language specific tokenizing,\n",
    "        #only the tokens that are in this set are supported by language specific tokenizing\n",
    "        self.tokenizer_langs = {\"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"it\", \"pt\", \"ru\", \"es\", \"sv\"}\n",
    "        langdetect_langs = [\"ar\", \"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"hu\", \"it\", \"no\", \"illegal\", \"pt\", \"ro\", \"ru\", \"es\", \"sv\"]\n",
    "        #a dictionary to map the corresponding snowball and langdetect properties\n",
    "        self.lang_dict = dict(zip(langdetect_langs, snowball_langs))\n",
    "        #declare some custom stop words\n",
    "        self.custom_stops = [\"http\",\"nhttp\",\"https\"]\n",
    "\n",
    "    #takes a tweet, detects its language, removes any stop words in the language, tokenizes and stems\n",
    "    #specific to the detected language and returns the simplified tokens paired with the language\n",
    "    def parse_tweet(self, tweet):\n",
    "        \n",
    "        try:\n",
    "            lang_prediction = l.detect(tweet)\n",
    "            #the nltk name for the predicted language\n",
    "            nltkprop = self.lang_dict[lang_prediction]\n",
    "        except:\n",
    "            #assume english stopwords and stemming if the language cannot be detected\n",
    "            lang_prediction = \"unknown\"\n",
    "            nltkprop = \"english\"\n",
    "            \n",
    "        # if the language is not supported by the tokenizer (including unkown) then assume tokenizing in English, however stemming\n",
    "        # and stopwords may still be supported in the language that does not support language specific tokenization\n",
    "        # e.g. arabic, hungarian, romanian so tokenize with the english\n",
    "        # version of the algorithm if this is the case and use the stemming and stopwords specific to \n",
    "        # the language if this is available even if the tokenization algorithm isnt\n",
    "        # use a python ternary expression to do this\n",
    "        tokens = nltk.word_tokenize(tweet, language = nltkprop if lang_prediction in self.tokenizer_langs else \"english\")\n",
    "        \n",
    "        #stop words specific to the language\n",
    "        stop_words = set(stopwords.words(nltkprop))\n",
    "        \n",
    "        #stemming algorithm specific to the language detected\n",
    "        stemmer = st.SnowballStemmer(nltkprop)\n",
    "        \n",
    "        # store all tokens to be output as a concatenated string here so that this string\n",
    "        # can later be fed to a CountVectorizer or TfIDFVectorizer , filter out any unwanted tokens \n",
    "        # and don't add them \n",
    "        filtered_tokens = \"\"\n",
    "        \n",
    "        for tok in tokens:\n",
    "            \n",
    "            #remove any hashtags\n",
    "            if tok[0] == '#':\n",
    "                tok = tok[1:]\n",
    "                \n",
    "            #discard non alphanumeric strings containing symbols or pure digits, or stop words\n",
    "            if (not tok.isalnum()) or tok.isdigit() or (tok in stop_words) or tok in self.custom_stops:\n",
    "                continue;\n",
    "            \n",
    "            #carry out stemming specific to the language detected\n",
    "            filtered_tokens += \" \" + stemmer.stem(tok)\n",
    "        \n",
    "        #comment these out when you do not need to check if it works anymore\n",
    "        #print(\"original tokens:\", tokens,\"\\n\")\n",
    "        #print(\"filtered tokens:\", filtered_tokens,\"\\n\")\n",
    "        \n",
    "        return filtered_tokens, lang_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOKMARK 8, transform the dataset from a dataset of tweets into a dataset of labelled tokens in concatenated\n",
    "# string form, along with the detected language\n",
    "\n",
    "def transform_data(arg):\n",
    "\n",
    "    #copy the instance given so we don't change the original instance and can keep it in memory and reuse it \n",
    "    #if necessary\n",
    "    dataset = copy.deepcopy(arg)\n",
    "    th = TweetHandler()\n",
    "    num_rows = dataset.label.size\n",
    "    \n",
    "    #the tweet text will be transformed into tokens so rename the column appropriately\n",
    "    dataset = dataset.rename(columns = {\"tweetText\" : \"tokens\"})\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "\n",
    "        tweet = dataset[\"tokens\"][i]\n",
    "        label = dataset[\"label\"][i]\n",
    "\n",
    "        #disregard the humour information for now, map humor and fake to a single class\n",
    "        if (\"humor\" in label) or (\"fake\" in label):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "            \n",
    "        #for testing\n",
    "        #print(\"The old value of the row is:\",dataset.loc[i],\"\\n\")\n",
    "        \n",
    "        tokens, lang = th.parse_tweet(tweet)\n",
    "        \n",
    "        #replace the row with the simplified tokens, the mapped labels and the detected language\n",
    "        dataset.loc[i] = tokens, label, lang\n",
    "        \n",
    "        #for testing\n",
    "        #print(\"The new value of the row is:\",dataset.loc[i],\"\\n\\n\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data\n",
    "simplified_training = transform_data(original_training)\n",
    "simplified_testing = transform_data(original_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simplified_testing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cd418bf9c853>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimplified_testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'simplified_testing' is not defined"
     ]
    }
   ],
   "source": [
    "simplified_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en         76.619738\n",
       "es          9.056524\n",
       "unknown     7.508580\n",
       "fr          1.505919\n",
       "pt          1.141696\n",
       "de          0.910555\n",
       "it          0.735449\n",
       "nl          0.595363\n",
       "ar          0.546333\n",
       "ru          0.427261\n",
       "sv          0.308188\n",
       "no          0.266162\n",
       "da          0.189115\n",
       "fi          0.091056\n",
       "hu          0.049030\n",
       "ro          0.049030\n",
       "Name: lang, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 9\n",
    "\n",
    "#get an idea of how many of each language there are, we can see that it is predominantly english\n",
    "simplified_training.lang.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9356\n",
       "0    4921\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_training.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to convert the sparse matrix produced by the tfidf vectorizer into a dense matrix in order for it \n",
    "#to be able to be used with different algorithms in a pipeline that require a dense matrix and not a sparse matrix\n",
    "class Densifier():\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **kwargs):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a pipeline and some parameters, perform a grid search with these\n",
    "def grid_search(pipeline, params):\n",
    "    \n",
    "    #make sure we are focusing on maximizing the f1 score and not a different metric, add some verbosity so we can\n",
    "    #see the progress of the grid search to get an idea of how much time it is taking\n",
    "    \n",
    "    #make n_jobs -1 so that all cores that are available cores are used which should hopefully make it quicker\n",
    "    \n",
    "    clf = GridSearchCV(pipeline, params, scoring = \"f1\", verbose = 6, n_jobs = -1)\n",
    "    \n",
    "    #grid_search does not need to take the training data as an argument, we can always assume that simplified_training\n",
    "    #is ready in memory so any algorithm will only ever need to use this to grid search therefore we can implicitly\n",
    "    #reference it like this without having to declare it as an argument to the function\n",
    "    \n",
    "    #make sure the labels are treated as ints and not objects\n",
    "    clf.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to report the scores of the model tuned with the optimal parameters found in grid search\n",
    "# by making predictions of the testing data using this newly found optimal model and calculating the \n",
    "# accuracy vs the true labels of the testing data\n",
    "def report_scores(grid_search_result):\n",
    "    \n",
    "    print(\"---- RESULTS ----\",\"\\n\")\n",
    "    print(\"The algorithm being optimised was:\", grid_search_result.estimator.steps[-1])\n",
    "    print(\"The best parameters found were:\", grid_search_result.best_params_)\n",
    "    y_test_true = simplified_testing.label.astype(\"int\")\n",
    "    y_test_predictions = grid_search_result.predict(simplified_testing.tokens).astype(\"int\")\n",
    "    print(\"Score report:\\n\")\n",
    "    #prevent the chance of any of the lists being treated as objects\n",
    "    print(classification_report(y_test_true.astype('int'), y_test_predictions.astype('int')))\n",
    "    \n",
    "    #TODO DELETE THESE AFTER, JUST FOR TESTING\n",
    "    print(\"f1 score:\", f1_score(y_test_true, y_test_predictions))\n",
    "    print(\"precision:\", precision_score(y_test_true, y_test_predictions))\n",
    "    print(\"recall:\", recall_score(y_test_true, y_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fc3be833d41a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mlrResult_V1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_pipelineV1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_paramsV1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mreport_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlrResult_V1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-93d5fda38593>\u001b[0m in \u001b[0;36mgrid_search\u001b[1;34m(pipeline, params)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#make sure the labels are treated as ints and not objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimplified_training\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimplified_training\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"int\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Start with the multinomial naive bayes algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "lr_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "lr_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'lr__C' : [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "lrResult_V1 = grid_search(lr_pipelineV1, lr_paramsV1)\n",
    "report_scores(lrResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 147 candidates, totalling 735 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   49.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 735 out of 735 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('lr', LogisticRegression())\n",
      "The best parameters found were: {'cv__max_features': 10000, 'cv__ngram_range': (1, 1), 'lr__C': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.78      0.59      1209\n",
      "           1       0.85      0.58      0.69      2546\n",
      "\n",
      "    accuracy                           0.65      3755\n",
      "   macro avg       0.66      0.68      0.64      3755\n",
      "weighted avg       0.72      0.65      0.66      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Start with the multinomial naive bayes algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "lr_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "lr_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,8)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,11)],\n",
    "    'lr__C' : [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "lrResult_V1 = grid_search(lr_pipelineV1, lr_paramsV1)\n",
    "report_scores(lrResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 147 candidates, totalling 735 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   52.5s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 735 out of 735 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('lr', LogisticRegression())\n",
      "The best parameters found were: {'cv__max_features': 11000, 'cv__ngram_range': (1, 1), 'lr__C': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.77      0.66      1209\n",
      "           1       0.87      0.74      0.80      2546\n",
      "\n",
      "    accuracy                           0.75      3755\n",
      "   macro avg       0.73      0.75      0.73      3755\n",
      "weighted avg       0.78      0.75      0.75      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Start with the multinomial naive bayes algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "lr_pipelineV3 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "lr_paramsV3 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,8)],\n",
    "    'cv__max_features' : [1000 * i for i in range (9,16)],\n",
    "    'lr__C' : [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "lrResult_V3 = grid_search(lr_pipelineV3, lr_paramsV3)\n",
    "report_scores(lrResult_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 126 candidates, totalling 630 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   45.9s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 630 out of 630 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('lr', LogisticRegression())\n",
      "The best parameters found were: {'cv__max_features': 11000, 'cv__ngram_range': (1, 1), 'lr__C': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.77      0.66      1209\n",
      "           1       0.87      0.73      0.80      2546\n",
      "\n",
      "    accuracy                           0.75      3755\n",
      "   macro avg       0.73      0.75      0.73      3755\n",
      "weighted avg       0.78      0.75      0.75      3755\n",
      "\n",
      "f1 score: 0.7970149253731343\n",
      "precision: 0.8717350746268657\n",
      "recall: 0.7340926944226237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\George\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Start with the multinomial naive bayes algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "lr_pipelineV4 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "lr_paramsV4 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,15)],\n",
    "    'cv__max_features' : [1000 * i for i in range (10,13)],\n",
    "    'lr__C' : [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "lrResult_V4 = grid_search(lr_pipelineV4, lr_paramsV4)\n",
    "report_scores(lrResult_V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 84 candidates, totalling 420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "#Start with the multinomial naive bayes algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "lrcv_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('dd', Densifier()),\n",
    "    ('ss', StandardScaler()),\n",
    "    ('lr', LogisticRegressionCV())\n",
    "])\n",
    "\n",
    "lrcv_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,5)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,8)],\n",
    "    'lr__Cs' : [1, 10, 100]\n",
    "}\n",
    "\n",
    "lrcvResult_V1 = grid_search(lrcv_pipelineV1, lrcv_paramsV1)\n",
    "report_scores(lrcvResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   35.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 1), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.84      0.51      1209\n",
      "           1       0.81      0.31      0.45      2546\n",
      "\n",
      "    accuracy                           0.48      3755\n",
      "   macro avg       0.59      0.58      0.48      3755\n",
      "weighted avg       0.67      0.48      0.47      3755\n",
      "\n",
      "f1 score: 0.45073612684031705\n",
      "precision: 0.8073022312373225\n",
      "recall: 0.3126472898664572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:   44.0s finished\n"
     ]
    }
   ],
   "source": [
    "#Start with the multinomial naive bayes algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNB_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "mNBResult_V1 = grid_search(mNB_pipelineV1, mNB_paramsV1)\n",
    "report_scores(mNBResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 1), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.84      0.51      1209\n",
      "           1       0.81      0.31      0.45      2546\n",
      "\n",
      "    accuracy                           0.48      3755\n",
      "   macro avg       0.59      0.58      0.48      3755\n",
      "weighted avg       0.67      0.48      0.47      3755\n",
      "\n",
      "f1 score: 0.45073612684031705\n",
      "precision: 0.8073022312373225\n",
      "recall: 0.3126472898664572\n"
     ]
    }
   ],
   "source": [
    "report_scores(mNBResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 252 candidates, totalling 1260 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1260 out of 1260 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 10000, 'cv__ngram_range': (1, 7), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72      1209\n",
      "           1       0.89      0.81      0.85      2546\n",
      "\n",
      "    accuracy                           0.80      3755\n",
      "   macro avg       0.77      0.80      0.78      3755\n",
      "weighted avg       0.82      0.80      0.80      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second iteration with the multinomial naive bayes algorithm,\n",
    "# trying ngrams from (1,1) to (1,9), 4000 max features in the vocabulary to 10000 max features\n",
    "# in increments of 1000, and try values of 2-5 for alpha\n",
    "\n",
    "mNB_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,10)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,11)],\n",
    "    'nb__alpha' : [i for i in range (2,6)]\n",
    "}\n",
    "\n",
    "mNBResult_V2 = grid_search(mNB_pipelineV2, mNB_paramsV2)\n",
    "report_scores(mNBResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5800405268490375"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features = 18000, ngram_range = (1,10))\n",
    "training = cv.fit_transform(simplified_training.tokens)#.todense()\n",
    "testing = cv.fit_transform(simplified_testing.tokens)#.todense()\n",
    "clf = MultinomialNB(alpha = 4).fit(training, simplified_training.label.astype(\"int\"))\n",
    "f1_score(clf.predict(testing), simplified_testing.label.astype(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72      1209\n",
      "           1       0.89      0.81      0.85      2546\n",
      "\n",
      "    accuracy                           0.80      3755\n",
      "   macro avg       0.77      0.80      0.78      3755\n",
      "weighted avg       0.82      0.80      0.81      3755\n",
      "\n",
      "f1 score: 0.846201358863496\n",
      "precision: 0.889225443530939\n",
      "recall: 0.8071484681853889\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"cv\", CountVectorizer(max_features = 10000, ngram_range = (1,7))), \n",
    "     (\"nb\", MultinomialNB(alpha = 3))\n",
    "])\n",
    "\n",
    "pipeline.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "report_scores(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.5s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 14000, 'cv__ngram_range': (1, 8), 'nb__alpha': 4}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      1209\n",
      "           1       0.89      0.84      0.87      2546\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.80      0.81      0.80      3755\n",
      "weighted avg       0.83      0.82      0.83      3755\n",
      "\n",
      "f1 score: 0.8652367462565763\n",
      "precision: 0.8923205342237062\n",
      "recall: 0.8397486252945797\n"
     ]
    }
   ],
   "source": [
    "# Third iteration with the multinomial naive bayes algorithm,\n",
    "# trying ngrams from (1,6) to (1,10), 9000 max features in the vocabulary up to 14000 max features\n",
    "# in increments of 1000, and try values of 2-4 for alpha\n",
    "\n",
    "mNB_pipelineV3 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV3 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (6,11)],\n",
    "    'cv__max_features' : [1000 * i for i in range (9,15)],\n",
    "    'nb__alpha' : [i for i in range (2,5)]\n",
    "}\n",
    "\n",
    "mNBResult_V3 = grid_search(mNB_pipelineV3, mNB_paramsV3)\n",
    "report_scores(mNBResult_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calc_f1(grid_search_result):\n",
    "    y_test_true = simplified_testing.label.astype(\"int\")\n",
    "    y_test_predictions = grid_search_result.predict(simplified_testing.tokens).astype(\"int\")\n",
    "    score = f1_score(y_test_true, y_test_predictions)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8667205169628432\n"
     ]
    }
   ],
   "source": [
    "calc_f1(mNBResult_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   41.0s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 18000, 'cv__ngram_range': (1, 10), 'nb__alpha': 4}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74      1209\n",
      "           1       0.89      0.84      0.87      2546\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.80      0.81      0.80      3755\n",
      "weighted avg       0.83      0.82      0.83      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fourth iteration with the multinomial naive bayes algorithm,\n",
    "# trying ngrams from (1,5) to (1,13), 13000 max features in the vocabulary up to 18000 max features\n",
    "# in increments of 1000, and try values of 1-6 for alpha\n",
    "\n",
    "mNB_pipelineV4 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV4 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (5,13)],\n",
    "    'cv__max_features' : [1000 * i for i in range (13,19)],\n",
    "    'nb__alpha' : [i for i in range (1,7)]\n",
    "}\n",
    "\n",
    "mNBResult_V4 = grid_search(mNB_pipelineV4, mNB_paramsV4)\n",
    "report_scores(mNBResult_V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   25.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 22000, 'cv__ngram_range': (1, 11), 'nb__alpha': 4}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.71      1209\n",
      "           1       0.89      0.79      0.84      2546\n",
      "\n",
      "    accuracy                           0.79      3755\n",
      "   macro avg       0.76      0.79      0.77      3755\n",
      "weighted avg       0.81      0.79      0.79      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fifth iteration with the multinomial naive bayes algorithm,\n",
    "# trying ngrams from (1,8) to (1,12), 17000 max features in the vocabulary up to 22000 max features\n",
    "# in increments of 1000, and try values of 3-6 for alpha\n",
    "\n",
    "mNB_pipelineV5 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV5 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (8,13)],\n",
    "    'cv__max_features' : [1000 * i for i in range (17,23)],\n",
    "    'nb__alpha' : [i for i in range (3,7)]\n",
    "}\n",
    "\n",
    "mNBResult_V5 = grid_search(mNB_pipelineV5, mNB_paramsV5)\n",
    "report_scores(mNBResult_V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 23000, 'cv__ngram_range': (1, 11), 'nb__alpha': 4}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.71      1209\n",
      "           1       0.89      0.79      0.84      2546\n",
      "\n",
      "    accuracy                           0.79      3755\n",
      "   macro avg       0.76      0.79      0.77      3755\n",
      "weighted avg       0.81      0.79      0.79      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sixth iteration with the multinomial naive bayes algorithm,\n",
    "# trying ngrams from (1,10) to (1,12), 21000 max features in the vocabulary up to 24000 max features\n",
    "# in increments of 1000, and try values of 2-5 for alpha\n",
    "\n",
    "mNB_pipelineV6 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV6 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (10,13)],\n",
    "    'cv__max_features' : [1000 * i for i in range (21,25)],\n",
    "    'nb__alpha' : [i for i in range (2,6)]\n",
    "}\n",
    "\n",
    "#we appear to have found a plateau, as the results are largely the same as the last search\n",
    "mNBResult_V6 = grid_search(mNB_pipelineV6, mNB_paramsV6)\n",
    "report_scores(mNBResult_V6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed:  9.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 25000, 'cv__ngram_range': (1, 10), 'nb__alpha': 3.5}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.79      0.62      1209\n",
      "           1       0.86      0.64      0.74      2546\n",
      "\n",
      "    accuracy                           0.69      3755\n",
      "   macro avg       0.69      0.72      0.68      3755\n",
      "weighted avg       0.75      0.69      0.70      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seventh iteration with the multinomial naive bayes algorithm, search around our \"optimal\" we have found and make it more\n",
    "# precise by searching in increments of 0.5 for alpha and 500 for max features to see if this tunes it to be even better.\n",
    "# trying ngrams from (1,8) to (1,12), 17000 max features in the vocabulary up to 22000 max features\n",
    "# in increments of 500, and try values of 2-7 for alpha in increments of 0.5\n",
    "\n",
    "# {'cv__max_features': 18000, 'cv__ngram_range': (1, 10), 'nb__alpha': 4}\n",
    "\n",
    "mNB_pipelineV7 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV7 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (8,13)],\n",
    "    'cv__max_features' : [500 * i for i in range (2 * 20,2 * 26)],\n",
    "    'nb__alpha' : [0.5 * i for i in range (2 * 2,7 * 2)]\n",
    "}\n",
    "\n",
    "mNBResult_V7 = grid_search(mNB_pipelineV7, mNB_paramsV7)\n",
    "report_scores(mNBResult_V7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 140 candidates, totalling 700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   38.8s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__ngram_range': (1, 11), 'nb__alpha': 2.25}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.79      0.61      1209\n",
      "           1       0.86      0.63      0.73      2546\n",
      "\n",
      "    accuracy                           0.68      3755\n",
      "   macro avg       0.68      0.71      0.67      3755\n",
      "weighted avg       0.75      0.68      0.69      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRY WITHOUTTTT SPECIFYING MAX FEATURES\n",
    "\n",
    "#Start with the multinomial naive bayes algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNB_pipelineV8 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV8 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (5,12)],\n",
    "    'nb__alpha' : [0.25 * i for i in range (4 * 1,4 * 6)]\n",
    "}\n",
    "\n",
    "mNBResult_V8 = grid_search(mNB_pipelineV8, mNB_paramsV8)\n",
    "report_scores(mNBResult_V8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   45.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__ngram_range': (1, 5), 'nb__alpha': 2.5}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.79      0.61      1209\n",
      "           1       0.86      0.63      0.73      2546\n",
      "\n",
      "    accuracy                           0.68      3755\n",
      "   macro avg       0.68      0.71      0.67      3755\n",
      "weighted avg       0.75      0.68      0.69      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRY WITHOUTTTT SPECIFYING MAX FEATURES\n",
    "\n",
    "#Start with the multinomial naive bayes algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNB_pipelineV9 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV9 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,7)],\n",
    "    'nb__alpha' : [0.25 * i for i in range (4 * 1,4 * 4)]\n",
    "}\n",
    "\n",
    "mNBResult_V9 = grid_search(mNB_pipelineV9, mNB_paramsV9)\n",
    "report_scores(mNBResult_V9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 858 candidates, totalling 4290 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3597 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4290 out of 4290 | elapsed: 19.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 24000, 'cv__ngram_range': (1, 15), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.79      0.62      1209\n",
      "           1       0.87      0.65      0.74      2546\n",
      "\n",
      "    accuracy                           0.69      3755\n",
      "   macro avg       0.69      0.72      0.68      3755\n",
      "weighted avg       0.75      0.69      0.70      3755\n",
      "\n",
      "f1 score: 0.7405077510671759\n",
      "precision: 0.8650918635170604\n",
      "recall: 0.6472898664571878\n"
     ]
    }
   ],
   "source": [
    "mNB_pipelineV10 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV10 = {\n",
    "    'cv__ngram_range' : [(1,15) for x in range (1,14)],\n",
    "    'cv__max_features' : [1000 * i for i in range (15, 26)],\n",
    "    'nb__alpha' : [1 * i for i in range (1, 7)]\n",
    "}\n",
    "\n",
    "mNBResult_V10 = grid_search(mNB_pipelineV10, mNB_paramsV10)\n",
    "report_scores(mNBResult_V10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   34.7s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:   43.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 5), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.87      0.55      1209\n",
      "           1       0.86      0.40      0.54      2546\n",
      "\n",
      "    accuracy                           0.55      3755\n",
      "   macro avg       0.63      0.63      0.55      3755\n",
      "weighted avg       0.71      0.55      0.55      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now try the ComplementNB algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "cNB_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "cNBResult_V1 = grid_search(cNB_pipelineV1, cNB_paramsV1)\n",
    "report_scores(cNBResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   38.7s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 9000, 'cv__ngram_range': (1, 9), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.85      0.62      1209\n",
      "           1       0.89      0.57      0.70      2546\n",
      "\n",
      "    accuracy                           0.66      3755\n",
      "   macro avg       0.69      0.71      0.66      3755\n",
      "weighted avg       0.76      0.66      0.67      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second iteration with the complement naive bayes algorithm, performance was poor and the best parameters\n",
    "# found were at the maxima of the ranges specified, so try again with larger ranges.\n",
    "# trying ngrams from (1,4) to (1,9), 4000 max features in the vocabulary up to 9000 max features\n",
    "# in increments of 1000, and try values of 1-3 for alpha in increments of 1\n",
    "\n",
    "cNB_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (4,10)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,10)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "cNBResult_V2 = grid_search(cNB_pipelineV2, cNB_paramsV2)\n",
    "report_scores(cNBResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 196 candidates, totalling 980 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.3s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 980 out of 980 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 14000, 'cv__ngram_range': (1, 11), 'nb__alpha': 2}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.85      0.62      1209\n",
      "           1       0.89      0.58      0.70      2546\n",
      "\n",
      "    accuracy                           0.66      3755\n",
      "   macro avg       0.69      0.71      0.66      3755\n",
      "weighted avg       0.76      0.66      0.67      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Third iteration with the complement naive bayes algorithm, performance is still quite poor and the best parameters\n",
    "# found were at the maxima of the ranges specified, so try again with larger ranges.\n",
    "# trying ngrams from (1,4) to (1,9), 4000 max features in the vocabulary up to 9000 max features\n",
    "# in increments of 1000, and try values of 1-3 for alpha in increments of 1\n",
    "\n",
    "cNB_pipelineV3 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV3 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (8,15)],\n",
    "    'cv__max_features' : [1000 * i for i in range (8,15)],\n",
    "    'nb__alpha' : [i for i in range (2,6)]\n",
    "}\n",
    "\n",
    "cNBResult_V3 = grid_search(cNB_pipelineV3, cNB_paramsV3)\n",
    "report_scores(cNBResult_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   49.4s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 18000, 'cv__ngram_range': (1, 10), 'nb__alpha': 2}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.85      0.62      1209\n",
      "           1       0.89      0.58      0.70      2546\n",
      "\n",
      "    accuracy                           0.67      3755\n",
      "   macro avg       0.69      0.71      0.66      3755\n",
      "weighted avg       0.76      0.67      0.68      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Third iteration with the complement naive bayes algorithm, performance is still quite poor and the best parameters\n",
    "# found were at the maxima of the ranges specified, so try again with larger ranges.\n",
    "# trying ngrams from (1,4) to (1,9), 4000 max features in the vocabulary up to 9000 max features\n",
    "# in increments of 1000, and try values of 1-3 for alpha in increments of 1\n",
    "\n",
    "cNB_pipelineV4 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV4 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (10,15)],\n",
    "    'cv__max_features' : [1000 * i for i in range (13,19)],\n",
    "    'nb__alpha' : [i for i in range (1,5)]\n",
    "}\n",
    "\n",
    "cNBResult_V4 = grid_search(cNB_pipelineV4, cNB_paramsV4)\n",
    "report_scores(cNBResult_V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.2s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1280 out of 1280 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 23000, 'cv__ngram_range': (1, 13), 'nb__alpha': 2}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.85      0.55      1209\n",
      "           1       0.85      0.41      0.55      2546\n",
      "\n",
      "    accuracy                           0.55      3755\n",
      "   macro avg       0.63      0.63      0.55      3755\n",
      "weighted avg       0.71      0.55      0.55      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Third iteration with the complement naive bayes algorithm, performance is still quite poor and the best parameters\n",
    "# found were at the maxima of the ranges specified, so try again with larger ranges.\n",
    "# trying ngrams from (1,4) to (1,9), 4000 max features in the vocabulary up to 9000 max features\n",
    "# in increments of 1000, and try values of 1-3 for alpha in increments of 1\n",
    "\n",
    "cNB_pipelineV5 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV5 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (7,15)],\n",
    "    'cv__max_features' : [1000 * i for i in range (17,25)],\n",
    "    'nb__alpha' : [i for i in range (1,5)]\n",
    "}\n",
    "\n",
    "cNBResult_V5 = grid_search(cNB_pipelineV5, cNB_paramsV5)\n",
    "report_scores(cNBResult_V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Third iteration with the complement naive bayes algorithm, performance is still quite poor and the best parameters\n",
    "# found were at the maxima of the ranges specified, so try again with larger ranges.\n",
    "# trying ngrams from (1,4) to (1,9), 4000 max features in the vocabulary up to 9000 max features\n",
    "# in increments of 1000, and try values of 1-3 for alpha in increments of 1\n",
    "\n",
    "cNB_pipelineV5 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV5 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (7,15)],\n",
    "    'cv__max_features' : [1000 * i for i in range (17,25)],\n",
    "    'nb__alpha' : [i for i in range (1,5)]\n",
    "}\n",
    "\n",
    "cNBResult_V5 = grid_search(cNB_pipelineV5, cNB_paramsV5)\n",
    "report_scores(cNBResult_V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:   41.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', BernoulliNB())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 2), 'nb__alpha': 2}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.72      1209\n",
      "           1       0.86      0.87      0.87      2546\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.79      0.79      0.79      3755\n",
      "weighted avg       0.82      0.82      0.82      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now try the BernoulliNB algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "bNB_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer(binary = True)), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('nb', BernoulliNB())\n",
    "])\n",
    "\n",
    "bNB_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "bNBResult_V1 = grid_search(bNB_pipelineV1, bNB_paramsV1)\n",
    "report_scores(bNBResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 252 candidates, totalling 1260 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   43.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1260 out of 1260 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', BernoulliNB())\n",
      "The best parameters found were: {'cv__max_features': 9000, 'cv__ngram_range': (1, 2), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.73      0.53      1209\n",
      "           1       0.80      0.50      0.62      2546\n",
      "\n",
      "    accuracy                           0.58      3755\n",
      "   macro avg       0.61      0.62      0.57      3755\n",
      "weighted avg       0.67      0.58      0.59      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It performs surprisingly excellent even with very limited parameters! Now try a second\n",
    "# search with ngrams from (1,1) to (1,9), 4000 max features in the vocabulary to 10000 max features\n",
    "# in increments of 1000, and try values of 1-4 for alpha\n",
    "\n",
    "bNB_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer(binary = True)), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('nb', BernoulliNB())\n",
    "])\n",
    "\n",
    "bNB_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,10)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,11)],\n",
    "    'nb__alpha' : [i for i in range (1,5)]\n",
    "}\n",
    "\n",
    "bNBResult_V2 = grid_search(bNB_pipelineV2, bNB_paramsV2)\n",
    "report_scores(bNBResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   49.5s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('svc', LinearSVC())\n",
      "The best parameters found were: {'cv__max_features': 3000, 'cv__ngram_range': (1, 1), 'svc__C': 1.0}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.81      0.73      1209\n",
      "           1       0.90      0.81      0.85      2546\n",
      "\n",
      "    accuracy                           0.81      3755\n",
      "   macro avg       0.78      0.81      0.79      3755\n",
      "weighted avg       0.82      0.81      0.81      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now try a support vector machine classifier, called LinearSVC in ScikitLearn. \n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "svc_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('svc', LinearSVC())\n",
    "])\n",
    "\n",
    "svc_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'svc__C' : [1.0, 10.0, 100.0, 1000.0]\n",
    "}\n",
    "\n",
    "svcResult_V1 = grid_search(svc_pipelineV1, svc_paramsV1)\n",
    "report_scores(svcResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   59.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1215 out of 1215 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('svc', LinearSVC())\n",
      "The best parameters found were: {'cv__max_features': 8000, 'cv__ngram_range': (1, 1), 'svc__C': 1.0}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.78      0.69      1209\n",
      "           1       0.88      0.77      0.82      2546\n",
      "\n",
      "    accuracy                           0.77      3755\n",
      "   macro avg       0.75      0.77      0.75      3755\n",
      "weighted avg       0.79      0.77      0.78      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now try a support vector machine classifier, called LinearSVC in ScikitLearn. \n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "svc_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer()), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('svc', LinearSVC())\n",
    "])\n",
    "\n",
    "svc_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,10)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,10)],\n",
    "    'svc__C' : [1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "svcResult_V2 = grid_search(svc_pipelineV2, svc_paramsV2)\n",
    "report_scores(svcResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 125 candidates, totalling 625 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed: 26.7min\n",
      "[Parallel(n_jobs=-1)]: Done 625 out of 625 | elapsed: 28.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('rf', RandomForestClassifier())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 2), 'rf__n_estimators': 300}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.07      0.11      1209\n",
      "           1       0.68      0.93      0.79      2546\n",
      "\n",
      "    accuracy                           0.65      3755\n",
      "   macro avg       0.50      0.50      0.45      3755\n",
      "weighted avg       0.56      0.65      0.57      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now try the RandomForest algorithm,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "# do this overnight :/ hehe\n",
    "\n",
    "rf_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "rf_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'rf__n_estimators' : [100 * x for x in range (1,6)]\n",
    "    #'rf__max_depth' : [25, 50, 75, None],\n",
    "    #'rf__max_features' : [\"auto\", \"sqrt\"],\n",
    "    #'rf__min_samples_leaf' : [1,2,4],\n",
    "    #'rf_min_samlpes_split' : [2, 5, 10],\n",
    "    #'rf__bootstrap' : [True, False]\n",
    "}\n",
    "\n",
    "rfResult_V1 = grid_search(rf_pipelineV1, rf_paramsV1)\n",
    "report_scores(rfResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6000 candidates, totalling 30000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed: 22.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 31.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 36.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3597 tasks      | elapsed: 43.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 47.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4497 tasks      | elapsed: 53.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 60.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5497 tasks      | elapsed: 68.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 76.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6597 tasks      | elapsed: 82.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 89.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7797 tasks      | elapsed: 96.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 104.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed: 112.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 122.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10497 tasks      | elapsed: 131.2min\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed: 141.9min\n",
      "[Parallel(n_jobs=-1)]: Done 11997 tasks      | elapsed: 154.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed: 162.3min\n",
      "[Parallel(n_jobs=-1)]: Done 13597 tasks      | elapsed: 172.3min\n",
      "[Parallel(n_jobs=-1)]: Done 14434 tasks      | elapsed: 184.9min\n",
      "[Parallel(n_jobs=-1)]: Done 15297 tasks      | elapsed: 195.0min\n",
      "[Parallel(n_jobs=-1)]: Done 16184 tasks      | elapsed: 209.4min\n",
      "[Parallel(n_jobs=-1)]: Done 17097 tasks      | elapsed: 221.5min\n",
      "[Parallel(n_jobs=-1)]: Done 18034 tasks      | elapsed: 237.0min\n",
      "[Parallel(n_jobs=-1)]: Done 18997 tasks      | elapsed: 247.0min\n",
      "[Parallel(n_jobs=-1)]: Done 19984 tasks      | elapsed: 260.6min\n",
      "[Parallel(n_jobs=-1)]: Done 20997 tasks      | elapsed: 276.3min\n",
      "[Parallel(n_jobs=-1)]: Done 22034 tasks      | elapsed: 289.9min\n",
      "[Parallel(n_jobs=-1)]: Done 23097 tasks      | elapsed: 306.5min\n",
      "[Parallel(n_jobs=-1)]: Done 24184 tasks      | elapsed: 323.6min\n",
      "[Parallel(n_jobs=-1)]: Done 25297 tasks      | elapsed: 338.6min\n",
      "[Parallel(n_jobs=-1)]: Done 26434 tasks      | elapsed: 355.3min\n",
      "[Parallel(n_jobs=-1)]: Done 27597 tasks      | elapsed: 373.1min\n",
      "[Parallel(n_jobs=-1)]: Done 28784 tasks      | elapsed: 391.8min\n",
      "[Parallel(n_jobs=-1)]: Done 30000 out of 30000 | elapsed: 411.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('rf', RandomForestClassifier())\n",
      "The best parameters found were: {'cv__max_features': 4000, 'cv__ngram_range': (1, 1), 'rf__bootstrap': True, 'rf__max_depth': None, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 2, 'rf__n_estimators': 100}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.03      0.05      1209\n",
      "           1       0.68      0.97      0.80      2546\n",
      "\n",
      "    accuracy                           0.67      3755\n",
      "   macro avg       0.51      0.50      0.43      3755\n",
      "weighted avg       0.57      0.67      0.56      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DO THIS OVERNIGHT!!\n",
    "\n",
    "# Now try the RandomForest algorithm,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "# do this overnight :/ hehe\n",
    "\n",
    "rf_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer()), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "rf_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'rf__n_estimators' : [100 * x for x in range (1,6)],\n",
    "    'rf__max_depth' : [25, 50, 75, None],\n",
    "    'rf__max_features' : [\"auto\", \"sqrt\"],\n",
    "    'rf__min_samples_leaf' : [1,2,4],\n",
    "    'rf__bootstrap' : [True, False]\n",
    "}\n",
    "\n",
    "rfResult_V2 = grid_search(rf_pipelineV2, rf_paramsV2)\n",
    "report_scores(rfResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2160 candidates, totalling 10800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed: 22.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 35.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed: 38.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 56.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 63.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 85.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3597 tasks      | elapsed: 119.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 127.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4497 tasks      | elapsed: 160.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 209.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5497 tasks      | elapsed: 254.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 318.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6597 tasks      | elapsed: 370.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 446.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7797 tasks      | elapsed: 507.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 593.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed: 660.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 756.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10497 tasks      | elapsed: 828.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10800 out of 10800 | elapsed: 922.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('rf', RandomForestClassifier())\n",
      "The best parameters found were: {'cv__ngram_range': (1, 9), 'rf__bootstrap': False, 'rf__max_depth': None, 'rf__max_features': 'auto', 'rf__min_samples_leaf': 2, 'rf__n_estimators': 300}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.05      0.08      1209\n",
      "           1       0.68      0.95      0.79      2546\n",
      "\n",
      "    accuracy                           0.66      3755\n",
      "   macro avg       0.50      0.50      0.44      3755\n",
      "weighted avg       0.56      0.66      0.56      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DO THIS OVERNIGHT!!\n",
    "\n",
    "# Now try the RandomForest algorithm,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "# do this overnight :/ hehe\n",
    "\n",
    "rf_pipelineV3 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "rf_paramsV3 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,10)],\n",
    "    'rf__n_estimators' : [100 * x for x in range (1,6)],\n",
    "    'rf__max_depth' : [25, 50, 75, None],\n",
    "    'rf__max_features' : [\"auto\", \"sqrt\"],\n",
    "    'rf__min_samples_leaf' : [1,2,4],\n",
    "    'rf__bootstrap' : [True, False]\n",
    "}\n",
    "\n",
    "rfResult_V3 = grid_search(rf_pipelineV3, rf_paramsV3)\n",
    "report_scores(rfResult_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 1), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.78      0.55      1209\n",
      "           1       0.82      0.49      0.62      2546\n",
      "\n",
      "    accuracy                           0.58      3755\n",
      "   macro avg       0.62      0.64      0.58      3755\n",
      "weighted avg       0.69      0.58      0.59      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do the multinomial naive bayes algorithm with a tfidf vectorizer instead of a countvectorizer,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV1 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "mNBtResult_V1 = grid_search(mNBt_pipelineV1, mNBt_paramsV1)\n",
    "report_scores(mNBtResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 9000, 'cv__ngram_range': (1, 2), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.78      0.55      1209\n",
      "           1       0.82      0.49      0.62      2546\n",
      "\n",
      "    accuracy                           0.58      3755\n",
      "   macro avg       0.62      0.63      0.58      3755\n",
      "weighted avg       0.69      0.58      0.59      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV2 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,11)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,10)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "mNBtResult_V2 = grid_search(mNBt_pipelineV2, mNBt_paramsV2)\n",
    "report_scores(mNBtResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 70 candidates, totalling 350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 350 out of 350 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 14000, 'cv__ngram_range': (1, 2), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.77      0.56      1209\n",
      "           1       0.83      0.55      0.66      2546\n",
      "\n",
      "    accuracy                           0.62      3755\n",
      "   macro avg       0.64      0.66      0.61      3755\n",
      "weighted avg       0.71      0.62      0.63      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV3 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV3 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (8,15)],\n",
    "    'nb__alpha' : [i for i in range (1,3)]\n",
    "}\n",
    "\n",
    "mNBtResult_V3 = grid_search(mNBt_pipelineV3, mNBt_paramsV3)\n",
    "report_scores(mNBtResult_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 105 candidates, totalling 525 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   37.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 525 out of 525 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 18000, 'cv__ngram_range': (1, 2), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.76      0.64      1209\n",
      "           1       0.86      0.71      0.78      2546\n",
      "\n",
      "    accuracy                           0.72      3755\n",
      "   macro avg       0.71      0.74      0.71      3755\n",
      "weighted avg       0.76      0.72      0.73      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV4 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV4 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (13,20)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "mNBtResult_V4 = grid_search(mNBt_pipelineV4, mNBt_paramsV4)\n",
    "report_scores(mNBtResult_V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 70 candidates, totalling 350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 350 out of 350 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 22000, 'cv__ngram_range': (1, 4), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      1209\n",
      "           1       0.86      0.72      0.78      2546\n",
      "\n",
      "    accuracy                           0.73      3755\n",
      "   macro avg       0.71      0.73      0.71      3755\n",
      "weighted avg       0.76      0.73      0.74      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV5 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV5 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (17,24)],\n",
    "    'nb__alpha' : [i for i in range (1,3)]\n",
    "}\n",
    "\n",
    "mNBtResult_V5 = grid_search(mNBt_pipelineV5, mNBt_paramsV5)\n",
    "report_scores(mNBtResult_V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   55.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 12.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 22000, 'cv__ngram_range': (1, 4), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      1209\n",
      "           1       0.86      0.72      0.78      2546\n",
      "\n",
      "    accuracy                           0.73      3755\n",
      "   macro avg       0.71      0.73      0.71      3755\n",
      "weighted avg       0.76      0.73      0.74      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV6 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV6 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (3,11)],\n",
    "    'cv__max_features' : [1000 * i for i in range (21,27)],\n",
    "    'nb__alpha' : [i for i in range (1,3)]\n",
    "}\n",
    "\n",
    "mNBtResult_V6 = grid_search(mNBt_pipelineV6, mNBt_paramsV6)\n",
    "report_scores(mNBtResult_V6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 132 candidates, totalling 660 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   49.2s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 660 out of 660 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__ngram_range': (1, 9), 'cv__norm': 'l2', 'nb__alpha': 0.25}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73      1209\n",
      "           1       0.87      0.87      0.87      2546\n",
      "\n",
      "    accuracy                           0.83      3755\n",
      "   macro avg       0.80      0.80      0.80      3755\n",
      "weighted avg       0.83      0.83      0.83      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV7 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    #('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV7 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (9,20)],\n",
    "    'cv__norm' : [\"l1\", \"l2\"],\n",
    "    'nb__alpha' : [0.25 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "mNBtResult_V7 = grid_search(mNBt_pipelineV7, mNBt_paramsV7)\n",
    "report_scores(mNBtResult_V7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   50.4s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 5), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.85      0.53      1209\n",
      "           1       0.84      0.36      0.51      2546\n",
      "\n",
      "    accuracy                           0.52      3755\n",
      "   macro avg       0.61      0.61      0.52      3755\n",
      "weighted avg       0.69      0.52      0.51      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do the multinomial naive bayes algorithm with a tfidf vectorizer instead of a countvectorizer,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "cNBt_pipelineV1 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNBt_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "cNBtResult_V1 = grid_search(cNBt_pipelineV1, cNBt_paramsV1)\n",
    "report_scores(cNBtResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 147 candidates, totalling 735 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   28.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   55.0s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 735 out of 735 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 10000, 'cv__ngram_range': (1, 7), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.84      0.63      1209\n",
      "           1       0.89      0.61      0.72      2546\n",
      "\n",
      "    accuracy                           0.68      3755\n",
      "   macro avg       0.70      0.72      0.68      3755\n",
      "weighted avg       0.76      0.68      0.69      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do the multinomial naive bayes algorithm with a tfidf vectorizer instead of a countvectorizer,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "cNBt_pipelineV2 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNBt_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (4,11)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,11)],\n",
    "    'nb__alpha' : [i for i in range (2,5)]\n",
    "}\n",
    "\n",
    "cNBtResult_V2 = grid_search(cNBt_pipelineV2, cNBt_paramsV2)\n",
    "report_scores(cNBtResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   42.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 13000, 'cv__ngram_range': (1, 11), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.83      0.74      1209\n",
      "           1       0.91      0.80      0.85      2546\n",
      "\n",
      "    accuracy                           0.81      3755\n",
      "   macro avg       0.78      0.81      0.79      3755\n",
      "weighted avg       0.83      0.81      0.81      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do the multinomial naive bayes algorithm with a tfidf vectorizer instead of a countvectorizer,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "cNBt_pipelineV3 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNBt_paramsV3 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (6,14)],\n",
    "    'cv__max_features' : [1000 * i for i in range (9,15)],\n",
    "    'nb__alpha' : [i for i in range (1,5)]\n",
    "}\n",
    "\n",
    "cNBtResult_V3 = grid_search(cNBt_pipelineV3, cNBt_paramsV3)\n",
    "report_scores(cNBtResult_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__ngram_range': (1, 13), 'cv__norm': 'l2', 'nb__alpha': 1.0}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73      1209\n",
      "           1       0.89      0.81      0.85      2546\n",
      "\n",
      "    accuracy                           0.81      3755\n",
      "   macro avg       0.78      0.80      0.79      3755\n",
      "weighted avg       0.82      0.81      0.81      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRY WITHOUTTT SPECIFYING MAX FEATURES\n",
    "\n",
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "# (its not actually version 7 im just too lazy to think of another name yet)\n",
    "\n",
    "cNBt_pipelineV4 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    #('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNBt_paramsV4 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (5,15)],\n",
    "    'cv__norm' : [\"l1\", \"l2\"],\n",
    "    'nb__alpha' : [0.25 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "cNBtResult_V4 = grid_search(cNBt_pipelineV4, cNBt_paramsV4)\n",
    "report_scores(cNBtResult_V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 154 candidates, totalling 770 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   50.6s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 770 out of 770 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__ngram_range': (1, 15), 'cv__norm': 'l2', 'nb__alpha': 1.0}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73      1209\n",
      "           1       0.89      0.81      0.85      2546\n",
      "\n",
      "    accuracy                           0.81      3755\n",
      "   macro avg       0.78      0.81      0.79      3755\n",
      "weighted avg       0.82      0.81      0.81      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRY WITHOUTTT SPECIFYING MAX FEATURES\n",
    "\n",
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "# (its not actually version 7 im just too lazy to think of another name yet)\n",
    "\n",
    "cNBt_pipelineV5 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    #('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNBt_paramsV5 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (11,18)],\n",
    "    'cv__norm' : [\"l1\", \"l2\"],\n",
    "    'nb__alpha' : [0.25 * i for i in range (1,4 * 3)]\n",
    "}\n",
    "\n",
    "cNBtResult_V5 = grid_search(cNBt_pipelineV5, cNBt_paramsV5)\n",
    "report_scores(cNBtResult_V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psvc_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    #('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('psvc', SVC())\n",
    "])\n",
    "\n",
    "psvc_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (11,18)],\n",
    "    'cv__norm' : [\"l1\", \"l2\"],\n",
    "    'psvc__alpha' : [0.25 * i for i in range (1,4 * 3)]\n",
    "}\n",
    "\n",
    "psvcResult_V1 = grid_search(psvc_pipelineV1, psvc_paramsV1)\n",
    "report_scores(psvcResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 625 candidates, totalling 3125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3125 out of 3125 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('sgd', SGDClassifier(n_jobs=-1, penalty='elasticnet'))\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 1), 'sgd__alpha': 0.0001, 'sgd__l1_ratio': 0.75}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78      1209\n",
      "           1       0.89      0.89      0.89      2546\n",
      "\n",
      "    accuracy                           0.86      3755\n",
      "   macro avg       0.83      0.84      0.84      3755\n",
      "weighted avg       0.86      0.86      0.86      3755\n",
      "\n",
      "f1 score: 0.8937340404635631\n",
      "precision: 0.8939096267190569\n",
      "recall: 0.8935585231736056\n"
     ]
    }
   ],
   "source": [
    "sgd_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    #('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('sgd', SGDClassifier(n_jobs = -1, penalty = \"elasticnet\"))\n",
    "])\n",
    "\n",
    "sgd_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    \"sgd__alpha\" : [10 ** x for x in range (-4, 1)],\n",
    "    \"sgd__l1_ratio\" : [0, 0.25, 0.5, 0.75, 1]\n",
    "    \n",
    "}\n",
    "\n",
    "sgdResult_V1 = grid_search(sgd_pipelineV1, sgd_paramsV1)\n",
    "report_scores(sgdResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 630 candidates, totalling 3150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3150 out of 3150 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('sgd', SGDClassifier(n_jobs=-1, penalty='elasticnet'))\n",
      "The best parameters found were: {'cv__max_features': 7000, 'cv__ngram_range': (1, 1), 'sgd__alpha': 0.0001, 'sgd__l1_ratio': 0.5}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74      1209\n",
      "           1       0.88      0.87      0.87      2546\n",
      "\n",
      "    accuracy                           0.83      3755\n",
      "   macro avg       0.81      0.81      0.81      3755\n",
      "weighted avg       0.83      0.83      0.83      3755\n",
      "\n",
      "f1 score: 0.8743320799525034\n",
      "precision: 0.8811328280813722\n",
      "recall: 0.8676355066771406\n"
     ]
    }
   ],
   "source": [
    "sgd_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    #('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('sgd', SGDClassifier(n_jobs = -1, penalty = \"elasticnet\"))\n",
    "])\n",
    "\n",
    "sgd_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,7)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,11)],\n",
    "    \"sgd__alpha\" : [10 ** x for x in range (-5, -2)],\n",
    "    \"sgd__l1_ratio\" : [0, 0.25, 0.5, 0.75, 1]\n",
    "    \n",
    "}\n",
    "\n",
    "sgdResult_V2 = grid_search(sgd_pipelineV2, sgd_paramsV2)\n",
    "report_scores(sgdResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:   46.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('sgd', SGDClassifier(n_jobs=-1, penalty='elasticnet'))\n",
      "The best parameters found were: {'cv__ngram_range': (1, 1), 'sgd__alpha': 0.0001, 'sgd__l1_ratio': 0.5}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.72      1209\n",
      "           1       0.86      0.87      0.87      2546\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.79      0.79      0.79      3755\n",
      "weighted avg       0.82      0.82      0.82      3755\n",
      "\n",
      "f1 score: 0.8660399529964747\n",
      "precision: 0.863671875\n",
      "recall: 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "sgdt_pipelineV1 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    #('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('sgd', SGDClassifier(n_jobs = -1, penalty = \"elasticnet\"))\n",
    "])\n",
    "\n",
    "sgdt_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    #'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    \"sgd__alpha\" : [10 ** x for x in range (-5, -2)],\n",
    "    \"sgd__l1_ratio\" : [0, 0.25, 0.5, 0.75, 1]\n",
    "    \n",
    "}\n",
    "\n",
    "sgdtResult_V1 = grid_search(sgdt_pipelineV1, sgdt_paramsV1)\n",
    "report_scores(sgdtResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      1209\n",
      "           1       0.89      0.84      0.87      2546\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.80      0.81      0.80      3755\n",
      "weighted avg       0.83      0.82      0.83      3755\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      " [[ 951  258]\n",
      " [ 408 2138]]\n"
     ]
    }
   ],
   "source": [
    "#MultinomialNB with optimal parameters confusion matrix \n",
    "\n",
    "mnb_CM_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(max_features = 14000, ngram_range = (1,8))), \n",
    "     (\"nb\", MultinomialNB(alpha = 4))\n",
    "])\n",
    "\n",
    "mnb_CM_pipe.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "tup = simplified_testing.label.astype(\"int\"), mnb_CM_pipe.predict(simplified_testing.tokens) \n",
    "print(classification_report(tup[0], tup[1]))\n",
    "print(\"CONFUSION MATRIX:\\n\\n\", confusion_matrix(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.85      0.62      1209\n",
      "           1       0.89      0.58      0.70      2546\n",
      "\n",
      "    accuracy                           0.66      3755\n",
      "   macro avg       0.69      0.71      0.66      3755\n",
      "weighted avg       0.76      0.66      0.67      3755\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      " [[1022  187]\n",
      " [1074 1472]]\n"
     ]
    }
   ],
   "source": [
    "#ComplementNB with optimal parameters confusion matrix \n",
    "\n",
    "cnb_CM_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(max_features = 14000, ngram_range = (1,11))), \n",
    "     (\"nb\", ComplementNB(alpha = 2))\n",
    "])\n",
    "\n",
    "cnb_CM_pipe.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "tup = simplified_testing.label.astype(\"int\"), cnb_CM_pipe.predict(simplified_testing.tokens) \n",
    "print(classification_report(tup[0], tup[1]))\n",
    "print(\"CONFUSION MATRIX:\\n\\n\", confusion_matrix(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.72      1209\n",
      "           1       0.86      0.87      0.87      2546\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.79      0.79      0.79      3755\n",
      "weighted avg       0.82      0.82      0.82      3755\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      " [[ 863  346]\n",
      " [ 331 2215]]\n"
     ]
    }
   ],
   "source": [
    "#BernoulliNB with optimal parameters confusion matrix \n",
    "\n",
    "bnb_CM_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(max_features = 5000, ngram_range = (1,2))), \n",
    "     (\"nb\", BernoulliNB(alpha = 2))\n",
    "])\n",
    "\n",
    "bnb_CM_pipe.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "tup = simplified_testing.label.astype(\"int\"), bnb_CM_pipe.predict(simplified_testing.tokens) \n",
    "print(classification_report(tup[0], tup[1]))\n",
    "print(\"CONFUSION MATRIX:\\n\\n\", confusion_matrix(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.82      0.73      1209\n",
      "           1       0.90      0.79      0.84      2546\n",
      "\n",
      "    accuracy                           0.80      3755\n",
      "   macro avg       0.78      0.81      0.79      3755\n",
      "weighted avg       0.82      0.80      0.81      3755\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      " [[ 996  213]\n",
      " [ 534 2012]]\n"
     ]
    }
   ],
   "source": [
    "#LinearSVC with optimal parameters confusion matrix \n",
    "\n",
    "svc_CM_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(max_features = 3000, ngram_range = (1,1))), \n",
    "     (\"nb\", LinearSVC(C = 1))\n",
    "])\n",
    "\n",
    "svc_CM_pipe.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "tup = simplified_testing.label.astype(\"int\"), svc_CM_pipe.predict(simplified_testing.tokens) \n",
    "print(classification_report(tup[0], tup[1]))\n",
    "print(\"CONFUSION MATRIX:\\n\\n\", confusion_matrix(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.06      0.11      1209\n",
      "           1       0.68      0.94      0.79      2546\n",
      "\n",
      "    accuracy                           0.66      3755\n",
      "   macro avg       0.52      0.50      0.45      3755\n",
      "weighted avg       0.58      0.66      0.57      3755\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      " [[  78 1131]\n",
      " [ 141 2405]]\n"
     ]
    }
   ],
   "source": [
    "#Random Forest with optimal parameters confusion matrix \n",
    "\n",
    "rf_CM_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(max_features = 5000, ngram_range = (1,2))), \n",
    "     (\"nb\", RandomForestClassifier(n_estimators = 300))\n",
    "])\n",
    "\n",
    "rf_CM_pipe.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "tup = simplified_testing.label.astype(\"int\"), rf_CM_pipe.predict(simplified_testing.tokens) \n",
    "print(classification_report(tup[0], tup[1]))\n",
    "print(\"CONFUSION MATRIX:\\n\\n\", confusion_matrix(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.77      0.66      1209\n",
      "           1       0.87      0.73      0.80      2546\n",
      "\n",
      "    accuracy                           0.75      3755\n",
      "   macro avg       0.73      0.75      0.73      3755\n",
      "weighted avg       0.78      0.75      0.75      3755\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      " [[ 934  275]\n",
      " [ 677 1869]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\George\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression with optimal parameters confusion matrix \n",
    "\n",
    "lr_CM_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(max_features = 11000, ngram_range = (1,1))), \n",
    "    (\"nb\", LogisticRegression(C = 1))\n",
    "])\n",
    "\n",
    "lr_CM_pipe.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "tup = simplified_testing.label.astype(\"int\"), lr_CM_pipe.predict(simplified_testing.tokens) \n",
    "print(classification_report(tup[0], tup[1]))\n",
    "print(\"CONFUSION MATRIX:\\n\\n\", confusion_matrix(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73      1209\n",
      "           1       0.87      0.87      0.87      2546\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.80      0.80      0.80      3755\n",
      "weighted avg       0.82      0.82      0.82      3755\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      " [[ 881  328]\n",
      " [ 330 2216]]\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes using TfidfVectorizer with optimal parameters confusion matrix\n",
    "\n",
    "mnbt_CM_pipe = Pipeline([\n",
    "    (\"cv\", TfidfVectorizer(ngram_range = (1,9))), \n",
    "    (\"nb\", MultinomialNB(alpha = 0.25))\n",
    "])\n",
    "\n",
    "mnbt_CM_pipe.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "tup = simplified_testing.label.astype(\"int\"), mnbt_CM_pipe.predict(simplified_testing.tokens) \n",
    "print(classification_report(tup[0], tup[1]))\n",
    "print(\"CONFUSION MATRIX:\\n\\n\", confusion_matrix(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73      1209\n",
      "           1       0.89      0.81      0.85      2546\n",
      "\n",
      "    accuracy                           0.81      3755\n",
      "   macro avg       0.78      0.81      0.79      3755\n",
      "weighted avg       0.82      0.81      0.81      3755\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      " [[ 964  245]\n",
      " [ 472 2074]]\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes using TfidfVectorizer with optimal parameters confusion matrix\n",
    "\n",
    "cnbt_CM_pipe = Pipeline([\n",
    "    (\"cv\", TfidfVectorizer(ngram_range = (1,15))), \n",
    "    (\"nb\", ComplementNB(alpha = 1))\n",
    "])\n",
    "\n",
    "cnbt_CM_pipe.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "tup = simplified_testing.label.astype(\"int\"), cnbt_CM_pipe.predict(simplified_testing.tokens) \n",
    "print(classification_report(tup[0], tup[1]))\n",
    "print(\"CONFUSION MATRIX:\\n\\n\", confusion_matrix(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW LETS TRY SOME ENSEMBLE LEARNING!\n",
    "\n",
    "# FIRST LETS TRY Random Forest, LinearSVC, LogisticRegression and the Multinomial NB using tfidf\n",
    "\n",
    "svc_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,1), max_features = 23000)),\n",
    "    (\"svc\", LinearSVC(C = 1))\n",
    "])\n",
    "\n",
    "bnb_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,2), max_features = 5000)),\n",
    "    (\"bnb\", BernoulliNB(alpha = 2))\n",
    "])\n",
    "\n",
    "mnb_pipe = Pipeline([\n",
    "    (\"tf\", TfidfVectorizer(ngram_range = (1,9))),\n",
    "    (\"mnb\", MultinomialNB(alpha = 0.25))\n",
    "])\n",
    "\n",
    "sgd_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,1), max_features = 5000)), \n",
    "    (\"sgdcf\", SGDClassifier(alpha = 0.0001, l1_ratio = 0.75, penalty = \"elasticnet\", random_state = 1))\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,1), max_features = 18000)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators = 800, max_depth = None, max_features = \"auto\", bootstrap = True, random_state = 1, n_jobs = -1))\n",
    "])\n",
    "\n",
    "vcf1 = VotingClassifier(estimators = [\n",
    "    (\"rf\", rf_pipe),\n",
    "    (\"svc\", svc_pipe),\n",
    "    #(\"bnb\", bnb_pipe), #this one is is more opposite to random forest so comment out lsvc for now\n",
    "    #(\"lr\", lr_pipe),\n",
    "    (\"mnbt\", mnb_pipe), #use the naive bayes classifier with tfidf as it has equal precision and recall\n",
    "    (\"sgd\", sgd_pipe)\n",
    "], voting = \"hard\")\n",
    "\n",
    "#vcf1.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "#print(classification_report(simplified_testing.label.astype(\"int\"), vcf1.predict(simplified_testing.tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    output = []\n",
    "    for i in range(1 << x):\n",
    "        subset = [s[j] for j in range(x) if (i & (1 << j))]\n",
    "        #we are only interested in pairs so disregard singletons\n",
    "        if(len(subset) > 1):\n",
    "            output.append(subset)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))])],\n",
       " [Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])],\n",
       " [Pipeline(steps=[('cv', CountVectorizer(max_features=23000)),\n",
       "                  ('svc', LinearSVC(C=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
       "                  ('bnb', BernoulliNB(alpha=2))]),\n",
       "  Pipeline(steps=[('tf', TfidfVectorizer(ngram_range=(1, 9))),\n",
       "                  ('mnb', MultinomialNB(alpha=0.25))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=5000)),\n",
       "                  ('sgdcf',\n",
       "                   SGDClassifier(l1_ratio=0.75, penalty='elasticnet',\n",
       "                                 random_state=1))]),\n",
       "  Pipeline(steps=[('cv', CountVectorizer(max_features=18000)),\n",
       "                  ('rf',\n",
       "                   RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
       "                                          random_state=1))])]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = powerset([svc_pipe, bnb_pipe, mnb_pipe, sgd_pipe, rf_pipe])\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the name is: svc\n",
      "the name is: mnb\n",
      "the names are: ['svc', 'mnb']\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "\n",
    "for pipeline in group:\n",
    "    name = pipeline.steps[1][0]\n",
    "    names.append(name)\n",
    "    print(\"the name is:\",name)\n",
    "    \n",
    "print(\"the names are:\",names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_voting_combinations():\n",
    "    possibilities = powerset([svc_pipe, bnb_pipe, mnb_pipe, sgd_pipe, rf_pipe])\n",
    "    j = 0\n",
    "    \n",
    "    #store the combinations and their respective scores in the same order so it can be easily copied into a spreadsheet\n",
    "    combinations = []\n",
    "    f1s = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for group in possibilities:\n",
    "        print(\"-- COMBINATION\",str(j),\"--\\n\")\n",
    "        model_names = []\n",
    "        for pipeline in group:\n",
    "            #get the names of the models in the combinations so we can print it\n",
    "            name = pipeline.steps[1][0]\n",
    "            model_names.append(name)\n",
    "        for_voter = []\n",
    "        for i in range(len(group)):\n",
    "            for_voter.append((str(i), group[i]))\n",
    "        vcf = VotingClassifier(estimators = for_voter, voting = \"hard\")\n",
    "        vcf.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "        true = simplified_testing.label.astype(\"int\")\n",
    "        predictions = vcf.predict(simplified_testing.tokens)\n",
    "        print(\"combination tried:\",model_names)\n",
    "        combinations.append(model_names)\n",
    "        f1 = f1_score(true, predictions)\n",
    "        f1s.append(f1)\n",
    "        precision = precision_score(true, predictions)\n",
    "        precisions.append(precision)\n",
    "        recall = recall_score(true, predictions)\n",
    "        recalls.append(recall)\n",
    "        print(\"f1 score:\",f1,\"| precision:\",precision,\"| recall:\",recall,\"\\n\")\n",
    "        j = j + 1\n",
    "        \n",
    "    return combinations, f1s, precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possibilities = powerset([svc_pipe, bnb_pipe, mnb_pipe, sgd_pipe, rf_pipe])\n",
    "len(possibilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- COMBINATION 0 --\n",
      "\n",
      "combination tried: ['svc', 'bnb']\n",
      "f1 score: 0.775224337929525 | precision: 0.8754325259515571 | recall: 0.6956009426551453 \n",
      "\n",
      "-- COMBINATION 1 --\n",
      "\n",
      "combination tried: ['svc', 'mnb']\n",
      "f1 score: 0.75284661754856 | precision: 0.8722193481634765 | recall: 0.6622152395915161 \n",
      "\n",
      "-- COMBINATION 2 --\n",
      "\n",
      "combination tried: ['bnb', 'mnb']\n",
      "f1 score: 0.831046487814868 | precision: 0.8682071031236628 | recall: 0.7969363707776905 \n",
      "\n",
      "-- COMBINATION 3 --\n",
      "\n",
      "combination tried: ['svc', 'bnb', 'mnb']\n",
      "f1 score: 0.8700098328416913 | precision: 0.8712091374556912 | recall: 0.8688138256087982 \n",
      "\n",
      "-- COMBINATION 4 --\n",
      "\n",
      "combination tried: ['svc', 'sgdcf']\n",
      "f1 score: 0.7484333034914951 | precision: 0.8699271592091571 | recall: 0.6567164179104478 \n",
      "\n",
      "-- COMBINATION 5 --\n",
      "\n",
      "combination tried: ['bnb', 'sgdcf']\n",
      "f1 score: 0.8089413749472797 | precision: 0.8734061930783242 | recall: 0.753338570306363 \n",
      "\n",
      "-- COMBINATION 6 --\n",
      "\n",
      "combination tried: ['svc', 'bnb', 'sgdcf']\n",
      "f1 score: 0.8524126791843327 | precision: 0.8770253427503116 | recall: 0.8291437549096622 \n",
      "\n",
      "-- COMBINATION 7 --\n",
      "\n",
      "combination tried: ['mnb', 'sgdcf']\n",
      "f1 score: 0.8251719108147529 | precision: 0.8788282290279628 | recall: 0.7776904948939513 \n",
      "\n",
      "-- COMBINATION 8 --\n",
      "\n",
      "combination tried: ['svc', 'mnb', 'sgdcf']\n",
      "f1 score: 0.8576025744167338 | precision: 0.8788128606760099 | recall: 0.8373919874312648 \n",
      "\n",
      "-- COMBINATION 9 --\n",
      "\n",
      "combination tried: ['bnb', 'mnb', 'sgdcf']\n",
      "f1 score: 0.8797665369649805 | precision: 0.8716268311488049 | recall: 0.8880597014925373 \n",
      "\n",
      "-- COMBINATION 10 --\n",
      "\n",
      "combination tried: ['svc', 'bnb', 'mnb', 'sgdcf']\n",
      "f1 score: 0.8317853457172343 | precision: 0.8764680295780775 | recall: 0.7914375490966221 \n",
      "\n",
      "-- COMBINATION 11 --\n",
      "\n",
      "combination tried: ['svc', 'rf']\n",
      "f1 score: 0.7932379713914175 | precision: 0.8849129593810445 | recall: 0.7187745483110762 \n",
      "\n",
      "-- COMBINATION 12 --\n",
      "\n",
      "combination tried: ['bnb', 'rf']\n",
      "f1 score: 0.864746772591857 | precision: 0.8746484531940538 | recall: 0.8550667714061273 \n",
      "\n",
      "-- COMBINATION 13 --\n",
      "\n",
      "combination tried: ['svc', 'bnb', 'rf']\n",
      "f1 score: 0.873491630984819 | precision: 0.8657407407407407 | recall: 0.8813825608798115 \n",
      "\n",
      "-- COMBINATION 14 --\n",
      "\n",
      "combination tried: ['mnb', 'rf']\n",
      "f1 score: 0.8693577251938757 | precision: 0.8803866290777286 | recall: 0.8586017282010998 \n",
      "\n",
      "-- COMBINATION 15 --\n",
      "\n",
      "combination tried: ['svc', 'mnb', 'rf']\n",
      "f1 score: 0.8950191570881226 | precision: 0.8735976065818998 | recall: 0.9175176747839748 \n",
      "\n",
      "-- COMBINATION 16 --\n",
      "\n",
      "combination tried: ['bnb', 'mnb', 'rf']\n",
      "f1 score: 0.8999053926206243 | precision: 0.8682000730193501 | recall: 0.9340141398271798 \n",
      "\n",
      "-- COMBINATION 17 --\n",
      "\n",
      "combination tried: ['svc', 'bnb', 'mnb', 'rf']\n",
      "f1 score: 0.866403162055336 | precision: 0.8719172633253779 | recall: 0.8609583660644148 \n",
      "\n",
      "-- COMBINATION 18 --\n",
      "\n",
      "combination tried: ['sgdcf', 'rf']\n",
      "f1 score: 0.850204081632653 | precision: 0.8848768054375531 | recall: 0.8181461115475255 \n",
      "\n",
      "-- COMBINATION 19 --\n",
      "\n",
      "combination tried: ['svc', 'sgdcf', 'rf']\n",
      "f1 score: 0.8815221655551196 | precision: 0.8804858934169278 | recall: 0.882560879811469 \n",
      "\n",
      "-- COMBINATION 20 --\n",
      "\n",
      "combination tried: ['bnb', 'sgdcf', 'rf']\n",
      "f1 score: 0.8965911254999048 | precision: 0.8702402957486137 | recall: 0.9245875883739199 \n",
      "\n",
      "-- COMBINATION 21 --\n",
      "\n",
      "combination tried: ['svc', 'bnb', 'sgdcf', 'rf']\n",
      "f1 score: 0.8518293915504346 | precision: 0.8775510204081632 | recall: 0.8275726630007856 \n",
      "\n",
      "-- COMBINATION 22 --\n",
      "\n",
      "combination tried: ['mnb', 'sgdcf', 'rf']\n",
      "f1 score: 0.8861882716049383 | precision: 0.8707354056103108 | recall: 0.9021995286724274 \n",
      "\n",
      "-- COMBINATION 23 --\n",
      "\n",
      "combination tried: ['svc', 'mnb', 'sgdcf', 'rf']\n",
      "f1 score: 0.8570277889649618 | precision: 0.8793388429752066 | recall: 0.835820895522388 \n",
      "\n",
      "-- COMBINATION 24 --\n",
      "\n",
      "combination tried: ['bnb', 'mnb', 'sgdcf', 'rf']\n",
      "f1 score: 0.8765890866418933 | precision: 0.8730035060381769 | recall: 0.8802042419481539 \n",
      "\n",
      "-- COMBINATION 25 --\n",
      "\n",
      "combination tried: ['svc', 'bnb', 'mnb', 'sgdcf', 'rf']\n",
      "f1 score: 0.8930286153255234 | precision: 0.8737316798196166 | recall: 0.9131971720345641 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "combinations, f1s, precisions, recalls = try_all_voting_combinations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = combinations, f1s, precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6956009426551453,\n",
       " 0.6622152395915161,\n",
       " 0.7969363707776905,\n",
       " 0.8688138256087982,\n",
       " 0.6567164179104478,\n",
       " 0.753338570306363,\n",
       " 0.8291437549096622,\n",
       " 0.7776904948939513,\n",
       " 0.8373919874312648,\n",
       " 0.8880597014925373,\n",
       " 0.7914375490966221,\n",
       " 0.7187745483110762,\n",
       " 0.8550667714061273,\n",
       " 0.8813825608798115,\n",
       " 0.8586017282010998,\n",
       " 0.9175176747839748,\n",
       " 0.9340141398271798,\n",
       " 0.8609583660644148,\n",
       " 0.8181461115475255,\n",
       " 0.882560879811469,\n",
       " 0.9245875883739199,\n",
       " 0.8275726630007856,\n",
       " 0.9021995286724274,\n",
       " 0.835820895522388,\n",
       " 0.8802042419481539,\n",
       " 0.9131971720345641]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.06      0.10      1209\n",
      "           1       0.68      0.97      0.80      2546\n",
      "\n",
      "    accuracy                           0.68      3755\n",
      "   macro avg       0.58      0.51      0.45      3755\n",
      "weighted avg       0.61      0.68      0.58      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitted = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,1), max_features = 18000)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators = 800, max_depth = None, max_features = \"auto\", bootstrap = True, random_state = 1))\n",
    "])\n",
    "fitted.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "print(classification_report(simplified_testing.label.astype(\"int\"), fitted.predict(simplified_testing.tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79      1209\n",
      "           1       0.89      0.91      0.90      2546\n",
      "\n",
      "    accuracy                           0.87      3755\n",
      "   macro avg       0.85      0.84      0.85      3755\n",
      "weighted avg       0.87      0.87      0.87      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitted2 = Pipeline([\n",
    "        (\"cv\", CountVectorizer(ngram_range = (1,7), max_features = 23000)), \n",
    "        (\"sgdcf\", SGDClassifier(alpha = 0.0001, l1_ratio = 0.6, penalty = \"elasticnet\", random_state = 1))\n",
    "    ]).fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "print(classification_report(simplified_testing.label.astype(\"int\"), fitted2.predict(simplified_testing.tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted3 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78      1209\n",
      "           1       0.89      0.90      0.90      2546\n",
      "\n",
      "    accuracy                           0.86      3755\n",
      "   macro avg       0.84      0.84      0.84      3755\n",
      "weighted avg       0.86      0.86      0.86      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vcf = VotingClassifier([\n",
    "    (\"f2\", fitted2),\n",
    "    (\"f\", fitted)\n",
    "], voting = \"hard\", n_jobs = -1)\n",
    "vcf.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "print(classification_report(simplified_testing.label.astype(\"int\"), vcf.predict(simplified_testing.tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.74      0.78      1209\n",
      "           1       0.88      0.92      0.90      2546\n",
      "\n",
      "    accuracy                           0.87      3755\n",
      "   macro avg       0.85      0.83      0.84      3755\n",
      "weighted avg       0.86      0.87      0.86      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,7), max_features = 23000)), \n",
    "    (\"sgdcf\", SGDClassifier(alpha = 0.0001, l1_ratio = 0.6, penalty = \"elasticnet\", random_state = 1))\n",
    "])\n",
    "\n",
    "svc_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,1), max_features = 3000)),\n",
    "    (\"svc\", LinearSVC(C = 1))\n",
    "])\n",
    "\n",
    "mnb_pipe = Pipeline([\n",
    "    (\"tf\", TfidfVectorizer(ngram_range = (1,9))),\n",
    "    (\"mnb\", MultinomialNB(alpha = 0.25))\n",
    "])\n",
    "\n",
    "bnb_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,2), max_features = 5000, binary = True)),\n",
    "    (\"bnb\", BernoulliNB(alpha = 2))\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,1), max_features = 18000)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators = 800, max_depth = None, max_features = \"auto\", bootstrap = True, random_state = 1))\n",
    "])\n",
    "\n",
    "vcf3 = VotingClassifier([\n",
    "    (\"sdg\", sgd_pipe),\n",
    "    (\"svc\", svc_pipe),\n",
    "    (\"mnb\", mnb_pipe),\n",
    "    (\"bnb\", bnb_pipe),\n",
    "    (\"rf\", rf_pipe)\n",
    "], voting = \"hard\", n_jobs = -1)\n",
    "\n",
    "vcf3.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "print(classification_report(simplified_testing.label.astype(\"int\"), vcf3.predict(simplified_testing.tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78      1209\n",
      "           1       0.90      0.90      0.90      2546\n",
      "\n",
      "    accuracy                           0.86      3755\n",
      "   macro avg       0.84      0.84      0.84      3755\n",
      "weighted avg       0.86      0.86      0.86      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vcf4 = VotingClassifier([\n",
    "    (\"sdg\", sgd_pipe),\n",
    "    (\"svc\", svc_pipe),\n",
    "    (\"mnb\", mnb_pipe),\n",
    "    #(\"bnb\", bnb_pipe),\n",
    "    (\"rf\", rf_pipe)\n",
    "], voting = \"hard\", n_jobs = -1)\n",
    "\n",
    "vcf4.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "print(classification_report(simplified_testing.label.astype(\"int\"), vcf4.predict(simplified_testing.tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.78      1209\n",
      "           1       0.89      0.90      0.90      2546\n",
      "\n",
      "    accuracy                           0.86      3755\n",
      "   macro avg       0.84      0.84      0.84      3755\n",
      "weighted avg       0.86      0.86      0.86      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(simplified_testing.label.astype(\"int\"), vcf.predict(simplified_testing.tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.73      0.74      1209\n",
      "           1       0.87      0.88      0.88      2546\n",
      "\n",
      "    accuracy                           0.83      3755\n",
      "   macro avg       0.81      0.81      0.81      3755\n",
      "weighted avg       0.83      0.83      0.83      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOW LETS TRY SOME ENSEMBLE LEARNING!\n",
    "\n",
    "# FIRST LETS TRY Random Forest, LinearSVC, LogisticRegression and the Multinomial NB using tfidf\n",
    "\n",
    "vcf2 = VotingClassifier(estimators = [\n",
    "    (\"rf\", rf_CM_pipe),\n",
    "    #(\"svc\", svc_CM_pipe),\n",
    "    #(\"cnb\", cnb_CM_pipe), #this one is is more opposite to random forest so comment out lsvc for now\n",
    "    (\"lr\", lr_CM_pipe),\n",
    "    (\"mnbt\", mnbt_CM_pipe), #use the naive bayes classifier with tfidf as it has equal precision and recall\n",
    "], voting = \"hard\", n_jobs = -1)\n",
    "\n",
    "vcf2.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "print(classification_report(simplified_testing.label.astype(\"int\"), vcf2.predict(simplified_testing.tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(pipeline, params):\n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 1, random_state = 1)\n",
    "    clf = RandomizedSearchCV(pipeline, params, scoring = \"f1\", verbose = 6, n_jobs = -1, cv = cv, n_iter = 600)\n",
    "    clf.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed: 21.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed: 44.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 58.9min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 74.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 97.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed: 110.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('rf', RandomForestClassifier(n_jobs=-1))\n",
      "The best parameters found were: {'rf__n_estimators': 800, 'rf__max_features': 'auto', 'rf__max_depth': None, 'rf__bootstrap': True, 'cv__ngram_range': (1, 1), 'cv__max_features': 18000}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.05      0.09      1209\n",
      "           1       0.68      0.97      0.80      2546\n",
      "\n",
      "    accuracy                           0.67      3755\n",
      "   macro avg       0.55      0.51      0.44      3755\n",
      "weighted avg       0.60      0.67      0.57      3755\n",
      "\n",
      "f1 score: 0.8003246753246753\n",
      "precision: 0.6820697288323188\n",
      "recall: 0.9681853888452474\n"
     ]
    }
   ],
   "source": [
    "rf_pipeline = Pipeline ([\n",
    "    (\"cv\", CountVectorizer()),\n",
    "    (\"rf\", RandomForestClassifier(n_jobs = -1))\n",
    "])\n",
    "\n",
    "rf_params = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,20)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,30)],\n",
    "    'rf__n_estimators' : [100 * x for x in range (1,11)],\n",
    "    'rf__max_depth' : [25, 50, 75, None],\n",
    "    'rf__max_features' : [\"auto\", \"sqrt\"],\n",
    "    'rf__bootstrap' : [True, False]\n",
    "}\n",
    "\n",
    "res = random_search(rf_pipeline, rf_params)\n",
    "report_scores(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.3s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 11.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('lr', LogisticRegression(n_jobs=-1))\n",
      "The best parameters found were: {'lr__dual': False, 'lr__C': 1, 'cv__ngram_range': (1, 3), 'cv__max_features': 22000}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68      1209\n",
      "           1       0.86      0.81      0.83      2546\n",
      "\n",
      "    accuracy                           0.78      3755\n",
      "   macro avg       0.75      0.76      0.76      3755\n",
      "weighted avg       0.79      0.78      0.78      3755\n",
      "\n",
      "f1 score: 0.8323886639676115\n",
      "precision: 0.858813700918964\n",
      "recall: 0.8075412411626081\n"
     ]
    }
   ],
   "source": [
    "lr_pipeline = Pipeline([\n",
    "    (\"cv\", CountVectorizer()),\n",
    "    (\"lr\", LogisticRegression(penalty = \"l2\", n_jobs = -1))\n",
    "])\n",
    "\n",
    "lr_params = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,21)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,31)],\n",
    "    'lr__C' : [10 ** i for i in range (-6, 6)],\n",
    "    'lr__dual' : [True, False]\n",
    "}\n",
    "\n",
    "res = random_search(lr_pipeline, lr_params)\n",
    "report_scores(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   53.3s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 13.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('svc', LinearSVC())\n",
      "The best parameters found were: {'svc__C': 0.1, 'cv__ngram_range': (1, 2), 'cv__max_features': 28000}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.70      1209\n",
      "           1       0.86      0.84      0.85      2546\n",
      "\n",
      "    accuracy                           0.80      3755\n",
      "   macro avg       0.77      0.78      0.77      3755\n",
      "weighted avg       0.80      0.80      0.80      3755\n",
      "\n",
      "f1 score: 0.851697438951757\n",
      "precision: 0.8610999598554797\n",
      "recall: 0.8424980361351139\n"
     ]
    }
   ],
   "source": [
    "svc_pipeline = Pipeline([\n",
    "    (\"cv\", CountVectorizer()),\n",
    "    (\"svc\", LinearSVC())\n",
    "])\n",
    "\n",
    "svc_params = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,30)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,30)],\n",
    "    'svc__C' : [10 ** i for i in range (-6,4)]\n",
    "}\n",
    "\n",
    "res = random_search(svc_pipeline, svc_params)\n",
    "report_scores(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   23.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('mnb', MultinomialNB())\n",
      "The best parameters found were: {'mnb__alpha': 0.5, 'cv__ngram_range': (1, 2), 'cv__max_features': 21000}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.78      0.56      1209\n",
      "           1       0.83      0.52      0.64      2546\n",
      "\n",
      "    accuracy                           0.60      3755\n",
      "   macro avg       0.63      0.65      0.60      3755\n",
      "weighted avg       0.70      0.60      0.61      3755\n",
      "\n",
      "f1 score: 0.6384726921217979\n",
      "precision: 0.8297738693467337\n",
      "recall: 0.51885310290652\n"
     ]
    }
   ],
   "source": [
    "mnb_pipeline = Pipeline([\n",
    "    (\"cv\", TfidfVectorizer()),\n",
    "    (\"mnb\", MultinomialNB())\n",
    "])\n",
    "\n",
    "mnb_params = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,30)],\n",
    "    #see if it is worth not choosing max features or if it is\n",
    "    'cv__max_features' : [1000 * i for i in range (1,30)] + [None],\n",
    "    'mnb__alpha' : [0.5 * i for i in range (2 * 0, 2 * 6)] #np.linspace(0.0, 5.0, num = 10),\n",
    "}\n",
    "\n",
    "res = random_search(mnb_pipeline, mnb_params)\n",
    "report_scores(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\George\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:278: UserWarning: The total space of parameters 348 is smaller than n_iter=600. Running 348 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 348 candidates, totalling 1740 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   37.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1740 out of 1740 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('mnb', MultinomialNB())\n",
      "The best parameters found were: {'mnb__alpha': 0.5, 'cv__ngram_range': (1, 2)}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.76      0.57      1209\n",
      "           1       0.83      0.57      0.68      2546\n",
      "\n",
      "    accuracy                           0.63      3755\n",
      "   macro avg       0.64      0.66      0.62      3755\n",
      "weighted avg       0.71      0.63      0.64      3755\n",
      "\n",
      "f1 score: 0.6750759168418594\n",
      "precision: 0.8328530259365994\n",
      "recall: 0.5675569520816968\n"
     ]
    }
   ],
   "source": [
    "mnb_pipeline = Pipeline([\n",
    "    (\"cv\", TfidfVectorizer()),\n",
    "    (\"mnb\", MultinomialNB())\n",
    "])\n",
    "\n",
    "mnb_params = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,30)],\n",
    "    #see if it is worth not choosing max features or if it is\n",
    "    'mnb__alpha' : [0.5 * i for i in range (2 * 0, 2 * 6)] #np.linspace(0.0, 5.0, num = 10),\n",
    "}\n",
    "\n",
    "res = random_search(mnb_pipeline, mnb_params)\n",
    "report_scores(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 11.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('mnb', MultinomialNB())\n",
      "The best parameters found were: {'mnb__alpha': 0.5, 'cv__ngram_range': (1, 2), 'cv__max_features': 24000}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.77      0.55      1209\n",
      "           1       0.82      0.52      0.64      2546\n",
      "\n",
      "    accuracy                           0.60      3755\n",
      "   macro avg       0.63      0.64      0.60      3755\n",
      "weighted avg       0.70      0.60      0.61      3755\n",
      "\n",
      "f1 score: 0.6390760346487007\n",
      "precision: 0.8248447204968944\n",
      "recall: 0.5216025137470542\n"
     ]
    }
   ],
   "source": [
    "mnb_pipeline = Pipeline([\n",
    "    (\"cv\", TfidfVectorizer()),\n",
    "    (\"mnb\", MultinomialNB())\n",
    "])\n",
    "\n",
    "mnb_params = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,30)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,30)],\n",
    "    'mnb__alpha' : [0.5 * i for i in range (2 * 0, 2 * 6)] #np.linspace(0.0, 5.0, num = 10),\n",
    "}\n",
    "\n",
    "res = random_search(mnb_pipeline, mnb_params)\n",
    "report_scores(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   27.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   52.1s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 12.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('sgd', SGDClassifier(max_iter=1500, penalty='elasticnet'))\n",
      "The best parameters found were: {'sgd__l1_ratio': 0.6000000000000001, 'sgd__alpha': 0.0001, 'cv__ngram_range': (1, 7), 'cv__max_features': 23000}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      1209\n",
      "           1       0.89      0.89      0.89      2546\n",
      "\n",
      "    accuracy                           0.85      3755\n",
      "   macro avg       0.83      0.83      0.83      3755\n",
      "weighted avg       0.85      0.85      0.85      3755\n",
      "\n",
      "f1 score: 0.892289582107122\n",
      "precision: 0.8914151313210505\n",
      "recall: 0.8931657501963864\n"
     ]
    }
   ],
   "source": [
    "sgd_pipeline = Pipeline([\n",
    "    (\"cv\", CountVectorizer()),\n",
    "    (\"sgd\", SGDClassifier(penalty = \"elasticnet\", max_iter = 1500))\n",
    "])\n",
    "\n",
    "sgd_params = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,30)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,30)],\n",
    "    'sgd__alpha' : [10 ** i for i in range (-6, 4)],\n",
    "    'sgd__l1_ratio' : [0.1 * i for i in range (0, 11)] #np.linspace(0.0, 1.0, num = 10)\n",
    "}\n",
    "\n",
    "res = random_search(sgd_pipeline, sgd_params)\n",
    "report_scores(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 12.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('cnb', ComplementNB())\n",
      "The best parameters found were: {'cv__ngram_range': (1, 4), 'cv__max_features': 25000, 'cnb__alpha': 1.0}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.86      0.51      1209\n",
      "           1       0.81      0.28      0.41      2546\n",
      "\n",
      "    accuracy                           0.47      3755\n",
      "   macro avg       0.59      0.57      0.46      3755\n",
      "weighted avg       0.66      0.47      0.44      3755\n",
      "\n",
      "f1 score: 0.41425650014607074\n",
      "precision: 0.8084378563283923\n",
      "recall: 0.2784760408483896\n"
     ]
    }
   ],
   "source": [
    "cnb_pipeline = Pipeline([\n",
    "    (\"cv\", CountVectorizer()),\n",
    "    (\"cnb\", ComplementNB())\n",
    "])\n",
    "\n",
    "cnb_params = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,30)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,30)],\n",
    "    'cnb__alpha' : [0.5 * i for i in range (2 * 0, 2 * 6)] #np.linspace (0.0, 5.0, num = 10)\n",
    "}\n",
    "\n",
    "res = random_search(cnb_pipeline, cnb_params)\n",
    "report_scores(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   47.9s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 12.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('bnb', BernoulliNB())\n",
      "The best parameters found were: {'cv__ngram_range': (1, 2), 'cv__max_features': 16000, 'bnb__alpha': 0.0}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.72      0.49      1209\n",
      "           1       0.75      0.40      0.53      2546\n",
      "\n",
      "    accuracy                           0.51      3755\n",
      "   macro avg       0.56      0.56      0.51      3755\n",
      "weighted avg       0.63      0.51      0.51      3755\n",
      "\n",
      "f1 score: 0.52620813091281\n",
      "precision: 0.7538461538461538\n",
      "recall: 0.40416339355852315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\George\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "bnb_pipeline = Pipeline([\n",
    "    (\"cv\", CountVectorizer(binary = True)),\n",
    "    (\"bnb\", BernoulliNB())\n",
    "])\n",
    "\n",
    "bnb_params = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,30)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,30)],\n",
    "    'bnb__alpha' : [0.5 * i for i in range (2 * 0, 2 * 6)] #np.linspace (0.0, 5.0, num = 10)\n",
    "}\n",
    "\n",
    "res = random_search(bnb_pipeline, bnb_params)\n",
    "report_scores(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.5 * i for i in range (2 * 0, 2 * 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.0s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 11.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('mnb', MultinomialNB())\n",
      "The best parameters found were: {'mnb__alpha': 0.5, 'cv__ngram_range': (1, 2), 'cv__max_features': 23000}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.82      0.50      1209\n",
      "           1       0.78      0.30      0.43      2546\n",
      "\n",
      "    accuracy                           0.47      3755\n",
      "   macro avg       0.57      0.56      0.47      3755\n",
      "weighted avg       0.64      0.47      0.45      3755\n",
      "\n",
      "f1 score: 0.43451202263083455\n",
      "precision: 0.7765419615773509\n",
      "recall: 0.3016496465043205\n"
     ]
    }
   ],
   "source": [
    "mnb_pipeline = Pipeline([\n",
    "    (\"cv\", CountVectorizer()),\n",
    "    (\"mnb\", MultinomialNB())\n",
    "])\n",
    "\n",
    "mnb_params = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,30)],\n",
    "    #see if it is worth not choosing max features or if it is\n",
    "    'cv__max_features' : [1000 * i for i in range (1,30)],\n",
    "    'mnb__alpha' : [0.5 * i for i in range (2 * 0, 2 * 5)] #np.linspace(0.0, 5.0, num = 2),\n",
    "}\n",
    "\n",
    "res = random_search(mnb_pipeline, mnb_params)\n",
    "report_scores(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    #('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('sgd', SGDClassifier(n_jobs = -1, penalty = \"elasticnet\"))\n",
    "])\n",
    "\n",
    "sgd_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,7)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,11)],\n",
    "    \"sgd__alpha\" : [10 ** x for x in range (-5, -2)],\n",
    "    \"sgd__l1_ratio\" : [0, 0.25, 0.5, 0.75, 1]\n",
    "    \n",
    "}\n",
    "\n",
    "sgdResult_V2 = grid_search(sgd_pipelineV2, sgd_paramsV2)\n",
    "report_scores(sgdResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_self_setattr_tracking': True,\n",
       " '_obj_reference_counts_dict': ObjectIdentityDictionary({<_ObjectIdentityWrapper wrapping 10000>: 1, <_ObjectIdentityWrapper wrapping 1>: 1, <_ObjectIdentityWrapper wrapping 'lower_and_strip_punctuation'>: 1, <_ObjectIdentityWrapper wrapping 'whitespace'>: 1, <_ObjectIdentityWrapper wrapping 'int'>: 1, <_ObjectIdentityWrapper wrapping 250>: 1, <_ObjectIdentityWrapper wrapping True>: 1, <_ObjectIdentityWrapper wrapping 0>: 1, <_ObjectIdentityWrapper wrapping False>: 2, <_ObjectIdentityWrapper wrapping DictWrapper(OrderedDict())>: 1, <_ObjectIdentityWrapper wrapping <tensorflow.python.keras.layers.preprocessing.string_lookup.StringLookup object at 0x000001D8E0C34C70>>: 1}),\n",
       " '_max_tokens': 10000,\n",
       " '_oov_value': 1,\n",
       " '_standardize': 'lower_and_strip_punctuation',\n",
       " '_split': 'whitespace',\n",
       " '_ngrams_arg': None,\n",
       " '_ngrams': None,\n",
       " '_output_mode': 'int',\n",
       " '_output_sequence_length': 250,\n",
       " '_pad_to_max': True,\n",
       " '_vocab_size': 0,\n",
       " '_called': False,\n",
       " '_instrumented_keras_api': True,\n",
       " '_instrumented_keras_layer_class': True,\n",
       " '_instrumented_keras_model_class': False,\n",
       " '_trainable': True,\n",
       " '_stateful': False,\n",
       " 'built': True,\n",
       " '_build_input_shape': TensorShape([14277, 1]),\n",
       " '_saved_model_inputs_spec': None,\n",
       " '_input_spec': None,\n",
       " '_supports_masking': False,\n",
       " '_name': 'text_vectorization_16',\n",
       " '_activity_regularizer': None,\n",
       " '_trainable_weights': [],\n",
       " '_non_trainable_weights': [],\n",
       " '_updates': [],\n",
       " '_thread_local': <_thread._local at 0x1d9671a3ea0>,\n",
       " '_callable_losses': [],\n",
       " '_losses': [],\n",
       " '_metrics': [],\n",
       " '_metrics_lock': <unlocked _thread.lock object at 0x000001D8D87D78A0>,\n",
       " '_dtype_policy': <Policy \"string\">,\n",
       " '_compute_dtype_object': tf.string,\n",
       " '_autocast': True,\n",
       " '_layers': [OrderedDict(),\n",
       "  <tensorflow.python.keras.layers.preprocessing.string_lookup.StringLookup at 0x1d8e0c34c70>],\n",
       " '_inbound_nodes_value': [],\n",
       " '_outbound_nodes_value': [],\n",
       " '_expects_training_arg': False,\n",
       " '_default_training_arg': None,\n",
       " '_expects_mask_arg': False,\n",
       " '_dynamic': False,\n",
       " '_initial_weights': None,\n",
       " '_auto_track_sub_layers': True,\n",
       " '_preserve_input_structure_in_config': False,\n",
       " '_combiner': None,\n",
       " '_previously_updated': False,\n",
       " '_self_unconditional_checkpoint_dependencies': [TrackableReference(name='state_variables', ref=DictWrapper(OrderedDict())),\n",
       "  TrackableReference(name='_index_lookup_layer', ref=<tensorflow.python.keras.layers.preprocessing.string_lookup.StringLookup object at 0x000001D8E0C34C70>)],\n",
       " '_self_unconditional_dependency_names': {'state_variables': OrderedDict(),\n",
       "  '_index_lookup_layer': <tensorflow.python.keras.layers.preprocessing.string_lookup.StringLookup at 0x1d8e0c34c70>},\n",
       " '_self_unconditional_deferred_dependencies': {},\n",
       " '_self_update_uid': -1,\n",
       " '_self_name_based_restores': set(),\n",
       " '_self_saveable_object_factories': {},\n",
       " 'state_variables': OrderedDict(),\n",
       " '_index_lookup_layer': <tensorflow.python.keras.layers.preprocessing.string_lookup.StringLookup at 0x1d8e0c34c70>,\n",
       " '_vectorize_layer': None}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training_vectorizer = CountVectorizer()\n",
    "#training_vectorizer.fit(simplified_training.tokens)\n",
    "#training_vectorizer.transform(simplified_training.tokens).toarray().shape\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens = 10000,\n",
    "    output_sequence_length = 250,\n",
    "    output_mode = \"int\"\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(simplified_training.tokens.to_numpy())\n",
    "vectorize_layer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.matrix"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(simplified_training.tokens)\n",
    "res = cv.transform(simplified_training.tokens).todense()\n",
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the matrix made by the CountVectorizer and the labels into a tensorflow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((res, simplified_training.label.astype(\"int\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-312-255f86faff4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m ])\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;31m# Legacy graph support is contained in `training_v1.Model`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m     \u001b[0mversion_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisallow_legacy_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m     \u001b[0m_disallow_inside_tf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2590\u001b[0m     \u001b[1;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2591\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2592\u001b[1;33m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[0;32m   2593\u001b[0m                          \u001b[1;34m'training/testing. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2594\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "real    4921\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "real_instances = original_training[original_training.label == \"real\"]\n",
    "fake_instances = original_training[original_training.label != \"real\"]\n",
    "#fake_instances = simplified_training[not (\"real\" in simplified_training.label)]\n",
    "#eal_toks = tokens[\"real\" in tokens.label]\n",
    "#ake_toks = tokens[\"real\" not in tokens.label]\n",
    "real_instances.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cv = CountVectorizer()\n",
    "real_cv.fit(real_instances.tweetText)\n",
    "\n",
    "fake_cv = CountVectorizer()\n",
    "fake_cv.fit(fake_instances.tweetText)\n",
    "\n",
    "reals = [(k,v) for k, v in real_cv.vocabulary_.items()]\n",
    "reals.sort(key = operator.itemgetter(1))\n",
    "reals.reverse()\n",
    "fakes = [(k,v) for k, v in fake_cv.vocabulary_.items()]\n",
    "fakes.sort(key = operator.itemgetter(1))\n",
    "fakes.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakess = simplified_training[simplified_training.label == 1]\n",
    "realss = simplified_training[simplified_training.label == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en         73.610517\n",
       "es         10.859342\n",
       "unknown     8.497221\n",
       "pt          1.613938\n",
       "fr          1.303976\n",
       "nl          0.780248\n",
       "de          0.684053\n",
       "it          0.662676\n",
       "ar          0.609235\n",
       "ru          0.502351\n",
       "no          0.245832\n",
       "sv          0.245832\n",
       "da          0.181702\n",
       "fi          0.106883\n",
       "hu          0.053442\n",
       "ro          0.042753\n",
       "Name: lang, dtype: float64"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakess.lang.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en         82.909978\n",
       "es          5.588295\n",
       "unknown     5.344442\n",
       "fr          1.910181\n",
       "de          1.219264\n",
       "it          0.690916\n",
       "ar          0.467385\n",
       "sv          0.447064\n",
       "nl          0.304816\n",
       "no          0.284495\n",
       "ru          0.284495\n",
       "pt          0.243853\n",
       "da          0.162569\n",
       "ro          0.081284\n",
       "fi          0.040642\n",
       "hu          0.020321\n",
       "Name: lang, dtype: float64"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realss.lang.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_cv__max_features</th>\n",
       "      <th>param_cv__ngram_range</th>\n",
       "      <th>param_nb__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.811465</td>\n",
       "      <td>0.052013</td>\n",
       "      <td>0.182936</td>\n",
       "      <td>0.014244</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>2</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 2.0}</td>\n",
       "      <td>0.837532</td>\n",
       "      <td>0.868768</td>\n",
       "      <td>0.941638</td>\n",
       "      <td>0.918847</td>\n",
       "      <td>0.818393</td>\n",
       "      <td>0.877036</td>\n",
       "      <td>0.046882</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.774688</td>\n",
       "      <td>0.042030</td>\n",
       "      <td>0.182933</td>\n",
       "      <td>0.018495</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>2.5</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 2.5}</td>\n",
       "      <td>0.835291</td>\n",
       "      <td>0.866933</td>\n",
       "      <td>0.942343</td>\n",
       "      <td>0.920810</td>\n",
       "      <td>0.819748</td>\n",
       "      <td>0.877025</td>\n",
       "      <td>0.047556</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.774855</td>\n",
       "      <td>0.073815</td>\n",
       "      <td>0.189302</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>3</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3.0}</td>\n",
       "      <td>0.833563</td>\n",
       "      <td>0.865421</td>\n",
       "      <td>0.944935</td>\n",
       "      <td>0.921502</td>\n",
       "      <td>0.818950</td>\n",
       "      <td>0.876874</td>\n",
       "      <td>0.048961</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.701766</td>\n",
       "      <td>0.060904</td>\n",
       "      <td>0.175637</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>3.5</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3.5}</td>\n",
       "      <td>0.835125</td>\n",
       "      <td>0.864071</td>\n",
       "      <td>0.945408</td>\n",
       "      <td>0.921184</td>\n",
       "      <td>0.823049</td>\n",
       "      <td>0.877767</td>\n",
       "      <td>0.047875</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.744092</td>\n",
       "      <td>0.097567</td>\n",
       "      <td>0.173818</td>\n",
       "      <td>0.009744</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>4</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 4.0}</td>\n",
       "      <td>0.835085</td>\n",
       "      <td>0.863065</td>\n",
       "      <td>0.945492</td>\n",
       "      <td>0.922234</td>\n",
       "      <td>0.824486</td>\n",
       "      <td>0.878072</td>\n",
       "      <td>0.047834</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>1.808868</td>\n",
       "      <td>0.033338</td>\n",
       "      <td>0.190575</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>22500</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>5.5</td>\n",
       "      <td>{'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 5.5}</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.860859</td>\n",
       "      <td>0.947341</td>\n",
       "      <td>0.926425</td>\n",
       "      <td>0.826633</td>\n",
       "      <td>0.878669</td>\n",
       "      <td>0.049379</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>1.806728</td>\n",
       "      <td>0.043773</td>\n",
       "      <td>0.181072</td>\n",
       "      <td>0.018915</td>\n",
       "      <td>22500</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>6</td>\n",
       "      <td>{'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 6.0}</td>\n",
       "      <td>0.830762</td>\n",
       "      <td>0.860660</td>\n",
       "      <td>0.944700</td>\n",
       "      <td>0.925381</td>\n",
       "      <td>0.826774</td>\n",
       "      <td>0.877655</td>\n",
       "      <td>0.048682</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>1.806973</td>\n",
       "      <td>0.057512</td>\n",
       "      <td>0.185379</td>\n",
       "      <td>0.010972</td>\n",
       "      <td>22500</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>6.5</td>\n",
       "      <td>{'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 6.5}</td>\n",
       "      <td>0.831104</td>\n",
       "      <td>0.859661</td>\n",
       "      <td>0.944487</td>\n",
       "      <td>0.925735</td>\n",
       "      <td>0.826913</td>\n",
       "      <td>0.877580</td>\n",
       "      <td>0.048670</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>1.791002</td>\n",
       "      <td>0.041953</td>\n",
       "      <td>0.194709</td>\n",
       "      <td>0.015359</td>\n",
       "      <td>22500</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>7</td>\n",
       "      <td>{'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 7.0}</td>\n",
       "      <td>0.829975</td>\n",
       "      <td>0.858731</td>\n",
       "      <td>0.944926</td>\n",
       "      <td>0.921423</td>\n",
       "      <td>0.824938</td>\n",
       "      <td>0.875999</td>\n",
       "      <td>0.048658</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>1.612976</td>\n",
       "      <td>0.102666</td>\n",
       "      <td>0.114108</td>\n",
       "      <td>0.020504</td>\n",
       "      <td>22500</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>7.5</td>\n",
       "      <td>{'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 7.5}</td>\n",
       "      <td>0.829677</td>\n",
       "      <td>0.856813</td>\n",
       "      <td>0.944473</td>\n",
       "      <td>0.922290</td>\n",
       "      <td>0.822886</td>\n",
       "      <td>0.875228</td>\n",
       "      <td>0.049322</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows √ó 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         1.811465      0.052013         0.182936        0.014244   \n",
       "1         1.774688      0.042030         0.182933        0.018495   \n",
       "2         1.774855      0.073815         0.189302        0.012025   \n",
       "3         1.701766      0.060904         0.175637        0.011096   \n",
       "4         1.744092      0.097567         0.173818        0.009744   \n",
       "..             ...           ...              ...             ...   \n",
       "715       1.808868      0.033338         0.190575        0.015797   \n",
       "716       1.806728      0.043773         0.181072        0.018915   \n",
       "717       1.806973      0.057512         0.185379        0.010972   \n",
       "718       1.791002      0.041953         0.194709        0.015359   \n",
       "719       1.612976      0.102666         0.114108        0.020504   \n",
       "\n",
       "    param_cv__max_features param_cv__ngram_range param_nb__alpha  \\\n",
       "0                    17000                (1, 8)               2   \n",
       "1                    17000                (1, 8)             2.5   \n",
       "2                    17000                (1, 8)               3   \n",
       "3                    17000                (1, 8)             3.5   \n",
       "4                    17000                (1, 8)               4   \n",
       "..                     ...                   ...             ...   \n",
       "715                  22500               (1, 12)             5.5   \n",
       "716                  22500               (1, 12)               6   \n",
       "717                  22500               (1, 12)             6.5   \n",
       "718                  22500               (1, 12)               7   \n",
       "719                  22500               (1, 12)             7.5   \n",
       "\n",
       "                                                                        params  \\\n",
       "0     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 2.0}   \n",
       "1     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 2.5}   \n",
       "2     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3.0}   \n",
       "3     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3.5}   \n",
       "4     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 4.0}   \n",
       "..                                                                         ...   \n",
       "715  {'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 5.5}   \n",
       "716  {'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 6.0}   \n",
       "717  {'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 6.5}   \n",
       "718  {'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 7.0}   \n",
       "719  {'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 7.5}   \n",
       "\n",
       "     split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0             0.837532           0.868768           0.941638   \n",
       "1             0.835291           0.866933           0.942343   \n",
       "2             0.833563           0.865421           0.944935   \n",
       "3             0.835125           0.864071           0.945408   \n",
       "4             0.835085           0.863065           0.945492   \n",
       "..                 ...                ...                ...   \n",
       "715           0.832084           0.860859           0.947341   \n",
       "716           0.830762           0.860660           0.944700   \n",
       "717           0.831104           0.859661           0.944487   \n",
       "718           0.829975           0.858731           0.944926   \n",
       "719           0.829677           0.856813           0.944473   \n",
       "\n",
       "     split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0             0.918847           0.818393         0.877036        0.046882   \n",
       "1             0.920810           0.819748         0.877025        0.047556   \n",
       "2             0.921502           0.818950         0.876874        0.048961   \n",
       "3             0.921184           0.823049         0.877767        0.047875   \n",
       "4             0.922234           0.824486         0.878072        0.047834   \n",
       "..                 ...                ...              ...             ...   \n",
       "715           0.926425           0.826633         0.878669        0.049379   \n",
       "716           0.925381           0.826774         0.877655        0.048682   \n",
       "717           0.925735           0.826913         0.877580        0.048670   \n",
       "718           0.921423           0.824938         0.875999        0.048658   \n",
       "719           0.922290           0.822886         0.875228        0.049322   \n",
       "\n",
       "     rank_test_score  \n",
       "0                548  \n",
       "1                553  \n",
       "2                574  \n",
       "3                402  \n",
       "4                310  \n",
       "..               ...  \n",
       "715              136  \n",
       "716              432  \n",
       "717              452  \n",
       "718              625  \n",
       "719              658  \n",
       "\n",
       "[720 rows x 16 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(mNBResult_V7.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_cv__max_features</th>\n",
       "      <th>param_cv__ngram_range</th>\n",
       "      <th>param_nb__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.700852</td>\n",
       "      <td>0.064033</td>\n",
       "      <td>0.182134</td>\n",
       "      <td>0.016466</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>3</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3}</td>\n",
       "      <td>0.833563</td>\n",
       "      <td>0.865421</td>\n",
       "      <td>0.944935</td>\n",
       "      <td>0.921502</td>\n",
       "      <td>0.818950</td>\n",
       "      <td>0.876874</td>\n",
       "      <td>0.048961</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.725590</td>\n",
       "      <td>0.058813</td>\n",
       "      <td>0.177360</td>\n",
       "      <td>0.019758</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>4</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 4}</td>\n",
       "      <td>0.835085</td>\n",
       "      <td>0.863065</td>\n",
       "      <td>0.945492</td>\n",
       "      <td>0.922234</td>\n",
       "      <td>0.824486</td>\n",
       "      <td>0.878072</td>\n",
       "      <td>0.047834</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.656569</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.174861</td>\n",
       "      <td>0.012518</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>5</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 5}</td>\n",
       "      <td>0.833523</td>\n",
       "      <td>0.860460</td>\n",
       "      <td>0.945904</td>\n",
       "      <td>0.921879</td>\n",
       "      <td>0.825911</td>\n",
       "      <td>0.877535</td>\n",
       "      <td>0.048030</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.654335</td>\n",
       "      <td>0.038229</td>\n",
       "      <td>0.164041</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>6</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 6}</td>\n",
       "      <td>0.832386</td>\n",
       "      <td>0.859861</td>\n",
       "      <td>0.946773</td>\n",
       "      <td>0.923475</td>\n",
       "      <td>0.824950</td>\n",
       "      <td>0.877489</td>\n",
       "      <td>0.049031</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.711115</td>\n",
       "      <td>0.046159</td>\n",
       "      <td>0.182961</td>\n",
       "      <td>0.019486</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 9)</td>\n",
       "      <td>3</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 9), 'nb__alpha': 3}</td>\n",
       "      <td>0.836688</td>\n",
       "      <td>0.863806</td>\n",
       "      <td>0.945209</td>\n",
       "      <td>0.921584</td>\n",
       "      <td>0.821155</td>\n",
       "      <td>0.877688</td>\n",
       "      <td>0.048074</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.771514</td>\n",
       "      <td>0.044925</td>\n",
       "      <td>0.190980</td>\n",
       "      <td>0.024662</td>\n",
       "      <td>22000</td>\n",
       "      <td>(1, 11)</td>\n",
       "      <td>6</td>\n",
       "      <td>{'cv__max_features': 22000, 'cv__ngram_range': (1, 11), 'nb__alpha': 6}</td>\n",
       "      <td>0.831440</td>\n",
       "      <td>0.860060</td>\n",
       "      <td>0.946776</td>\n",
       "      <td>0.925381</td>\n",
       "      <td>0.826720</td>\n",
       "      <td>0.878076</td>\n",
       "      <td>0.049182</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.797188</td>\n",
       "      <td>0.039153</td>\n",
       "      <td>0.178165</td>\n",
       "      <td>0.015074</td>\n",
       "      <td>22000</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>3</td>\n",
       "      <td>{'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 3}</td>\n",
       "      <td>0.836305</td>\n",
       "      <td>0.865623</td>\n",
       "      <td>0.944833</td>\n",
       "      <td>0.922996</td>\n",
       "      <td>0.824252</td>\n",
       "      <td>0.878802</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.769360</td>\n",
       "      <td>0.048427</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>0.014231</td>\n",
       "      <td>22000</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>4</td>\n",
       "      <td>{'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 4}</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>0.863467</td>\n",
       "      <td>0.945304</td>\n",
       "      <td>0.924720</td>\n",
       "      <td>0.825944</td>\n",
       "      <td>0.878782</td>\n",
       "      <td>0.048011</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.807535</td>\n",
       "      <td>0.059350</td>\n",
       "      <td>0.191093</td>\n",
       "      <td>0.015101</td>\n",
       "      <td>22000</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>5</td>\n",
       "      <td>{'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 5}</td>\n",
       "      <td>0.833599</td>\n",
       "      <td>0.861460</td>\n",
       "      <td>0.947260</td>\n",
       "      <td>0.924392</td>\n",
       "      <td>0.826788</td>\n",
       "      <td>0.878700</td>\n",
       "      <td>0.048610</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.631261</td>\n",
       "      <td>0.100160</td>\n",
       "      <td>0.134660</td>\n",
       "      <td>0.026520</td>\n",
       "      <td>22000</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>6</td>\n",
       "      <td>{'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 6}</td>\n",
       "      <td>0.830762</td>\n",
       "      <td>0.859861</td>\n",
       "      <td>0.945184</td>\n",
       "      <td>0.924465</td>\n",
       "      <td>0.826359</td>\n",
       "      <td>0.877326</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows √ó 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         1.700852      0.064033         0.182134        0.016466   \n",
       "1         1.725590      0.058813         0.177360        0.019758   \n",
       "2         1.656569      0.041990         0.174861        0.012518   \n",
       "3         1.654335      0.038229         0.164041        0.012219   \n",
       "4         1.711115      0.046159         0.182961        0.019486   \n",
       "..             ...           ...              ...             ...   \n",
       "115       1.771514      0.044925         0.190980        0.024662   \n",
       "116       1.797188      0.039153         0.178165        0.015074   \n",
       "117       1.769360      0.048427         0.191083        0.014231   \n",
       "118       1.807535      0.059350         0.191093        0.015101   \n",
       "119       1.631261      0.100160         0.134660        0.026520   \n",
       "\n",
       "    param_cv__max_features param_cv__ngram_range param_nb__alpha  \\\n",
       "0                    17000                (1, 8)               3   \n",
       "1                    17000                (1, 8)               4   \n",
       "2                    17000                (1, 8)               5   \n",
       "3                    17000                (1, 8)               6   \n",
       "4                    17000                (1, 9)               3   \n",
       "..                     ...                   ...             ...   \n",
       "115                  22000               (1, 11)               6   \n",
       "116                  22000               (1, 12)               3   \n",
       "117                  22000               (1, 12)               4   \n",
       "118                  22000               (1, 12)               5   \n",
       "119                  22000               (1, 12)               6   \n",
       "\n",
       "                                                                      params  \\\n",
       "0     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3}   \n",
       "1     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 4}   \n",
       "2     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 5}   \n",
       "3     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 6}   \n",
       "4     {'cv__max_features': 17000, 'cv__ngram_range': (1, 9), 'nb__alpha': 3}   \n",
       "..                                                                       ...   \n",
       "115  {'cv__max_features': 22000, 'cv__ngram_range': (1, 11), 'nb__alpha': 6}   \n",
       "116  {'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 3}   \n",
       "117  {'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 4}   \n",
       "118  {'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 5}   \n",
       "119  {'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 6}   \n",
       "\n",
       "     split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0             0.833563           0.865421           0.944935   \n",
       "1             0.835085           0.863065           0.945492   \n",
       "2             0.833523           0.860460           0.945904   \n",
       "3             0.832386           0.859861           0.946773   \n",
       "4             0.836688           0.863806           0.945209   \n",
       "..                 ...                ...                ...   \n",
       "115           0.831440           0.860060           0.946776   \n",
       "116           0.836305           0.865623           0.944833   \n",
       "117           0.834473           0.863467           0.945304   \n",
       "118           0.833599           0.861460           0.947260   \n",
       "119           0.830762           0.859861           0.945184   \n",
       "\n",
       "     split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0             0.921502           0.818950         0.876874        0.048961   \n",
       "1             0.922234           0.824486         0.878072        0.047834   \n",
       "2             0.921879           0.825911         0.877535        0.048030   \n",
       "3             0.923475           0.824950         0.877489        0.049031   \n",
       "4             0.921584           0.821155         0.877688        0.048074   \n",
       "..                 ...                ...              ...             ...   \n",
       "115           0.925381           0.826720         0.878076        0.049182   \n",
       "116           0.922996           0.824252         0.878802        0.047473   \n",
       "117           0.924720           0.825944         0.878782        0.048011   \n",
       "118           0.924392           0.826788         0.878700        0.048610   \n",
       "119           0.924465           0.826359         0.877326        0.048780   \n",
       "\n",
       "     rank_test_score  \n",
       "0                116  \n",
       "1                 68  \n",
       "2                 99  \n",
       "3                101  \n",
       "4                 91  \n",
       "..               ...  \n",
       "115               66  \n",
       "116               18  \n",
       "117               20  \n",
       "118               26  \n",
       "119              104  \n",
       "\n",
       "[120 rows x 16 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how to display the results of the grid search as a large table\n",
    "pd.DataFrame(mNBResult_V5.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline which always starts with a countvectorizer\n",
    "mNaiveBayesPipeline = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    #('dt', Densifier()), #used for converting from sparse matrix to dense matrix, not needed here\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#the second lot of parameters to try\n",
    "mNaiveBayesParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'cv__max_features' : [500 * i for i in range(2 * 12,2 * 17)],\n",
    "    'nb__alpha' : [0.5 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "#ignore this one for now\n",
    "cNBpipeline = Pipeline([('cv', CountVectorizer()), ('cnb', ComplementNB())])\n",
    "cNBparams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'cv__max_features' : [1000* i for i in range(11,18)],\n",
    "    'cnb__alpha' : [1 * i for i in range (0,9)]\n",
    "}\n",
    "\n",
    "#add: ('pca', PCA()),\n",
    "lsvcPipeline = Pipeline([('cv', CountVectorizer()), ('df', Densifier()), ('pca', PCA()), ('lsvc', LinearSVC())])\n",
    "lsvcParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(10,15)],\n",
    "    'cv__max_features' : [1000* i for i in range(14,18)],\n",
    "    'lsvc__C' : [1 * i for i in range(0,3)]\n",
    "}\n",
    "\n",
    "rForestPipeline = Pipeline([('cv', CountVectorizer()), ('rf', RandomForestClassifier())])\n",
    "rForestParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(13,19)],\n",
    "    'cv__max_features' : [1000* i for i in range(10,17)],\n",
    "    'rf__n_estimators' : [10 * i for i in range(10,15)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def reportScores(clf):\n",
    "    \n",
    "    print(\"---- RESULTS ----\",\"\\n\")\n",
    "    print(\"The algorithm being optimised was:\",clf.estimator.steps[-1])\n",
    "    print(\"The best parameters found were:\", clf.best_params_)\n",
    "    y_test_true = simplified_testing.label\n",
    "    y_test_predictions = clf.predict(simplified_testing.tokens)\n",
    "    print(\"Score report:\\n\")\n",
    "    #prevent the chance of any of the lists being treated as objects\n",
    "    print(classification_report(y_test_true.astype('int'), y_test_predictions.astype('int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST SEARCH: ngram 3-16, max features 5-16, alpha 0-9, RESULT: ngram: (1,15), features: 15000, alpha: 0\n",
    "#so now try making the features and ngram even LARGER! got an accuracy of 81\n",
    "\n",
    "#SECOND SEARCH: similar not much change, ngram 14-18, features 13-19, alpha 0-2\n",
    "#RESULTS: features 14000, ngram 15, alpha 0\n",
    "mNBResult = gridSearch(mNaiveBayesPipeline, mNaiveBayesParams)\n",
    "reportScores(mNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters found from alpha 0-9, max features 5k-16k and ngram (3,16): \n",
    "# alpha 2, max features 15000, ngram (1,15) with accuracy of 66. Not very good\n",
    "cNBResult = gridSearch(cNBpipeline, cNBparams) \n",
    "reportScores(cNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 66 was okay... tried ngram 13-17, max features 13-17 and estimators 80-110 and best was 13000 features, ngram 1,15 and n estimators 110\n",
    "#so try again with everything a bit higher\n",
    "rForestResult = gridSearch(rForestPipeline, rForestParams)\n",
    "reportScores(rForestResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters searched: ngram 10-12, max features 10-12, C 0-2, found: ngram 11, max features 12000\n",
    "#introducing PCA made the computer die so don't use it again\n",
    "lsvcResult = gridSearch(lsvcPipeline, lsvcParams)\n",
    "reportScores(lsvcResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA ONES TO DO AFTERWARDS, KEEP ADDING THEM BELOW THEN COMMENT THE RESULTS UP ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 210 candidates, totalling 1050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 30.7min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 40.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed: 42.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('rf', RandomForestClassifier())\n",
      "The best parameters found were: {'cv__max_features': 12000, 'cv__ngram_range': (1, 15), 'rf__n_estimators': 100}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.05      0.08      1209\n",
      "           1       0.68      0.94      0.79      2546\n",
      "\n",
      "    accuracy                           0.65      3755\n",
      "   macro avg       0.48      0.49      0.43      3755\n",
      "weighted avg       0.55      0.65      0.56      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rForestResult = gridSearch(rForestPipeline, rForestParams)\n",
    "reportScores(rForestResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('lsvc', LinearSVC())\n",
      "The best parameters found were: {'cv__max_features': 15000, 'cv__ngram_range': (1, 13), 'lsvc__C': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.80      0.71      1209\n",
      "           1       0.89      0.78      0.84      2546\n",
      "\n",
      "    accuracy                           0.79      3755\n",
      "   macro avg       0.77      0.79      0.77      3755\n",
      "weighted avg       0.81      0.79      0.79      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lsvcResult = gridSearch(lsvcPipeline, lsvcParams)\n",
    "reportScores(lsvcResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mNBResult = gridSearch(mNaiveBayesPipeline, mNaiveBayesParams)\n",
    "reportScores(mNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW TRY DOING THE SAME BUT USING TFIDF INSTEAD\n",
    "\n",
    "mnbtPipeline = Pipeline([\n",
    "    ('tf', TfidfVectorizer()), \n",
    "    #('dt', Densifier()), #used for converting from sparse matrix to dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#the second lot of parameters to try\n",
    "mnbtParams = {\n",
    "    'tf__ngram_range': [(1,0.5 * x) for x in range(2 * 12,2 *17)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'nb__alpha' : [0.25 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "#ignore this one for now\n",
    "cnbtPipeline = Pipeline([('tf', TfidfVectorizer()), ('cnb', ComplementNB())])\n",
    "cnbtParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'cnb__alpha' : [1 * i for i in range (0,9)]\n",
    "}\n",
    "\n",
    "#add: ('pca', PCA()),\n",
    "lsvctPipeline = Pipeline([('tf', TfidfVectorizer()), ('df', Densifier()), ('lsvc', LinearSVC())])\n",
    "lsvctParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(10,15)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'lsvc__C' : [1 * i for i in range(0,3)]\n",
    "}\n",
    "\n",
    "rftPipeline = Pipeline([('tf', TfidfVectorizer()), ('rf', RandomForestClassifier())])\n",
    "rftParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(13,19)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'rf__n_estimators' : [10 * i for i in range(10,15)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST SEARCH: ngram 14-18, alpha 0-6, RESULT: alpha = 0.5, ngram = 14, norm = l2, accuracy = 87\n",
    "\n",
    "#SECOND SEARCH: \n",
    "mnbtResult = gridSearch(mnbtPipeline, mnbtParams)\n",
    "reportScores(mnbtResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnbtResult = gridSearch(cnbtPipeline, cnbtParams)\n",
    "reportScores(cnbtResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvctResult = gridSearch(lsvctPipeline, lsvctParams)\n",
    "reportScores(lsvctResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 39.7min\n"
     ]
    }
   ],
   "source": [
    "#FIRST SEARCH: \n",
    "rftResult = gridSearch(rftPipeline, rftParams)\n",
    "reportScores(rftResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 5000)\n",
    "tokenizer.fit_on_texts(simplified_training.tokens)\n",
    "\n",
    "raw_train = tokenizer.texts_to_sequences(simplified_training.tokens)\n",
    "raw_test = tokenizer.texts_to_sequences(simplified_testing.tokens)\n",
    "\n",
    "padded_train = pad_sequences(raw_train, padding = \"post\", maxlen = 24)\n",
    "padded_test = pad_sequences(raw_train, padding = \"post\", maxlen = 24)\n",
    "\n",
    "#pair each instance with it's label\n",
    "unsplitwlabels = list(zip(padded_train, simplified_training.label))\n",
    "testwlabels = list(zip(padded_test, simplified_testing.label))\n",
    "\n",
    "#shuffle the training data before splitting it into a validation set\n",
    "shuffle(unsplitwlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and validation\n",
    "\n",
    "#take the first n elements of the list\n",
    "trainwlabels = unsplitwlabels[:12000]\n",
    "\n",
    "#take the last n elements of the list\n",
    "validationwlabels = unsplitwlabels[12000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11844"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 24, 5)             59220     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_3 (Average (None, 1, 5)              0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 59,256\n",
      "Trainable params: 59,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Embedding(input_dim = vocab_size, output_dim = 5, input_length = 24))\n",
    "model.add(layers.AveragePooling1D(24))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(5, activation = \"relu\"))\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(list(np.array(x[0]).astype(int) for x in trainwlabels))\n",
    "train_label = np.array(list(np.array(x[1]).astype(int) for x in trainwlabels))\n",
    "validation_data = np.array(list(np.array(x[0]).astype(int) for x in validationwlabels))\n",
    "validation_label = np.array(list(np.array(x[1]).astype(int) for x in validationwlabels))\n",
    "test_data = np.array(list(np.array(x[0]).astype(int) for x in testwlabels))\n",
    "test_label = np.array(list(np.array(x[1]).astype(int) for x in testwlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.6799 - accuracy: 0.6271 - val_loss: 0.6137 - val_accuracy: 0.8076\n",
      "Epoch 2/30\n",
      "375/375 [==============================] - 0s 965us/step - loss: 0.5754 - accuracy: 0.8393 - val_loss: 0.4764 - val_accuracy: 0.8902\n",
      "Epoch 3/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4460 - accuracy: 0.8931 - val_loss: 0.3956 - val_accuracy: 0.8933\n",
      "Epoch 4/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3608 - accuracy: 0.9111 - val_loss: 0.3323 - val_accuracy: 0.9078\n",
      "Epoch 5/30\n",
      "375/375 [==============================] - 0s 975us/step - loss: 0.3038 - accuracy: 0.9198 - val_loss: 0.2980 - val_accuracy: 0.9122\n",
      "Epoch 6/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2595 - accuracy: 0.9300 - val_loss: 0.2737 - val_accuracy: 0.9135\n",
      "Epoch 7/30\n",
      "375/375 [==============================] - 0s 997us/step - loss: 0.2338 - accuracy: 0.9357 - val_loss: 0.2582 - val_accuracy: 0.9144\n",
      "Epoch 8/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9351 - val_loss: 0.2520 - val_accuracy: 0.9126\n",
      "Epoch 9/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.9429 - val_loss: 0.2422 - val_accuracy: 0.9166\n",
      "Epoch 10/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9475 - val_loss: 0.2406 - val_accuracy: 0.9104\n",
      "Epoch 11/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9507 - val_loss: 0.2415 - val_accuracy: 0.9126\n",
      "Epoch 12/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9535 - val_loss: 0.2410 - val_accuracy: 0.9126\n",
      "Epoch 13/30\n",
      "375/375 [==============================] - 0s 967us/step - loss: 0.1532 - accuracy: 0.9505 - val_loss: 0.2343 - val_accuracy: 0.9179\n",
      "Epoch 14/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9533 - val_loss: 0.2374 - val_accuracy: 0.9139\n",
      "Epoch 15/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9584 - val_loss: 0.2454 - val_accuracy: 0.9082\n",
      "Epoch 16/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9584 - val_loss: 0.2433 - val_accuracy: 0.9122\n",
      "Epoch 17/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9577 - val_loss: 0.2549 - val_accuracy: 0.9073\n",
      "Epoch 18/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9559 - val_loss: 0.2648 - val_accuracy: 0.9056\n",
      "Epoch 19/30\n",
      "375/375 [==============================] - 0s 1000us/step - loss: 0.1198 - accuracy: 0.9610 - val_loss: 0.2655 - val_accuracy: 0.9056\n",
      "Epoch 20/30\n",
      "375/375 [==============================] - 0s 986us/step - loss: 0.1279 - accuracy: 0.9576 - val_loss: 0.2664 - val_accuracy: 0.9065\n",
      "Epoch 21/30\n",
      "375/375 [==============================] - 0s 934us/step - loss: 0.1248 - accuracy: 0.9586 - val_loss: 0.2896 - val_accuracy: 0.9003\n",
      "Epoch 22/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9627 - val_loss: 0.2820 - val_accuracy: 0.9021\n",
      "Epoch 23/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9581 - val_loss: 0.2839 - val_accuracy: 0.9034\n",
      "Epoch 24/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9611 - val_loss: 0.2841 - val_accuracy: 0.9021\n",
      "Epoch 25/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9669 - val_loss: 0.2930 - val_accuracy: 0.9025\n",
      "Epoch 26/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9625 - val_loss: 0.3025 - val_accuracy: 0.9003\n",
      "Epoch 27/30\n",
      "375/375 [==============================] - 0s 964us/step - loss: 0.1129 - accuracy: 0.9611 - val_loss: 0.3095 - val_accuracy: 0.8999\n",
      "Epoch 28/30\n",
      "375/375 [==============================] - 0s 986us/step - loss: 0.1120 - accuracy: 0.9608 - val_loss: 0.3298 - val_accuracy: 0.8972\n",
      "Epoch 29/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9629 - val_loss: 0.3265 - val_accuracy: 0.8981\n",
      "Epoch 30/30\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9626 - val_loss: 0.3277 - val_accuracy: 0.8968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f2b426bbe0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data, train_label,\n",
    "    epochs = 30, \n",
    "    verbose = True,\n",
    "    validation_data = (validation_data, validation_label)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)\n",
    "\n",
    "#The model outputs probabilities, so convert them into classes by mapping them to True/False depending on whether they\n",
    "#are greater than 0.5, then casting this to an integer type will convert True and False into 1s and 0s compatible with \n",
    "#the labels from the testing set\n",
    "predictions = (predictions > 0.5).astype('int').reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.02      0.04      1209\n",
      "           1       0.68      0.98      0.80      2546\n",
      "\n",
      "    accuracy                           0.67      3755\n",
      "   macro avg       0.51      0.50      0.42      3755\n",
      "weighted avg       0.57      0.67      0.56      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_label, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
