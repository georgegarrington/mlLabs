{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "#make the columns as wide as possible so we can see all the text\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  263046056240115712   \n",
       "1  262995061304852481   \n",
       "2  262979898002534400   \n",
       "3  262996108400271360   \n",
       "4  263018881839411200   \n",
       "\n",
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "\n",
       "      userId      imageId(s)        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
       "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_training = pd.read_csv(\"mediaeval-2015-trainingset.txt\", delimiter = \"\\t\")\n",
    "original_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS.... http://t.co/je2hcFpVfN</td>\n",
       "      <td>70824972</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>peay_s</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse from ISS.... http://t.co/oqwtTL0ThS</td>\n",
       "      <td>344707006</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>JaredUcanChange</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>‚Äú@Shyman33: Eclipse from ISS.... http://t.co/C0VfboScRj‚Äù Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!</td>\n",
       "      <td>224839607</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>tpjp1231</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>Eclipse from ISS.... http://t.co/En87OtvsU6</td>\n",
       "      <td>134543073</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Shyman33</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose... http://t.co/yNBN7c4O51\\n\\nLa cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>1150728872</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Epimethee_</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  578854927457349632   \n",
       "1  578874632670953472   \n",
       "2  578891261353984000   \n",
       "3  578846612312748032   \n",
       "4  578975333841551360   \n",
       "\n",
       "                                                                                                             tweetText  \\\n",
       "0                                                    kereeen RT @Shyman33: Eclipse from ISS.... http://t.co/je2hcFpVfN   \n",
       "1                                      Absolutely beautiful! RT @Shyman33: Eclipse from ISS.... http://t.co/oqwtTL0ThS   \n",
       "2                                 ‚Äú@Shyman33: Eclipse from ISS.... http://t.co/C0VfboScRj‚Äù Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!   \n",
       "3                                                                          Eclipse from ISS.... http://t.co/En87OtvsU6   \n",
       "4  @ebonfigli: √âclipse vue de l'ISS... Autre chose... http://t.co/yNBN7c4O51\\n\\nLa cr√©ation divine n'a pas de limite üòç   \n",
       "\n",
       "       userId   imageId(s)         username                       timestamp  \\\n",
       "0    70824972  eclipse_01            peay_s  Fri Mar 20 09:45:43 +0000 2015   \n",
       "1   344707006  eclipse_01   JaredUcanChange  Fri Mar 20 11:04:02 +0000 2015   \n",
       "2   224839607  eclipse_01          tpjp1231  Fri Mar 20 12:10:06 +0000 2015   \n",
       "3   134543073  eclipse_01          Shyman33  Fri Mar 20 09:12:41 +0000 2015   \n",
       "4  1150728872   eclipse_01       Epimethee_  Fri Mar 20 17:44:11 +0000 2015   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeat the same process for the testing dataset\n",
    "original_testing = pd.read_csv(\"mediaeval-2015-testset.txt\", delimiter = \"\\t\")\n",
    "original_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake     47.222806\n",
       "real     34.468025\n",
       "humor    18.309169\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 1\n",
    "\n",
    "#we can see that the dataset is skewed towards fake and humor tweets\n",
    "original_training.label.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake     6742\n",
       "real     4921\n",
       "humor    2614\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 2\n",
    "\n",
    "#(6742 + 2614) - 4921 = 4435 additional real entries needed to make the dataset balanced\n",
    "original_training.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14277 entries, 0 to 14276\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweetId     14277 non-null  int64 \n",
      " 1   tweetText   14277 non-null  object\n",
      " 2   userId      14277 non-null  int64 \n",
      " 3   imageId(s)  14277 non-null  object\n",
      " 4   username    14277 non-null  object\n",
      " 5   timestamp   14277 non-null  object\n",
      " 6   label       14277 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 780.9+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3755 entries, 0 to 3754\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweetId     3755 non-null   int64 \n",
      " 1   tweetText   3755 non-null   object\n",
      " 2   userId      3755 non-null   int64 \n",
      " 3   imageId(s)  3755 non-null   object\n",
      " 4   username    3755 non-null   object\n",
      " 5   timestamp   3755 non-null   object\n",
      " 6   label       3755 non-null   object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 205.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# BOOKMARK 3\n",
    "\n",
    "#there are no non null values to being with we can see\n",
    "original_training.info()\n",
    "print(\"\\n\")\n",
    "original_testing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 4\n",
    "\n",
    "#drop all columns apart from the text and the label as none of the other data appears to be useful\n",
    "original_training = original_training.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "\n",
    "#Do the same for the testing set\n",
    "original_testing = original_testing.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "\n",
    "original_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane http://t.co/daX5YY7X</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Just in time for #halloween a photo of #hurricane #sandy #frankenstorm http://t.co/xquKB4VN</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast http://t.co/c4sceiMt</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #USA http://t.co/iQfEbO1E</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#nyc #hurricane http://t.co/Gv3QxZlq</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "5                                                                         42nd #time #square #NYC #subway #hurricane http://t.co/daX5YY7X   \n",
       "6                                             Just in time for #halloween a photo of #hurricane #sandy #frankenstorm http://t.co/xquKB4VN   \n",
       "7                              Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast http://t.co/c4sceiMt   \n",
       "8                                                                   #sandy #newyork #hurricane #statueofliberty #USA http://t.co/iQfEbO1E   \n",
       "9                                                                                                    #nyc #hurricane http://t.co/Gv3QxZlq   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  \n",
       "5  fake  \n",
       "6  fake  \n",
       "7  fake  \n",
       "8  fake  \n",
       "9  fake  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 5 \n",
    "\n",
    "#we can see that not all the posts are in English\n",
    "original_training[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "\n",
       "  label  lang  \n",
       "0  fake   NaN  \n",
       "1  fake   NaN  \n",
       "2  fake   NaN  \n",
       "3  fake   NaN  \n",
       "4  fake   NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 6\n",
    "\n",
    "#add a column to store the language, initially empty before langdetect populates it\n",
    "original_training[\"lang\"] = np.nan\n",
    "original_testing[\"lang\"] = np.nan\n",
    "original_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/georgegarrington/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/georgegarrington/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import langdetect as l\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import nltk.stem as st\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the languages supported by the stemming algorithm\n",
    "st.SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOKMARK 7 TweetHandler class\n",
    "\n",
    "#responsible for parsing tweets\n",
    "class TweetHandler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        snowball_langs = list(st.SnowballStemmer.languages)\n",
    "        #some languages are supported by stemming but NOT supported by language specific tokenizing,\n",
    "        #only the tokens that are in this set are supported by language specific tokenizing\n",
    "        self.tokenizer_langs = {\"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"it\", \"pt\", \"ru\", \"es\", \"sv\"}\n",
    "        langdetect_langs = [\"ar\", \"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"hu\", \"it\", \"no\", \"illegal\", \"pt\", \"ro\", \"ru\", \"es\", \"sv\"]\n",
    "        #a dictionary to map the corresponding snowball and langdetect properties\n",
    "        self.lang_dict = dict(zip(langdetect_langs, snowball_langs))\n",
    "        #declare some custom stop words\n",
    "        self.custom_stops = [\"http\",\"nhttp\",\"https\"]\n",
    "\n",
    "    #takes a tweet, detects its language, removes any stop words in the language, tokenizes and stems\n",
    "    #specific to the detected language and returns the simplified tokens paired with the language\n",
    "    def parse_tweet(self, tweet):\n",
    "        \n",
    "        try:\n",
    "            lang_prediction = l.detect(tweet)\n",
    "            #the nltk name for the predicted language\n",
    "            nltkprop = self.lang_dict[lang_prediction]\n",
    "        except:\n",
    "            #assume english stopwords and stemming if the language cannot be detected\n",
    "            lang_prediction = \"unknown\"\n",
    "            nltkprop = \"english\"\n",
    "            \n",
    "        # if the language is not supported by the tokenizer (including unkown) then assume tokenizing in English, however stemming\n",
    "        # and stopwords may still be supported in the language that does not support language specific tokenization\n",
    "        # e.g. arabic, hungarian, romanian so tokenize with the english\n",
    "        # version of the algorithm if this is the case and use the stemming and stopwords specific to \n",
    "        # the language if this is available even if the tokenization algorithm isnt\n",
    "        # use a python ternary expression to do this\n",
    "        tokens = nltk.word_tokenize(tweet, language = nltkprop if lang_prediction in self.tokenizer_langs else \"english\")\n",
    "        \n",
    "        #stop words specific to the language\n",
    "        stop_words = set(stopwords.words(nltkprop))\n",
    "        \n",
    "        #stemming algorithm specific to the language detected\n",
    "        stemmer = st.SnowballStemmer(nltkprop)\n",
    "        \n",
    "        # store all tokens to be output as a concatenated string here so that this string\n",
    "        # can later be fed to a CountVectorizer or TfIDFVectorizer , filter out any unwanted tokens \n",
    "        # and don't add them \n",
    "        filtered_tokens = \"\"\n",
    "        \n",
    "        for tok in tokens:\n",
    "            \n",
    "            #remove any hashtags\n",
    "            if tok[0] == '#':\n",
    "                tok = tok[1:]\n",
    "                \n",
    "            #discard non alphanumeric strings containing symbols or pure digits, or stop words\n",
    "            if (not tok.isalnum()) or tok.isdigit() or (tok in stop_words) or tok in self.custom_stops:\n",
    "                continue;\n",
    "            \n",
    "            #carry out stemming specific to the language detected\n",
    "            filtered_tokens += \" \" + stemmer.stem(tok)\n",
    "        \n",
    "        #comment these out when you do not need to check if it works anymore\n",
    "        #print(\"original tokens:\", tokens,\"\\n\")\n",
    "        #print(\"filtered tokens:\", filtered_tokens,\"\\n\")\n",
    "        \n",
    "        return filtered_tokens, lang_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOKMARK 8, transform the dataset from a dataset of tweets into a dataset of labelled tokens in concatenated\n",
    "# string form, along with the detected language\n",
    "\n",
    "def transform_data(arg):\n",
    "\n",
    "    #copy the instance given so we don't change the original instance and can keep it in memory and reuse it \n",
    "    #if necessary\n",
    "    dataset = copy.deepcopy(arg)\n",
    "    th = TweetHandler()\n",
    "    num_rows = dataset.label.size\n",
    "    \n",
    "    #the tweet text will be transformed into tokens so rename the column appropriately\n",
    "    dataset = dataset.rename(columns = {\"tweetText\" : \"tokens\"})\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "\n",
    "        tweet = dataset[\"tokens\"][i]\n",
    "        label = dataset[\"label\"][i]\n",
    "\n",
    "        #disregard the humour information for now, map humor and fake to a single class\n",
    "        if (\"humor\" in label) or (\"fake\" in label):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "            \n",
    "        #for testing\n",
    "        #print(\"The old value of the row is:\",dataset.loc[i],\"\\n\")\n",
    "        \n",
    "        tokens, lang = th.parse_tweet(tweet)\n",
    "        \n",
    "        #replace the row with the simplified tokens, the mapped labels and the detected language\n",
    "        dataset.loc[i] = tokens, label, lang\n",
    "        \n",
    "        #for testing\n",
    "        #print(\"The new value of the row is:\",dataset.loc[i],\"\\n\\n\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data\n",
    "simplified_training = transform_data(original_training)\n",
    "simplified_testing = transform_data(original_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en         76.794845\n",
       "es          9.091546\n",
       "unknown     7.410520\n",
       "fr          1.505919\n",
       "pt          1.134692\n",
       "de          0.854521\n",
       "it          0.672410\n",
       "nl          0.595363\n",
       "ar          0.553338\n",
       "ru          0.427261\n",
       "sv          0.308188\n",
       "no          0.245150\n",
       "da          0.189115\n",
       "fi          0.112068\n",
       "ro          0.063038\n",
       "hu          0.042026\n",
       "Name: lang, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 9\n",
    "\n",
    "#get an idea of how many of each language there are, we can see that it is predominantly english\n",
    "simplified_training.lang.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9356\n",
       "0    4921\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_training.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to convert the sparse matrix produced by the count vectorizer into a dense matrix in order for it \n",
    "#to be able to be used with different algorithms in a pipeline that require a dense matrix and not a sparse matrix\n",
    "class Densifier():\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **kwargs):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! DONT THINK YOU NEED THIS !!!!!\n",
    "\n",
    "def testNaiveBayes():\n",
    "    \n",
    "    #pipeline which always starts with a countvectorizer\n",
    "    pipeline = Pipeline([\n",
    "        ('cv', CountVectorizer()), \n",
    "        #('dt', Densifier()), \n",
    "        ('nb', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    #carry out a grid search for optimal parameter selection. Due to constraints of using my laptop this\n",
    "    #can't be done that extensively, I will leave it to run overnight a few times\n",
    "    clf = GridSearchCV(pipeline, {\n",
    "        'cv__ngram_range': [(1,x) for x in range(9,10)],\n",
    "        'cv__max_features' : [1000* i for i in range(8,12)]\n",
    "        #,'nb__alpha' : [0.5 * i for i in range (5,7)]\n",
    "    }, scoring='f1', verbose = 4) \n",
    "    #make sure we are focusing on maximizing the f1 score and not a different metric, add some verbosity so we can\n",
    "    #see the progress of the grid search to get an idea of how much time it is taking\n",
    "    \n",
    "    #make sure the labels are treated as ints and not objects\n",
    "    clf.fit(simplified_training.tokens,simplified_training.label.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(pipeline, params):\n",
    "    \n",
    "    #carry out a grid search for optimal parameter selection. Due to constraints of using my laptop this\n",
    "    #can't be done that extensively, I will leave it to run overnight a few times\n",
    "    \n",
    "    #make n_jobs -1 so that all cores that are available are used which should hopefully make it quicker\n",
    "    \n",
    "    clf = GridSearchCV(pipeline, params, scoring = \"f1\", verbose = 6, n_jobs = -1)\n",
    "    \n",
    "    #make sure we are focusing on maximizing the f1 score and not a different metric, add some verbosity so we can\n",
    "    #see the progress of the grid search to get an idea of how much time it is taking\n",
    "    \n",
    "    #make sure the labels are treated as ints and not objects\n",
    "    clf.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline which always starts with a countvectorizer\n",
    "mNaiveBayesPipeline = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    #('dt', Densifier()), #used for converting from sparse matrix to dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#the second lot of parameters to try\n",
    "mNaiveBayesParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'cv__max_features' : [500 * i for i in range(2 * 12,2 * 17)],\n",
    "    'nb__alpha' : [0.5 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "#ignore this one for now\n",
    "cNBpipeline = Pipeline([('cv', CountVectorizer()), ('cnb', ComplementNB())])\n",
    "cNBparams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'cv__max_features' : [1000* i for i in range(11,18)],\n",
    "    'cnb__alpha' : [1 * i for i in range (0,9)]\n",
    "}\n",
    "\n",
    "#add: ('pca', PCA()),\n",
    "lsvcPipeline = Pipeline([('cv', CountVectorizer()), ('df', Densifier()), ('pca', PCA()), ('lsvc', LinearSVC())])\n",
    "lsvcParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(10,15)],\n",
    "    'cv__max_features' : [1000* i for i in range(14,18)],\n",
    "    'lsvc__C' : [1 * i for i in range(0,3)]\n",
    "}\n",
    "\n",
    "rForestPipeline = Pipeline([('cv', CountVectorizer()), ('rf', RandomForestClassifier())])\n",
    "rForestParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(13,19)],\n",
    "    'cv__max_features' : [1000* i for i in range(10,17)],\n",
    "    'rf__n_estimators' : [10 * i for i in range(10,15)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportScores(clf):\n",
    "    \n",
    "    print(\"---- RESULTS ----\",\"\\n\")\n",
    "    print(\"The algorithm being optimised was:\",clf.estimator.steps[-1])\n",
    "    print(\"The best parameters found were:\", clf.best_params_)\n",
    "    y_test_true = simplified_testing.label\n",
    "    y_test_predictions = clf.predict(simplified_testing.tokens)\n",
    "    print(\"Score report:\\n\")\n",
    "    #prevent the chance of any of the lists being treated as objects\n",
    "    print(classification_report(y_test_true.astype('int'), y_test_predictions.astype('int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST SEARCH: ngram 3-16, max features 5-16, alpha 0-9, RESULT: ngram: (1,15), features: 15000, alpha: 0\n",
    "#so now try making the features and ngram even LARGER! got an accuracy of 81\n",
    "\n",
    "#SECOND SEARCH: similar not much change, ngram 14-18, features 13-19, alpha 0-2\n",
    "#RESULTS: features 14000, ngram 15, alpha 0\n",
    "mNBResult = gridSearch(mNaiveBayesPipeline, mNaiveBayesParams)\n",
    "reportScores(mNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters found from alpha 0-9, max features 5k-16k and ngram (3,16): \n",
    "# alpha 2, max features 15000, ngram (1,15) with accuracy of 66. Not very good\n",
    "cNBResult = gridSearch(cNBpipeline, cNBparams) \n",
    "reportScores(cNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 66 was okay... tried ngram 13-17, max features 13-17 and estimators 80-110 and best was 13000 features, ngram 1,15 and n estimators 110\n",
    "#so try again with everything a bit higher\n",
    "rForestResult = gridSearch(rForestPipeline, rForestParams)\n",
    "reportScores(rForestResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters searched: ngram 10-12, max features 10-12, C 0-2, found: ngram 11, max features 12000\n",
    "#introducing PCA made the computer die so don't use it again\n",
    "lsvcResult = gridSearch(lsvcPipeline, lsvcParams)\n",
    "reportScores(lsvcResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA ONES TO DO AFTERWARDS, KEEP ADDING THEM BELOW THEN COMMENT THE RESULTS UP ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 210 candidates, totalling 1050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 30.7min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 40.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed: 42.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('rf', RandomForestClassifier())\n",
      "The best parameters found were: {'cv__max_features': 12000, 'cv__ngram_range': (1, 15), 'rf__n_estimators': 100}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.05      0.08      1209\n",
      "           1       0.68      0.94      0.79      2546\n",
      "\n",
      "    accuracy                           0.65      3755\n",
      "   macro avg       0.48      0.49      0.43      3755\n",
      "weighted avg       0.55      0.65      0.56      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rForestResult = gridSearch(rForestPipeline, rForestParams)\n",
    "reportScores(rForestResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('lsvc', LinearSVC())\n",
      "The best parameters found were: {'cv__max_features': 15000, 'cv__ngram_range': (1, 13), 'lsvc__C': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.80      0.71      1209\n",
      "           1       0.89      0.78      0.84      2546\n",
      "\n",
      "    accuracy                           0.79      3755\n",
      "   macro avg       0.77      0.79      0.77      3755\n",
      "weighted avg       0.81      0.79      0.79      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lsvcResult = gridSearch(lsvcPipeline, lsvcParams)\n",
    "reportScores(lsvcResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mNBResult = gridSearch(mNaiveBayesPipeline, mNaiveBayesParams)\n",
    "reportScores(mNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW TRY DOING THE SAME BUT USING TFIDF INSTEAD\n",
    "\n",
    "mnbtPipeline = Pipeline([\n",
    "    ('tf', TfidfVectorizer()), \n",
    "    #('dt', Densifier()), #used for converting from sparse matrix to dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#the second lot of parameters to try\n",
    "mnbtParams = {\n",
    "    'tf__ngram_range': [(1,0.5 * x) for x in range(2 * 12,2 *17)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'nb__alpha' : [0.25 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "#ignore this one for now\n",
    "cnbtPipeline = Pipeline([('tf', TfidfVectorizer()), ('cnb', ComplementNB())])\n",
    "cnbtParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'cnb__alpha' : [1 * i for i in range (0,9)]\n",
    "}\n",
    "\n",
    "#add: ('pca', PCA()),\n",
    "lsvctPipeline = Pipeline([('tf', TfidfVectorizer()), ('df', Densifier()), ('lsvc', LinearSVC())])\n",
    "lsvctParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(10,15)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'lsvc__C' : [1 * i for i in range(0,3)]\n",
    "}\n",
    "\n",
    "rftPipeline = Pipeline([('tf', TfidfVectorizer()), ('rf', RandomForestClassifier())])\n",
    "rftParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(13,19)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'rf__n_estimators' : [10 * i for i in range(10,15)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST SEARCH: ngram 14-18, alpha 0-6, RESULT: alpha = 0.5, ngram = 14, norm = l2, accuracy = 87\n",
    "\n",
    "#SECOND SEARCH: \n",
    "mnbtResult = gridSearch(mnbtPipeline, mnbtParams)\n",
    "reportScores(mnbtResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnbtResult = gridSearch(cnbtPipeline, cnbtParams)\n",
    "reportScores(cnbtResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvctResult = gridSearch(lsvctPipeline, lsvctParams)\n",
    "reportScores(lsvctResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 39.7min\n"
     ]
    }
   ],
   "source": [
    "#FIRST SEARCH: \n",
    "rftResult = gridSearch(rftPipeline, rftParams)\n",
    "reportScores(rftResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
