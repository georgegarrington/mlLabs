{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "#make the columns as wide as possible so we can see all the text\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  263046056240115712   \n",
       "1  262995061304852481   \n",
       "2  262979898002534400   \n",
       "3  262996108400271360   \n",
       "4  263018881839411200   \n",
       "\n",
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "\n",
       "      userId      imageId(s)        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
       "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_training = pd.read_csv(\"mediaeval-2015-trainingset.txt\", delimiter = \"\\t\")\n",
    "original_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS.... http://t.co/je2hcFpVfN</td>\n",
       "      <td>70824972</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>peay_s</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse from ISS.... http://t.co/oqwtTL0ThS</td>\n",
       "      <td>344707006</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>JaredUcanChange</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>‚Äú@Shyman33: Eclipse from ISS.... http://t.co/C0VfboScRj‚Äù Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!</td>\n",
       "      <td>224839607</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>tpjp1231</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>Eclipse from ISS.... http://t.co/En87OtvsU6</td>\n",
       "      <td>134543073</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Shyman33</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose... http://t.co/yNBN7c4O51\\n\\nLa cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>1150728872</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Epimethee_</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  578854927457349632   \n",
       "1  578874632670953472   \n",
       "2  578891261353984000   \n",
       "3  578846612312748032   \n",
       "4  578975333841551360   \n",
       "\n",
       "                                                                                                             tweetText  \\\n",
       "0                                                    kereeen RT @Shyman33: Eclipse from ISS.... http://t.co/je2hcFpVfN   \n",
       "1                                      Absolutely beautiful! RT @Shyman33: Eclipse from ISS.... http://t.co/oqwtTL0ThS   \n",
       "2                                 ‚Äú@Shyman33: Eclipse from ISS.... http://t.co/C0VfboScRj‚Äù Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!   \n",
       "3                                                                          Eclipse from ISS.... http://t.co/En87OtvsU6   \n",
       "4  @ebonfigli: √âclipse vue de l'ISS... Autre chose... http://t.co/yNBN7c4O51\\n\\nLa cr√©ation divine n'a pas de limite üòç   \n",
       "\n",
       "       userId   imageId(s)         username                       timestamp  \\\n",
       "0    70824972  eclipse_01            peay_s  Fri Mar 20 09:45:43 +0000 2015   \n",
       "1   344707006  eclipse_01   JaredUcanChange  Fri Mar 20 11:04:02 +0000 2015   \n",
       "2   224839607  eclipse_01          tpjp1231  Fri Mar 20 12:10:06 +0000 2015   \n",
       "3   134543073  eclipse_01          Shyman33  Fri Mar 20 09:12:41 +0000 2015   \n",
       "4  1150728872   eclipse_01       Epimethee_  Fri Mar 20 17:44:11 +0000 2015   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeat the same process for the testing dataset\n",
    "original_testing = pd.read_csv(\"mediaeval-2015-testset.txt\", delimiter = \"\\t\")\n",
    "original_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake     47.222806\n",
       "real     34.468025\n",
       "humor    18.309169\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 1\n",
    "\n",
    "#we can see that the dataset is skewed towards fake and humor tweets\n",
    "original_training.label.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake     6742\n",
       "real     4921\n",
       "humor    2614\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 2\n",
    "\n",
    "#(6742 + 2614) - 4921 = 4435 additional real entries needed to make the dataset balanced\n",
    "original_training.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14277 entries, 0 to 14276\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweetId     14277 non-null  int64 \n",
      " 1   tweetText   14277 non-null  object\n",
      " 2   userId      14277 non-null  int64 \n",
      " 3   imageId(s)  14277 non-null  object\n",
      " 4   username    14277 non-null  object\n",
      " 5   timestamp   14277 non-null  object\n",
      " 6   label       14277 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 780.9+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3755 entries, 0 to 3754\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweetId     3755 non-null   int64 \n",
      " 1   tweetText   3755 non-null   object\n",
      " 2   userId      3755 non-null   int64 \n",
      " 3   imageId(s)  3755 non-null   object\n",
      " 4   username    3755 non-null   object\n",
      " 5   timestamp   3755 non-null   object\n",
      " 6   label       3755 non-null   object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 205.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# BOOKMARK 3\n",
    "\n",
    "#there are no non null values to being with we can see\n",
    "original_training.info()\n",
    "print(\"\\n\")\n",
    "original_testing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 4\n",
    "\n",
    "#drop all columns apart from the text and the label as none of the other data appears to be useful\n",
    "original_training = original_training.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "\n",
    "#Do the same for the testing set\n",
    "original_testing = original_testing.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "\n",
    "original_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane http://t.co/daX5YY7X</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Just in time for #halloween a photo of #hurricane #sandy #frankenstorm http://t.co/xquKB4VN</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast http://t.co/c4sceiMt</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #USA http://t.co/iQfEbO1E</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#nyc #hurricane http://t.co/Gv3QxZlq</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "5                                                                         42nd #time #square #NYC #subway #hurricane http://t.co/daX5YY7X   \n",
       "6                                             Just in time for #halloween a photo of #hurricane #sandy #frankenstorm http://t.co/xquKB4VN   \n",
       "7                              Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast http://t.co/c4sceiMt   \n",
       "8                                                                   #sandy #newyork #hurricane #statueofliberty #USA http://t.co/iQfEbO1E   \n",
       "9                                                                                                    #nyc #hurricane http://t.co/Gv3QxZlq   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  \n",
       "5  fake  \n",
       "6  fake  \n",
       "7  fake  \n",
       "8  fake  \n",
       "9  fake  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 5 \n",
    "\n",
    "#we can see that not all the posts are in English\n",
    "original_training[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN   \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                          Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ http://t.co/Ex61doZk   \n",
       "\n",
       "  label  lang  \n",
       "0  fake   NaN  \n",
       "1  fake   NaN  \n",
       "2  fake   NaN  \n",
       "3  fake   NaN  \n",
       "4  fake   NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 6\n",
    "\n",
    "#add a column to store the language, initially empty before langdetect populates it\n",
    "original_training[\"lang\"] = np.nan\n",
    "original_testing[\"lang\"] = np.nan\n",
    "original_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\George\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\George\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import langdetect as l\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import nltk.stem as st\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the languages supported by the stemming algorithm\n",
    "st.SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOKMARK 7 TweetHandler class\n",
    "\n",
    "#responsible for parsing tweets\n",
    "class TweetHandler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        snowball_langs = list(st.SnowballStemmer.languages)\n",
    "        #some languages are supported by stemming but NOT supported by language specific tokenizing,\n",
    "        #only the tokens that are in this set are supported by language specific tokenizing\n",
    "        self.tokenizer_langs = {\"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"it\", \"pt\", \"ru\", \"es\", \"sv\"}\n",
    "        langdetect_langs = [\"ar\", \"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"hu\", \"it\", \"no\", \"illegal\", \"pt\", \"ro\", \"ru\", \"es\", \"sv\"]\n",
    "        #a dictionary to map the corresponding snowball and langdetect properties\n",
    "        self.lang_dict = dict(zip(langdetect_langs, snowball_langs))\n",
    "        #declare some custom stop words\n",
    "        self.custom_stops = [\"http\",\"nhttp\",\"https\"]\n",
    "\n",
    "    #takes a tweet, detects its language, removes any stop words in the language, tokenizes and stems\n",
    "    #specific to the detected language and returns the simplified tokens paired with the language\n",
    "    def parse_tweet(self, tweet):\n",
    "        \n",
    "        try:\n",
    "            lang_prediction = l.detect(tweet)\n",
    "            #the nltk name for the predicted language\n",
    "            nltkprop = self.lang_dict[lang_prediction]\n",
    "        except:\n",
    "            #assume english stopwords and stemming if the language cannot be detected\n",
    "            lang_prediction = \"unknown\"\n",
    "            nltkprop = \"english\"\n",
    "            \n",
    "        # if the language is not supported by the tokenizer (including unkown) then assume tokenizing in English, however stemming\n",
    "        # and stopwords may still be supported in the language that does not support language specific tokenization\n",
    "        # e.g. arabic, hungarian, romanian so tokenize with the english\n",
    "        # version of the algorithm if this is the case and use the stemming and stopwords specific to \n",
    "        # the language if this is available even if the tokenization algorithm isnt\n",
    "        # use a python ternary expression to do this\n",
    "        tokens = nltk.word_tokenize(tweet, language = nltkprop if lang_prediction in self.tokenizer_langs else \"english\")\n",
    "        \n",
    "        #stop words specific to the language\n",
    "        stop_words = set(stopwords.words(nltkprop))\n",
    "        \n",
    "        #stemming algorithm specific to the language detected\n",
    "        stemmer = st.SnowballStemmer(nltkprop)\n",
    "        \n",
    "        # store all tokens to be output as a concatenated string here so that this string\n",
    "        # can later be fed to a CountVectorizer or TfIDFVectorizer , filter out any unwanted tokens \n",
    "        # and don't add them \n",
    "        filtered_tokens = \"\"\n",
    "        \n",
    "        for tok in tokens:\n",
    "            \n",
    "            #remove any hashtags\n",
    "            if tok[0] == '#':\n",
    "                tok = tok[1:]\n",
    "                \n",
    "            #discard non alphanumeric strings containing symbols or pure digits, or stop words\n",
    "            if (not tok.isalnum()) or tok.isdigit() or (tok in stop_words) or tok in self.custom_stops:\n",
    "                continue;\n",
    "            \n",
    "            #carry out stemming specific to the language detected\n",
    "            filtered_tokens += \" \" + stemmer.stem(tok)\n",
    "        \n",
    "        #comment these out when you do not need to check if it works anymore\n",
    "        #print(\"original tokens:\", tokens,\"\\n\")\n",
    "        #print(\"filtered tokens:\", filtered_tokens,\"\\n\")\n",
    "        \n",
    "        return filtered_tokens, lang_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOKMARK 8, transform the dataset from a dataset of tweets into a dataset of labelled tokens in concatenated\n",
    "# string form, along with the detected language\n",
    "\n",
    "def transform_data(arg):\n",
    "\n",
    "    #copy the instance given so we don't change the original instance and can keep it in memory and reuse it \n",
    "    #if necessary\n",
    "    dataset = copy.deepcopy(arg)\n",
    "    th = TweetHandler()\n",
    "    num_rows = dataset.label.size\n",
    "    \n",
    "    #the tweet text will be transformed into tokens so rename the column appropriately\n",
    "    dataset = dataset.rename(columns = {\"tweetText\" : \"tokens\"})\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "\n",
    "        tweet = dataset[\"tokens\"][i]\n",
    "        label = dataset[\"label\"][i]\n",
    "\n",
    "        #disregard the humour information for now, map humor and fake to a single class\n",
    "        if (\"humor\" in label) or (\"fake\" in label):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "            \n",
    "        #for testing\n",
    "        #print(\"The old value of the row is:\",dataset.loc[i],\"\\n\")\n",
    "        \n",
    "        tokens, lang = th.parse_tweet(tweet)\n",
    "        \n",
    "        #replace the row with the simplified tokens, the mapped labels and the detected language\n",
    "        dataset.loc[i] = tokens, label, lang\n",
    "        \n",
    "        #for testing\n",
    "        #print(\"The new value of the row is:\",dataset.loc[i],\"\\n\\n\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data\n",
    "simplified_training = transform_data(original_training)\n",
    "simplified_testing = transform_data(original_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en         76.745815\n",
       "es          9.028507\n",
       "unknown     7.438538\n",
       "fr          1.519927\n",
       "pt          1.134692\n",
       "de          0.917560\n",
       "it          0.707432\n",
       "nl          0.609372\n",
       "ar          0.553338\n",
       "ru          0.441269\n",
       "sv          0.294179\n",
       "no          0.266162\n",
       "da          0.154094\n",
       "fi          0.112068\n",
       "ro          0.042026\n",
       "hu          0.035021\n",
       "Name: lang, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOKMARK 9\n",
    "\n",
    "#get an idea of how many of each language there are, we can see that it is predominantly english\n",
    "simplified_training.lang.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9356\n",
       "0    4921\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_training.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to convert the sparse matrix produced by the tfidf vectorizer into a dense matrix in order for it \n",
    "#to be able to be used with different algorithms in a pipeline that require a dense matrix and not a sparse matrix\n",
    "class Densifier():\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **kwargs):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a pipeline and some parameters, perform a grid search with these\n",
    "def grid_search(pipeline, params):\n",
    "    \n",
    "    #make sure we are focusing on maximizing the f1 score and not a different metric, add some verbosity so we can\n",
    "    #see the progress of the grid search to get an idea of how much time it is taking\n",
    "    \n",
    "    #make n_jobs -1 so that all cores that are available cores are used which should hopefully make it quicker\n",
    "    \n",
    "    clf = GridSearchCV(pipeline, params, scoring = \"f1\", verbose = 6, n_jobs = -1)\n",
    "    \n",
    "    #grid_search does not need to take the training data as an argument, we can always assume that simplified_training\n",
    "    #is ready in memory so any algorithm will only ever need to use this to grid search therefore we can implicitly\n",
    "    #reference it like this without having to declare it as an argument to the function\n",
    "    \n",
    "    #make sure the labels are treated as ints and not objects\n",
    "    clf.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Simple function to report the results of the grid search\n",
    "def report_scores(clf):\n",
    "    \n",
    "    print(\"---- RESULTS ----\",\"\\n\")\n",
    "    print(\"The algorithm being optimised was:\",clf.estimator.steps[-1])\n",
    "    print(\"The best parameters found were:\", clf.best_params_)\n",
    "    y_test_true = simplified_testing.label\n",
    "    y_test_predictions = clf.predict(simplified_testing.tokens)\n",
    "    print(\"Score report:\\n\")\n",
    "    #prevent the chance of any of the lists being treated as objects\n",
    "    print(classification_report(y_test_true.astype('int'), y_test_predictions.astype('int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   33.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 1), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.84      0.51      1209\n",
      "           1       0.81      0.31      0.45      2546\n",
      "\n",
      "    accuracy                           0.48      3755\n",
      "   macro avg       0.59      0.58      0.48      3755\n",
      "weighted avg       0.67      0.48      0.47      3755\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:   42.5s finished\n"
     ]
    }
   ],
   "source": [
    "#Start with the multinomial naive bayes algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNB_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "mNBResult_V1 = grid_search(mNB_pipelineV1, mNB_paramsV1)\n",
    "report_scores(mNBResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 252 candidates, totalling 1260 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1260 out of 1260 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 10000, 'cv__ngram_range': (1, 7), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72      1209\n",
      "           1       0.89      0.81      0.85      2546\n",
      "\n",
      "    accuracy                           0.80      3755\n",
      "   macro avg       0.77      0.80      0.78      3755\n",
      "weighted avg       0.82      0.80      0.80      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second iteration with the multinomial naive bayes algorithm,\n",
    "# trying ngrams from (1,1) to (1,9), 4000 max features in the vocabulary to 10000 max features\n",
    "# in increments of 1000, and try values of 2-5 for alpha\n",
    "\n",
    "mNB_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,10)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,11)],\n",
    "    'nb__alpha' : [i for i in range (2,6)]\n",
    "}\n",
    "\n",
    "mNBResult_V2 = grid_search(mNB_pipelineV2, mNB_paramsV2)\n",
    "report_scores(mNBResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 14000, 'cv__ngram_range': (1, 7), 'nb__alpha': 4}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72      1209\n",
      "           1       0.89      0.81      0.85      2546\n",
      "\n",
      "    accuracy                           0.80      3755\n",
      "   macro avg       0.78      0.80      0.78      3755\n",
      "weighted avg       0.82      0.80      0.81      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Third iteration with the multinomial naive bayes algorithm,\n",
    "# trying ngrams from (1,6) to (1,10), 9000 max features in the vocabulary up to 14000 max features\n",
    "# in increments of 1000, and try values of 2-4 for alpha\n",
    "\n",
    "mNB_pipelineV3 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV3 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (6,11)],\n",
    "    'cv__max_features' : [1000 * i for i in range (9,15)],\n",
    "    'nb__alpha' : [i for i in range (2,5)]\n",
    "}\n",
    "\n",
    "mNBResult_V3 = grid_search(mNB_pipelineV3, mNB_paramsV3)\n",
    "report_scores(mNBResult_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   41.0s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 18000, 'cv__ngram_range': (1, 10), 'nb__alpha': 4}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74      1209\n",
      "           1       0.89      0.84      0.87      2546\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.80      0.81      0.80      3755\n",
      "weighted avg       0.83      0.82      0.83      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fourth iteration with the multinomial naive bayes algorithm,\n",
    "# trying ngrams from (1,5) to (1,13), 13000 max features in the vocabulary up to 18000 max features\n",
    "# in increments of 1000, and try values of 1-6 for alpha\n",
    "\n",
    "mNB_pipelineV4 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV4 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (5,13)],\n",
    "    'cv__max_features' : [1000 * i for i in range (13,19)],\n",
    "    'nb__alpha' : [i for i in range (1,7)]\n",
    "}\n",
    "\n",
    "mNBResult_V4 = grid_search(mNB_pipelineV4, mNB_paramsV4)\n",
    "report_scores(mNBResult_V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   25.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 22000, 'cv__ngram_range': (1, 11), 'nb__alpha': 4}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.71      1209\n",
      "           1       0.89      0.79      0.84      2546\n",
      "\n",
      "    accuracy                           0.79      3755\n",
      "   macro avg       0.76      0.79      0.77      3755\n",
      "weighted avg       0.81      0.79      0.79      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fifth iteration with the multinomial naive bayes algorithm,\n",
    "# trying ngrams from (1,8) to (1,12), 17000 max features in the vocabulary up to 22000 max features\n",
    "# in increments of 1000, and try values of 3-6 for alpha\n",
    "\n",
    "mNB_pipelineV5 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV5 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (8,13)],\n",
    "    'cv__max_features' : [1000 * i for i in range (17,23)],\n",
    "    'nb__alpha' : [i for i in range (3,7)]\n",
    "}\n",
    "\n",
    "mNBResult_V5 = grid_search(mNB_pipelineV5, mNB_paramsV5)\n",
    "report_scores(mNBResult_V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 23000, 'cv__ngram_range': (1, 11), 'nb__alpha': 4}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.71      1209\n",
      "           1       0.89      0.79      0.84      2546\n",
      "\n",
      "    accuracy                           0.79      3755\n",
      "   macro avg       0.76      0.79      0.77      3755\n",
      "weighted avg       0.81      0.79      0.79      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sixth iteration with the multinomial naive bayes algorithm,\n",
    "# trying ngrams from (1,10) to (1,12), 21000 max features in the vocabulary up to 24000 max features\n",
    "# in increments of 1000, and try values of 2-5 for alpha\n",
    "\n",
    "mNB_pipelineV6 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV6 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (10,13)],\n",
    "    'cv__max_features' : [1000 * i for i in range (21,25)],\n",
    "    'nb__alpha' : [i for i in range (2,6)]\n",
    "}\n",
    "\n",
    "#we appear to have found a plateau, as the results are largely the same as the last search\n",
    "mNBResult_V6 = grid_search(mNB_pipelineV6, mNB_paramsV6)\n",
    "report_scores(mNBResult_V6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed:  9.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 25000, 'cv__ngram_range': (1, 10), 'nb__alpha': 3.5}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.79      0.62      1209\n",
      "           1       0.86      0.64      0.74      2546\n",
      "\n",
      "    accuracy                           0.69      3755\n",
      "   macro avg       0.69      0.72      0.68      3755\n",
      "weighted avg       0.75      0.69      0.70      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seventh iteration with the multinomial naive bayes algorithm, search around our \"optimal\" we have found and make it more\n",
    "# precise by searching in increments of 0.5 for alpha and 500 for max features to see if this tunes it to be even better.\n",
    "# trying ngrams from (1,8) to (1,12), 17000 max features in the vocabulary up to 22000 max features\n",
    "# in increments of 500, and try values of 2-7 for alpha in increments of 0.5\n",
    "\n",
    "# {'cv__max_features': 18000, 'cv__ngram_range': (1, 10), 'nb__alpha': 4}\n",
    "\n",
    "mNB_pipelineV7 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV7 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (8,13)],\n",
    "    'cv__max_features' : [500 * i for i in range (2 * 20,2 * 26)],\n",
    "    'nb__alpha' : [0.5 * i for i in range (2 * 2,7 * 2)]\n",
    "}\n",
    "\n",
    "mNBResult_V7 = grid_search(mNB_pipelineV7, mNB_paramsV7)\n",
    "report_scores(mNBResult_V7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY WITHOUTTTT SPECIFYING MAX FEATURES\n",
    "\n",
    "#Start with the multinomial naive bayes algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNB_pipelineV0 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNB_paramsV0 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "mNBResult_V0 = grid_search(mNB_pipelineV0, mNB_paramsV0)\n",
    "report_scores(mNBResult_V0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   34.7s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:   43.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 5), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.87      0.55      1209\n",
      "           1       0.86      0.40      0.54      2546\n",
      "\n",
      "    accuracy                           0.55      3755\n",
      "   macro avg       0.63      0.63      0.55      3755\n",
      "weighted avg       0.71      0.55      0.55      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now try the ComplementNB algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "cNB_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "cNBResult_V1 = grid_search(cNB_pipelineV1, cNB_paramsV1)\n",
    "report_scores(cNBResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   38.7s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 9000, 'cv__ngram_range': (1, 9), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.85      0.62      1209\n",
      "           1       0.89      0.57      0.70      2546\n",
      "\n",
      "    accuracy                           0.66      3755\n",
      "   macro avg       0.69      0.71      0.66      3755\n",
      "weighted avg       0.76      0.66      0.67      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second iteration with the complement naive bayes algorithm, performance was poor and the best parameters\n",
    "# found were at the maxima of the ranges specified, so try again with larger ranges.\n",
    "# trying ngrams from (1,4) to (1,9), 4000 max features in the vocabulary up to 9000 max features\n",
    "# in increments of 1000, and try values of 1-3 for alpha in increments of 1\n",
    "\n",
    "cNB_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (4,10)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,10)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "cNBResult_V2 = grid_search(cNB_pipelineV2, cNB_paramsV2)\n",
    "report_scores(cNBResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 196 candidates, totalling 980 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.3s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 980 out of 980 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 14000, 'cv__ngram_range': (1, 11), 'nb__alpha': 2}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.85      0.62      1209\n",
      "           1       0.89      0.58      0.70      2546\n",
      "\n",
      "    accuracy                           0.66      3755\n",
      "   macro avg       0.69      0.71      0.66      3755\n",
      "weighted avg       0.76      0.66      0.67      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Third iteration with the complement naive bayes algorithm, performance is still quite poor and the best parameters\n",
    "# found were at the maxima of the ranges specified, so try again with larger ranges.\n",
    "# trying ngrams from (1,4) to (1,9), 4000 max features in the vocabulary up to 9000 max features\n",
    "# in increments of 1000, and try values of 1-3 for alpha in increments of 1\n",
    "\n",
    "cNB_pipelineV3 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV3 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (8,15)],\n",
    "    'cv__max_features' : [1000 * i for i in range (8,15)],\n",
    "    'nb__alpha' : [i for i in range (2,6)]\n",
    "}\n",
    "\n",
    "cNBResult_V3 = grid_search(cNB_pipelineV3, cNB_paramsV3)\n",
    "report_scores(cNBResult_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   49.4s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 18000, 'cv__ngram_range': (1, 10), 'nb__alpha': 2}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.85      0.62      1209\n",
      "           1       0.89      0.58      0.70      2546\n",
      "\n",
      "    accuracy                           0.67      3755\n",
      "   macro avg       0.69      0.71      0.66      3755\n",
      "weighted avg       0.76      0.67      0.68      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Third iteration with the complement naive bayes algorithm, performance is still quite poor and the best parameters\n",
    "# found were at the maxima of the ranges specified, so try again with larger ranges.\n",
    "# trying ngrams from (1,4) to (1,9), 4000 max features in the vocabulary up to 9000 max features\n",
    "# in increments of 1000, and try values of 1-3 for alpha in increments of 1\n",
    "\n",
    "cNB_pipelineV4 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV4 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (10,15)],\n",
    "    'cv__max_features' : [1000 * i for i in range (13,19)],\n",
    "    'nb__alpha' : [i for i in range (1,5)]\n",
    "}\n",
    "\n",
    "cNBResult_V4 = grid_search(cNB_pipelineV4, cNB_paramsV4)\n",
    "report_scores(cNBResult_V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.2s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1280 out of 1280 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 23000, 'cv__ngram_range': (1, 13), 'nb__alpha': 2}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.85      0.55      1209\n",
      "           1       0.85      0.41      0.55      2546\n",
      "\n",
      "    accuracy                           0.55      3755\n",
      "   macro avg       0.63      0.63      0.55      3755\n",
      "weighted avg       0.71      0.55      0.55      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Third iteration with the complement naive bayes algorithm, performance is still quite poor and the best parameters\n",
    "# found were at the maxima of the ranges specified, so try again with larger ranges.\n",
    "# trying ngrams from (1,4) to (1,9), 4000 max features in the vocabulary up to 9000 max features\n",
    "# in increments of 1000, and try values of 1-3 for alpha in increments of 1\n",
    "\n",
    "cNB_pipelineV5 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV5 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (7,15)],\n",
    "    'cv__max_features' : [1000 * i for i in range (17,25)],\n",
    "    'nb__alpha' : [i for i in range (1,5)]\n",
    "}\n",
    "\n",
    "cNBResult_V5 = grid_search(cNB_pipelineV5, cNB_paramsV5)\n",
    "report_scores(cNBResult_V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Third iteration with the complement naive bayes algorithm, performance is still quite poor and the best parameters\n",
    "# found were at the maxima of the ranges specified, so try again with larger ranges.\n",
    "# trying ngrams from (1,4) to (1,9), 4000 max features in the vocabulary up to 9000 max features\n",
    "# in increments of 1000, and try values of 1-3 for alpha in increments of 1\n",
    "\n",
    "cNB_pipelineV5 = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNB_paramsV5 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (7,15)],\n",
    "    'cv__max_features' : [1000 * i for i in range (17,25)],\n",
    "    'nb__alpha' : [i for i in range (1,5)]\n",
    "}\n",
    "\n",
    "cNBResult_V5 = grid_search(cNB_pipelineV5, cNB_paramsV5)\n",
    "report_scores(cNBResult_V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:   41.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', BernoulliNB())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 2), 'nb__alpha': 2}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.72      1209\n",
      "           1       0.86      0.87      0.87      2546\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.79      0.79      0.79      3755\n",
      "weighted avg       0.82      0.82      0.82      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now try the BernoulliNB algorithm commonly used for text classification,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "bNB_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer(binary = True)), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('nb', BernoulliNB())\n",
    "])\n",
    "\n",
    "bNB_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "bNBResult_V1 = grid_search(bNB_pipelineV1, bNB_paramsV1)\n",
    "report_scores(bNBResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 252 candidates, totalling 1260 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   43.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1260 out of 1260 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', BernoulliNB())\n",
      "The best parameters found were: {'cv__max_features': 9000, 'cv__ngram_range': (1, 2), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.73      0.53      1209\n",
      "           1       0.80      0.50      0.62      2546\n",
      "\n",
      "    accuracy                           0.58      3755\n",
      "   macro avg       0.61      0.62      0.57      3755\n",
      "weighted avg       0.67      0.58      0.59      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It performs surprisingly excellent even with very limited parameters! Now try a second\n",
    "# search with ngrams from (1,1) to (1,9), 4000 max features in the vocabulary to 10000 max features\n",
    "# in increments of 1000, and try values of 1-4 for alpha\n",
    "\n",
    "bNB_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer(binary = True)), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('nb', BernoulliNB())\n",
    "])\n",
    "\n",
    "bNB_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,10)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,11)],\n",
    "    'nb__alpha' : [i for i in range (1,5)]\n",
    "}\n",
    "\n",
    "bNBResult_V2 = grid_search(bNB_pipelineV2, bNB_paramsV2)\n",
    "report_scores(bNBResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   49.5s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('svc', LinearSVC())\n",
      "The best parameters found were: {'cv__max_features': 3000, 'cv__ngram_range': (1, 1), 'svc__C': 1.0}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.81      0.73      1209\n",
      "           1       0.90      0.81      0.85      2546\n",
      "\n",
      "    accuracy                           0.81      3755\n",
      "   macro avg       0.78      0.81      0.79      3755\n",
      "weighted avg       0.82      0.81      0.81      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now try a support vector machine classifier, called LinearSVC in ScikitLearn. \n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "svc_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('svc', LinearSVC())\n",
    "])\n",
    "\n",
    "svc_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'svc__C' : [1.0, 10.0, 100.0, 1000.0]\n",
    "}\n",
    "\n",
    "svcResult_V1 = grid_search(svc_pipelineV1, svc_paramsV1)\n",
    "report_scores(svcResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   59.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1215 out of 1215 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('svc', LinearSVC())\n",
      "The best parameters found were: {'cv__max_features': 8000, 'cv__ngram_range': (1, 1), 'svc__C': 1.0}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.78      0.69      1209\n",
      "           1       0.88      0.77      0.82      2546\n",
      "\n",
      "    accuracy                           0.77      3755\n",
      "   macro avg       0.75      0.77      0.75      3755\n",
      "weighted avg       0.79      0.77      0.78      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now try a support vector machine classifier, called LinearSVC in ScikitLearn. \n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "svc_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer()), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('svc', LinearSVC())\n",
    "])\n",
    "\n",
    "svc_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,10)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,10)],\n",
    "    'svc__C' : [1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "svcResult_V2 = grid_search(svc_pipelineV2, svc_paramsV2)\n",
    "report_scores(svcResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 125 candidates, totalling 625 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed: 26.7min\n",
      "[Parallel(n_jobs=-1)]: Done 625 out of 625 | elapsed: 28.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('rf', RandomForestClassifier())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 2), 'rf__n_estimators': 300}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.07      0.11      1209\n",
      "           1       0.68      0.93      0.79      2546\n",
      "\n",
      "    accuracy                           0.65      3755\n",
      "   macro avg       0.50      0.50      0.45      3755\n",
      "weighted avg       0.56      0.65      0.57      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now try the RandomForest algorithm,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "# do this overnight :/ hehe\n",
    "\n",
    "rf_pipelineV1 = Pipeline([\n",
    "    ('cv', CountVectorizer()), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "rf_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'rf__n_estimators' : [100 * x for x in range (1,6)]\n",
    "    #'rf__max_depth' : [25, 50, 75, None],\n",
    "    #'rf__max_features' : [\"auto\", \"sqrt\"],\n",
    "    #'rf__min_samples_leaf' : [1,2,4],\n",
    "    #'rf_min_samlpes_split' : [2, 5, 10],\n",
    "    #'rf__bootstrap' : [True, False]\n",
    "}\n",
    "\n",
    "rfResult_V1 = grid_search(rf_pipelineV1, rf_paramsV1)\n",
    "report_scores(rfResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO THIS OVERNIGHT!!\n",
    "\n",
    "# Now try the RandomForest algorithm,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "# do this overnight :/ hehe\n",
    "\n",
    "rf_pipelineV2 = Pipeline([\n",
    "    ('cv', CountVectorizer()), #We are looking at occurences rather than counts with the Bernoulli Naive Bayes algorithm\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "rf_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'rf__n_estimators' : [100 * x for x in range (1,6)],\n",
    "    'rf__max_depth' : [25, 50, 75, None],\n",
    "    'rf__max_features' : [\"auto\", \"sqrt\"],\n",
    "    'rf__min_samples_leaf' : [1,2,4],\n",
    "    'rf__min_samlpes_split' : [2, 5, 10],\n",
    "    'rf__bootstrap' : [True, False]\n",
    "}\n",
    "\n",
    "rfResult_V2 = grid_search(rf_pipelineV2, rf_paramsV2)\n",
    "report_scores(rfResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 1), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.78      0.55      1209\n",
      "           1       0.82      0.49      0.62      2546\n",
      "\n",
      "    accuracy                           0.58      3755\n",
      "   macro avg       0.62      0.64      0.58      3755\n",
      "weighted avg       0.69      0.58      0.59      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do the multinomial naive bayes algorithm with a tfidf vectorizer instead of a countvectorizer,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV1 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "mNBtResult_V1 = grid_search(mNBt_pipelineV1, mNBt_paramsV1)\n",
    "report_scores(mNBtResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 9000, 'cv__ngram_range': (1, 2), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.78      0.55      1209\n",
      "           1       0.82      0.49      0.62      2546\n",
      "\n",
      "    accuracy                           0.58      3755\n",
      "   macro avg       0.62      0.63      0.58      3755\n",
      "weighted avg       0.69      0.58      0.59      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV2 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,11)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,10)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "mNBtResult_V2 = grid_search(mNBt_pipelineV2, mNBt_paramsV2)\n",
    "report_scores(mNBtResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 70 candidates, totalling 350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 350 out of 350 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 14000, 'cv__ngram_range': (1, 2), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.77      0.56      1209\n",
      "           1       0.83      0.55      0.66      2546\n",
      "\n",
      "    accuracy                           0.62      3755\n",
      "   macro avg       0.64      0.66      0.61      3755\n",
      "weighted avg       0.71      0.62      0.63      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV3 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV3 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (8,15)],\n",
    "    'nb__alpha' : [i for i in range (1,3)]\n",
    "}\n",
    "\n",
    "mNBtResult_V3 = grid_search(mNBt_pipelineV3, mNBt_paramsV3)\n",
    "report_scores(mNBtResult_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 105 candidates, totalling 525 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   37.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 525 out of 525 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 18000, 'cv__ngram_range': (1, 2), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.76      0.64      1209\n",
      "           1       0.86      0.71      0.78      2546\n",
      "\n",
      "    accuracy                           0.72      3755\n",
      "   macro avg       0.71      0.74      0.71      3755\n",
      "weighted avg       0.76      0.72      0.73      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV4 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV4 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (13,20)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "mNBtResult_V4 = grid_search(mNBt_pipelineV4, mNBt_paramsV4)\n",
    "report_scores(mNBtResult_V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 70 candidates, totalling 350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 350 out of 350 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 22000, 'cv__ngram_range': (1, 4), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      1209\n",
      "           1       0.86      0.72      0.78      2546\n",
      "\n",
      "    accuracy                           0.73      3755\n",
      "   macro avg       0.71      0.73      0.71      3755\n",
      "weighted avg       0.76      0.73      0.74      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV5 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV5 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (17,24)],\n",
    "    'nb__alpha' : [i for i in range (1,3)]\n",
    "}\n",
    "\n",
    "mNBtResult_V5 = grid_search(mNBt_pipelineV5, mNBt_paramsV5)\n",
    "report_scores(mNBtResult_V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   55.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 12.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__max_features': 22000, 'cv__ngram_range': (1, 4), 'nb__alpha': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      1209\n",
      "           1       0.86      0.72      0.78      2546\n",
      "\n",
      "    accuracy                           0.73      3755\n",
      "   macro avg       0.71      0.73      0.71      3755\n",
      "weighted avg       0.76      0.73      0.74      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV6 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV6 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (3,11)],\n",
    "    'cv__max_features' : [1000 * i for i in range (21,27)],\n",
    "    'nb__alpha' : [i for i in range (1,3)]\n",
    "}\n",
    "\n",
    "mNBtResult_V6 = grid_search(mNBt_pipelineV6, mNBt_paramsV6)\n",
    "report_scores(mNBtResult_V6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 132 candidates, totalling 660 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   49.2s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 660 out of 660 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', MultinomialNB())\n",
      "The best parameters found were: {'cv__ngram_range': (1, 9), 'cv__norm': 'l2', 'nb__alpha': 0.25}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73      1209\n",
      "           1       0.87      0.87      0.87      2546\n",
      "\n",
      "    accuracy                           0.83      3755\n",
      "   macro avg       0.80      0.80      0.80      3755\n",
      "weighted avg       0.83      0.83      0.83      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "mNBt_pipelineV7 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    #('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "mNBt_paramsV7 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (9,20)],\n",
    "    'cv__norm' : [\"l1\", \"l2\"],\n",
    "    'nb__alpha' : [0.25 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "mNBtResult_V7 = grid_search(mNBt_pipelineV7, mNBt_paramsV7)\n",
    "report_scores(mNBtResult_V7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   50.4s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 5000, 'cv__ngram_range': (1, 5), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.85      0.53      1209\n",
      "           1       0.84      0.36      0.51      2546\n",
      "\n",
      "    accuracy                           0.52      3755\n",
      "   macro avg       0.61      0.61      0.52      3755\n",
      "weighted avg       0.69      0.52      0.51      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do the multinomial naive bayes algorithm with a tfidf vectorizer instead of a countvectorizer,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "cNBt_pipelineV1 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNBt_paramsV1 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (1,6)],\n",
    "    'cv__max_features' : [1000 * i for i in range (1,6)],\n",
    "    'nb__alpha' : [i for i in range (1,4)]\n",
    "}\n",
    "\n",
    "cNBtResult_V1 = grid_search(cNBt_pipelineV1, cNBt_paramsV1)\n",
    "report_scores(cNBtResult_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 147 candidates, totalling 735 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   28.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   55.0s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 735 out of 735 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 10000, 'cv__ngram_range': (1, 7), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.84      0.63      1209\n",
      "           1       0.89      0.61      0.72      2546\n",
      "\n",
      "    accuracy                           0.68      3755\n",
      "   macro avg       0.70      0.72      0.68      3755\n",
      "weighted avg       0.76      0.68      0.69      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do the multinomial naive bayes algorithm with a tfidf vectorizer instead of a countvectorizer,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "cNBt_pipelineV2 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNBt_paramsV2 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (4,11)],\n",
    "    'cv__max_features' : [1000 * i for i in range (4,11)],\n",
    "    'nb__alpha' : [i for i in range (2,5)]\n",
    "}\n",
    "\n",
    "cNBtResult_V2 = grid_search(cNBt_pipelineV2, cNBt_paramsV2)\n",
    "report_scores(cNBtResult_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   42.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__max_features': 13000, 'cv__ngram_range': (1, 11), 'nb__alpha': 3}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.83      0.74      1209\n",
      "           1       0.91      0.80      0.85      2546\n",
      "\n",
      "    accuracy                           0.81      3755\n",
      "   macro avg       0.78      0.81      0.79      3755\n",
      "weighted avg       0.83      0.81      0.81      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do the multinomial naive bayes algorithm with a tfidf vectorizer instead of a countvectorizer,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "cNBt_pipelineV3 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    ('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNBt_paramsV3 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (6,14)],\n",
    "    'cv__max_features' : [1000 * i for i in range (9,15)],\n",
    "    'nb__alpha' : [i for i in range (1,5)]\n",
    "}\n",
    "\n",
    "cNBtResult_V3 = grid_search(cNBt_pipelineV3, cNBt_paramsV3)\n",
    "report_scores(cNBtResult_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('nb', ComplementNB())\n",
      "The best parameters found were: {'cv__ngram_range': (1, 13), 'cv__norm': 'l2', 'nb__alpha': 1.0}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73      1209\n",
      "           1       0.89      0.81      0.85      2546\n",
      "\n",
      "    accuracy                           0.81      3755\n",
      "   macro avg       0.78      0.80      0.79      3755\n",
      "weighted avg       0.82      0.81      0.81      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRY WITHOUTTT SPECIFYING MAX FEATURES\n",
    "\n",
    "# Seems to be performing initially better already. Now try a wider parameter search:,\n",
    "# trying ngrams from (1,1) to (1,5), 1000 max features in the vocabulary to 5000 max features\n",
    "# in increments of 1000, and try values of 1, 2 and 3 for alpha\n",
    "\n",
    "# (its not actually version 7 im just too lazy to think of another name yet)\n",
    "\n",
    "cNBt_pipelineV7 = Pipeline([\n",
    "    ('cv', TfidfVectorizer()),\n",
    "    #('df', Densifier()), #we need to convert the sparse matrix into a dense matrix\n",
    "    ('nb', ComplementNB())\n",
    "])\n",
    "\n",
    "cNBt_paramsV7 = {\n",
    "    'cv__ngram_range' : [(1,x) for x in range (5,15)],\n",
    "    'cv__norm' : [\"l1\", \"l2\"],\n",
    "    'nb__alpha' : [0.25 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "cNBtResult_V7 = grid_search(cNBt_pipelineV7, cNBt_paramsV7)\n",
    "report_scores(cNBtResult_V7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_cv__max_features</th>\n",
       "      <th>param_cv__ngram_range</th>\n",
       "      <th>param_nb__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.811465</td>\n",
       "      <td>0.052013</td>\n",
       "      <td>0.182936</td>\n",
       "      <td>0.014244</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>2</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 2.0}</td>\n",
       "      <td>0.837532</td>\n",
       "      <td>0.868768</td>\n",
       "      <td>0.941638</td>\n",
       "      <td>0.918847</td>\n",
       "      <td>0.818393</td>\n",
       "      <td>0.877036</td>\n",
       "      <td>0.046882</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.774688</td>\n",
       "      <td>0.042030</td>\n",
       "      <td>0.182933</td>\n",
       "      <td>0.018495</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>2.5</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 2.5}</td>\n",
       "      <td>0.835291</td>\n",
       "      <td>0.866933</td>\n",
       "      <td>0.942343</td>\n",
       "      <td>0.920810</td>\n",
       "      <td>0.819748</td>\n",
       "      <td>0.877025</td>\n",
       "      <td>0.047556</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.774855</td>\n",
       "      <td>0.073815</td>\n",
       "      <td>0.189302</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>3</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3.0}</td>\n",
       "      <td>0.833563</td>\n",
       "      <td>0.865421</td>\n",
       "      <td>0.944935</td>\n",
       "      <td>0.921502</td>\n",
       "      <td>0.818950</td>\n",
       "      <td>0.876874</td>\n",
       "      <td>0.048961</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.701766</td>\n",
       "      <td>0.060904</td>\n",
       "      <td>0.175637</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>3.5</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3.5}</td>\n",
       "      <td>0.835125</td>\n",
       "      <td>0.864071</td>\n",
       "      <td>0.945408</td>\n",
       "      <td>0.921184</td>\n",
       "      <td>0.823049</td>\n",
       "      <td>0.877767</td>\n",
       "      <td>0.047875</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.744092</td>\n",
       "      <td>0.097567</td>\n",
       "      <td>0.173818</td>\n",
       "      <td>0.009744</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>4</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 4.0}</td>\n",
       "      <td>0.835085</td>\n",
       "      <td>0.863065</td>\n",
       "      <td>0.945492</td>\n",
       "      <td>0.922234</td>\n",
       "      <td>0.824486</td>\n",
       "      <td>0.878072</td>\n",
       "      <td>0.047834</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>1.808868</td>\n",
       "      <td>0.033338</td>\n",
       "      <td>0.190575</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>22500</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>5.5</td>\n",
       "      <td>{'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 5.5}</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.860859</td>\n",
       "      <td>0.947341</td>\n",
       "      <td>0.926425</td>\n",
       "      <td>0.826633</td>\n",
       "      <td>0.878669</td>\n",
       "      <td>0.049379</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>1.806728</td>\n",
       "      <td>0.043773</td>\n",
       "      <td>0.181072</td>\n",
       "      <td>0.018915</td>\n",
       "      <td>22500</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>6</td>\n",
       "      <td>{'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 6.0}</td>\n",
       "      <td>0.830762</td>\n",
       "      <td>0.860660</td>\n",
       "      <td>0.944700</td>\n",
       "      <td>0.925381</td>\n",
       "      <td>0.826774</td>\n",
       "      <td>0.877655</td>\n",
       "      <td>0.048682</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>1.806973</td>\n",
       "      <td>0.057512</td>\n",
       "      <td>0.185379</td>\n",
       "      <td>0.010972</td>\n",
       "      <td>22500</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>6.5</td>\n",
       "      <td>{'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 6.5}</td>\n",
       "      <td>0.831104</td>\n",
       "      <td>0.859661</td>\n",
       "      <td>0.944487</td>\n",
       "      <td>0.925735</td>\n",
       "      <td>0.826913</td>\n",
       "      <td>0.877580</td>\n",
       "      <td>0.048670</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>1.791002</td>\n",
       "      <td>0.041953</td>\n",
       "      <td>0.194709</td>\n",
       "      <td>0.015359</td>\n",
       "      <td>22500</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>7</td>\n",
       "      <td>{'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 7.0}</td>\n",
       "      <td>0.829975</td>\n",
       "      <td>0.858731</td>\n",
       "      <td>0.944926</td>\n",
       "      <td>0.921423</td>\n",
       "      <td>0.824938</td>\n",
       "      <td>0.875999</td>\n",
       "      <td>0.048658</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>1.612976</td>\n",
       "      <td>0.102666</td>\n",
       "      <td>0.114108</td>\n",
       "      <td>0.020504</td>\n",
       "      <td>22500</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>7.5</td>\n",
       "      <td>{'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 7.5}</td>\n",
       "      <td>0.829677</td>\n",
       "      <td>0.856813</td>\n",
       "      <td>0.944473</td>\n",
       "      <td>0.922290</td>\n",
       "      <td>0.822886</td>\n",
       "      <td>0.875228</td>\n",
       "      <td>0.049322</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows √ó 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         1.811465      0.052013         0.182936        0.014244   \n",
       "1         1.774688      0.042030         0.182933        0.018495   \n",
       "2         1.774855      0.073815         0.189302        0.012025   \n",
       "3         1.701766      0.060904         0.175637        0.011096   \n",
       "4         1.744092      0.097567         0.173818        0.009744   \n",
       "..             ...           ...              ...             ...   \n",
       "715       1.808868      0.033338         0.190575        0.015797   \n",
       "716       1.806728      0.043773         0.181072        0.018915   \n",
       "717       1.806973      0.057512         0.185379        0.010972   \n",
       "718       1.791002      0.041953         0.194709        0.015359   \n",
       "719       1.612976      0.102666         0.114108        0.020504   \n",
       "\n",
       "    param_cv__max_features param_cv__ngram_range param_nb__alpha  \\\n",
       "0                    17000                (1, 8)               2   \n",
       "1                    17000                (1, 8)             2.5   \n",
       "2                    17000                (1, 8)               3   \n",
       "3                    17000                (1, 8)             3.5   \n",
       "4                    17000                (1, 8)               4   \n",
       "..                     ...                   ...             ...   \n",
       "715                  22500               (1, 12)             5.5   \n",
       "716                  22500               (1, 12)               6   \n",
       "717                  22500               (1, 12)             6.5   \n",
       "718                  22500               (1, 12)               7   \n",
       "719                  22500               (1, 12)             7.5   \n",
       "\n",
       "                                                                        params  \\\n",
       "0     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 2.0}   \n",
       "1     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 2.5}   \n",
       "2     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3.0}   \n",
       "3     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3.5}   \n",
       "4     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 4.0}   \n",
       "..                                                                         ...   \n",
       "715  {'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 5.5}   \n",
       "716  {'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 6.0}   \n",
       "717  {'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 6.5}   \n",
       "718  {'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 7.0}   \n",
       "719  {'cv__max_features': 22500, 'cv__ngram_range': (1, 12), 'nb__alpha': 7.5}   \n",
       "\n",
       "     split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0             0.837532           0.868768           0.941638   \n",
       "1             0.835291           0.866933           0.942343   \n",
       "2             0.833563           0.865421           0.944935   \n",
       "3             0.835125           0.864071           0.945408   \n",
       "4             0.835085           0.863065           0.945492   \n",
       "..                 ...                ...                ...   \n",
       "715           0.832084           0.860859           0.947341   \n",
       "716           0.830762           0.860660           0.944700   \n",
       "717           0.831104           0.859661           0.944487   \n",
       "718           0.829975           0.858731           0.944926   \n",
       "719           0.829677           0.856813           0.944473   \n",
       "\n",
       "     split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0             0.918847           0.818393         0.877036        0.046882   \n",
       "1             0.920810           0.819748         0.877025        0.047556   \n",
       "2             0.921502           0.818950         0.876874        0.048961   \n",
       "3             0.921184           0.823049         0.877767        0.047875   \n",
       "4             0.922234           0.824486         0.878072        0.047834   \n",
       "..                 ...                ...              ...             ...   \n",
       "715           0.926425           0.826633         0.878669        0.049379   \n",
       "716           0.925381           0.826774         0.877655        0.048682   \n",
       "717           0.925735           0.826913         0.877580        0.048670   \n",
       "718           0.921423           0.824938         0.875999        0.048658   \n",
       "719           0.922290           0.822886         0.875228        0.049322   \n",
       "\n",
       "     rank_test_score  \n",
       "0                548  \n",
       "1                553  \n",
       "2                574  \n",
       "3                402  \n",
       "4                310  \n",
       "..               ...  \n",
       "715              136  \n",
       "716              432  \n",
       "717              452  \n",
       "718              625  \n",
       "719              658  \n",
       "\n",
       "[720 rows x 16 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(mNBResult_V7.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_cv__max_features</th>\n",
       "      <th>param_cv__ngram_range</th>\n",
       "      <th>param_nb__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.700852</td>\n",
       "      <td>0.064033</td>\n",
       "      <td>0.182134</td>\n",
       "      <td>0.016466</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>3</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3}</td>\n",
       "      <td>0.833563</td>\n",
       "      <td>0.865421</td>\n",
       "      <td>0.944935</td>\n",
       "      <td>0.921502</td>\n",
       "      <td>0.818950</td>\n",
       "      <td>0.876874</td>\n",
       "      <td>0.048961</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.725590</td>\n",
       "      <td>0.058813</td>\n",
       "      <td>0.177360</td>\n",
       "      <td>0.019758</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>4</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 4}</td>\n",
       "      <td>0.835085</td>\n",
       "      <td>0.863065</td>\n",
       "      <td>0.945492</td>\n",
       "      <td>0.922234</td>\n",
       "      <td>0.824486</td>\n",
       "      <td>0.878072</td>\n",
       "      <td>0.047834</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.656569</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.174861</td>\n",
       "      <td>0.012518</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>5</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 5}</td>\n",
       "      <td>0.833523</td>\n",
       "      <td>0.860460</td>\n",
       "      <td>0.945904</td>\n",
       "      <td>0.921879</td>\n",
       "      <td>0.825911</td>\n",
       "      <td>0.877535</td>\n",
       "      <td>0.048030</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.654335</td>\n",
       "      <td>0.038229</td>\n",
       "      <td>0.164041</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 8)</td>\n",
       "      <td>6</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 6}</td>\n",
       "      <td>0.832386</td>\n",
       "      <td>0.859861</td>\n",
       "      <td>0.946773</td>\n",
       "      <td>0.923475</td>\n",
       "      <td>0.824950</td>\n",
       "      <td>0.877489</td>\n",
       "      <td>0.049031</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.711115</td>\n",
       "      <td>0.046159</td>\n",
       "      <td>0.182961</td>\n",
       "      <td>0.019486</td>\n",
       "      <td>17000</td>\n",
       "      <td>(1, 9)</td>\n",
       "      <td>3</td>\n",
       "      <td>{'cv__max_features': 17000, 'cv__ngram_range': (1, 9), 'nb__alpha': 3}</td>\n",
       "      <td>0.836688</td>\n",
       "      <td>0.863806</td>\n",
       "      <td>0.945209</td>\n",
       "      <td>0.921584</td>\n",
       "      <td>0.821155</td>\n",
       "      <td>0.877688</td>\n",
       "      <td>0.048074</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.771514</td>\n",
       "      <td>0.044925</td>\n",
       "      <td>0.190980</td>\n",
       "      <td>0.024662</td>\n",
       "      <td>22000</td>\n",
       "      <td>(1, 11)</td>\n",
       "      <td>6</td>\n",
       "      <td>{'cv__max_features': 22000, 'cv__ngram_range': (1, 11), 'nb__alpha': 6}</td>\n",
       "      <td>0.831440</td>\n",
       "      <td>0.860060</td>\n",
       "      <td>0.946776</td>\n",
       "      <td>0.925381</td>\n",
       "      <td>0.826720</td>\n",
       "      <td>0.878076</td>\n",
       "      <td>0.049182</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.797188</td>\n",
       "      <td>0.039153</td>\n",
       "      <td>0.178165</td>\n",
       "      <td>0.015074</td>\n",
       "      <td>22000</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>3</td>\n",
       "      <td>{'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 3}</td>\n",
       "      <td>0.836305</td>\n",
       "      <td>0.865623</td>\n",
       "      <td>0.944833</td>\n",
       "      <td>0.922996</td>\n",
       "      <td>0.824252</td>\n",
       "      <td>0.878802</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.769360</td>\n",
       "      <td>0.048427</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>0.014231</td>\n",
       "      <td>22000</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>4</td>\n",
       "      <td>{'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 4}</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>0.863467</td>\n",
       "      <td>0.945304</td>\n",
       "      <td>0.924720</td>\n",
       "      <td>0.825944</td>\n",
       "      <td>0.878782</td>\n",
       "      <td>0.048011</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.807535</td>\n",
       "      <td>0.059350</td>\n",
       "      <td>0.191093</td>\n",
       "      <td>0.015101</td>\n",
       "      <td>22000</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>5</td>\n",
       "      <td>{'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 5}</td>\n",
       "      <td>0.833599</td>\n",
       "      <td>0.861460</td>\n",
       "      <td>0.947260</td>\n",
       "      <td>0.924392</td>\n",
       "      <td>0.826788</td>\n",
       "      <td>0.878700</td>\n",
       "      <td>0.048610</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.631261</td>\n",
       "      <td>0.100160</td>\n",
       "      <td>0.134660</td>\n",
       "      <td>0.026520</td>\n",
       "      <td>22000</td>\n",
       "      <td>(1, 12)</td>\n",
       "      <td>6</td>\n",
       "      <td>{'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 6}</td>\n",
       "      <td>0.830762</td>\n",
       "      <td>0.859861</td>\n",
       "      <td>0.945184</td>\n",
       "      <td>0.924465</td>\n",
       "      <td>0.826359</td>\n",
       "      <td>0.877326</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows √ó 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         1.700852      0.064033         0.182134        0.016466   \n",
       "1         1.725590      0.058813         0.177360        0.019758   \n",
       "2         1.656569      0.041990         0.174861        0.012518   \n",
       "3         1.654335      0.038229         0.164041        0.012219   \n",
       "4         1.711115      0.046159         0.182961        0.019486   \n",
       "..             ...           ...              ...             ...   \n",
       "115       1.771514      0.044925         0.190980        0.024662   \n",
       "116       1.797188      0.039153         0.178165        0.015074   \n",
       "117       1.769360      0.048427         0.191083        0.014231   \n",
       "118       1.807535      0.059350         0.191093        0.015101   \n",
       "119       1.631261      0.100160         0.134660        0.026520   \n",
       "\n",
       "    param_cv__max_features param_cv__ngram_range param_nb__alpha  \\\n",
       "0                    17000                (1, 8)               3   \n",
       "1                    17000                (1, 8)               4   \n",
       "2                    17000                (1, 8)               5   \n",
       "3                    17000                (1, 8)               6   \n",
       "4                    17000                (1, 9)               3   \n",
       "..                     ...                   ...             ...   \n",
       "115                  22000               (1, 11)               6   \n",
       "116                  22000               (1, 12)               3   \n",
       "117                  22000               (1, 12)               4   \n",
       "118                  22000               (1, 12)               5   \n",
       "119                  22000               (1, 12)               6   \n",
       "\n",
       "                                                                      params  \\\n",
       "0     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 3}   \n",
       "1     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 4}   \n",
       "2     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 5}   \n",
       "3     {'cv__max_features': 17000, 'cv__ngram_range': (1, 8), 'nb__alpha': 6}   \n",
       "4     {'cv__max_features': 17000, 'cv__ngram_range': (1, 9), 'nb__alpha': 3}   \n",
       "..                                                                       ...   \n",
       "115  {'cv__max_features': 22000, 'cv__ngram_range': (1, 11), 'nb__alpha': 6}   \n",
       "116  {'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 3}   \n",
       "117  {'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 4}   \n",
       "118  {'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 5}   \n",
       "119  {'cv__max_features': 22000, 'cv__ngram_range': (1, 12), 'nb__alpha': 6}   \n",
       "\n",
       "     split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0             0.833563           0.865421           0.944935   \n",
       "1             0.835085           0.863065           0.945492   \n",
       "2             0.833523           0.860460           0.945904   \n",
       "3             0.832386           0.859861           0.946773   \n",
       "4             0.836688           0.863806           0.945209   \n",
       "..                 ...                ...                ...   \n",
       "115           0.831440           0.860060           0.946776   \n",
       "116           0.836305           0.865623           0.944833   \n",
       "117           0.834473           0.863467           0.945304   \n",
       "118           0.833599           0.861460           0.947260   \n",
       "119           0.830762           0.859861           0.945184   \n",
       "\n",
       "     split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0             0.921502           0.818950         0.876874        0.048961   \n",
       "1             0.922234           0.824486         0.878072        0.047834   \n",
       "2             0.921879           0.825911         0.877535        0.048030   \n",
       "3             0.923475           0.824950         0.877489        0.049031   \n",
       "4             0.921584           0.821155         0.877688        0.048074   \n",
       "..                 ...                ...              ...             ...   \n",
       "115           0.925381           0.826720         0.878076        0.049182   \n",
       "116           0.922996           0.824252         0.878802        0.047473   \n",
       "117           0.924720           0.825944         0.878782        0.048011   \n",
       "118           0.924392           0.826788         0.878700        0.048610   \n",
       "119           0.924465           0.826359         0.877326        0.048780   \n",
       "\n",
       "     rank_test_score  \n",
       "0                116  \n",
       "1                 68  \n",
       "2                 99  \n",
       "3                101  \n",
       "4                 91  \n",
       "..               ...  \n",
       "115               66  \n",
       "116               18  \n",
       "117               20  \n",
       "118               26  \n",
       "119              104  \n",
       "\n",
       "[120 rows x 16 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how to display the results of the grid search as a large table\n",
    "pd.DataFrame(mNBResult_V5.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline which always starts with a countvectorizer\n",
    "mNaiveBayesPipeline = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    #('dt', Densifier()), #used for converting from sparse matrix to dense matrix, not needed here\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#the second lot of parameters to try\n",
    "mNaiveBayesParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'cv__max_features' : [500 * i for i in range(2 * 12,2 * 17)],\n",
    "    'nb__alpha' : [0.5 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "#ignore this one for now\n",
    "cNBpipeline = Pipeline([('cv', CountVectorizer()), ('cnb', ComplementNB())])\n",
    "cNBparams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'cv__max_features' : [1000* i for i in range(11,18)],\n",
    "    'cnb__alpha' : [1 * i for i in range (0,9)]\n",
    "}\n",
    "\n",
    "#add: ('pca', PCA()),\n",
    "lsvcPipeline = Pipeline([('cv', CountVectorizer()), ('df', Densifier()), ('pca', PCA()), ('lsvc', LinearSVC())])\n",
    "lsvcParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(10,15)],\n",
    "    'cv__max_features' : [1000* i for i in range(14,18)],\n",
    "    'lsvc__C' : [1 * i for i in range(0,3)]\n",
    "}\n",
    "\n",
    "rForestPipeline = Pipeline([('cv', CountVectorizer()), ('rf', RandomForestClassifier())])\n",
    "rForestParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(13,19)],\n",
    "    'cv__max_features' : [1000* i for i in range(10,17)],\n",
    "    'rf__n_estimators' : [10 * i for i in range(10,15)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def reportScores(clf):\n",
    "    \n",
    "    print(\"---- RESULTS ----\",\"\\n\")\n",
    "    print(\"The algorithm being optimised was:\",clf.estimator.steps[-1])\n",
    "    print(\"The best parameters found were:\", clf.best_params_)\n",
    "    y_test_true = simplified_testing.label\n",
    "    y_test_predictions = clf.predict(simplified_testing.tokens)\n",
    "    print(\"Score report:\\n\")\n",
    "    #prevent the chance of any of the lists being treated as objects\n",
    "    print(classification_report(y_test_true.astype('int'), y_test_predictions.astype('int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST SEARCH: ngram 3-16, max features 5-16, alpha 0-9, RESULT: ngram: (1,15), features: 15000, alpha: 0\n",
    "#so now try making the features and ngram even LARGER! got an accuracy of 81\n",
    "\n",
    "#SECOND SEARCH: similar not much change, ngram 14-18, features 13-19, alpha 0-2\n",
    "#RESULTS: features 14000, ngram 15, alpha 0\n",
    "mNBResult = gridSearch(mNaiveBayesPipeline, mNaiveBayesParams)\n",
    "reportScores(mNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters found from alpha 0-9, max features 5k-16k and ngram (3,16): \n",
    "# alpha 2, max features 15000, ngram (1,15) with accuracy of 66. Not very good\n",
    "cNBResult = gridSearch(cNBpipeline, cNBparams) \n",
    "reportScores(cNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 66 was okay... tried ngram 13-17, max features 13-17 and estimators 80-110 and best was 13000 features, ngram 1,15 and n estimators 110\n",
    "#so try again with everything a bit higher\n",
    "rForestResult = gridSearch(rForestPipeline, rForestParams)\n",
    "reportScores(rForestResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters searched: ngram 10-12, max features 10-12, C 0-2, found: ngram 11, max features 12000\n",
    "#introducing PCA made the computer die so don't use it again\n",
    "lsvcResult = gridSearch(lsvcPipeline, lsvcParams)\n",
    "reportScores(lsvcResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA ONES TO DO AFTERWARDS, KEEP ADDING THEM BELOW THEN COMMENT THE RESULTS UP ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 210 candidates, totalling 1050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 30.7min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 40.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed: 42.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('rf', RandomForestClassifier())\n",
      "The best parameters found were: {'cv__max_features': 12000, 'cv__ngram_range': (1, 15), 'rf__n_estimators': 100}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.05      0.08      1209\n",
      "           1       0.68      0.94      0.79      2546\n",
      "\n",
      "    accuracy                           0.65      3755\n",
      "   macro avg       0.48      0.49      0.43      3755\n",
      "weighted avg       0.55      0.65      0.56      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rForestResult = gridSearch(rForestPipeline, rForestParams)\n",
    "reportScores(rForestResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('lsvc', LinearSVC())\n",
      "The best parameters found were: {'cv__max_features': 15000, 'cv__ngram_range': (1, 13), 'lsvc__C': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.80      0.71      1209\n",
      "           1       0.89      0.78      0.84      2546\n",
      "\n",
      "    accuracy                           0.79      3755\n",
      "   macro avg       0.77      0.79      0.77      3755\n",
      "weighted avg       0.81      0.79      0.79      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lsvcResult = gridSearch(lsvcPipeline, lsvcParams)\n",
    "reportScores(lsvcResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mNBResult = gridSearch(mNaiveBayesPipeline, mNaiveBayesParams)\n",
    "reportScores(mNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW TRY DOING THE SAME BUT USING TFIDF INSTEAD\n",
    "\n",
    "mnbtPipeline = Pipeline([\n",
    "    ('tf', TfidfVectorizer()), \n",
    "    #('dt', Densifier()), #used for converting from sparse matrix to dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#the second lot of parameters to try\n",
    "mnbtParams = {\n",
    "    'tf__ngram_range': [(1,0.5 * x) for x in range(2 * 12,2 *17)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'nb__alpha' : [0.25 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "#ignore this one for now\n",
    "cnbtPipeline = Pipeline([('tf', TfidfVectorizer()), ('cnb', ComplementNB())])\n",
    "cnbtParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'cnb__alpha' : [1 * i for i in range (0,9)]\n",
    "}\n",
    "\n",
    "#add: ('pca', PCA()),\n",
    "lsvctPipeline = Pipeline([('tf', TfidfVectorizer()), ('df', Densifier()), ('lsvc', LinearSVC())])\n",
    "lsvctParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(10,15)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'lsvc__C' : [1 * i for i in range(0,3)]\n",
    "}\n",
    "\n",
    "rftPipeline = Pipeline([('tf', TfidfVectorizer()), ('rf', RandomForestClassifier())])\n",
    "rftParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(13,19)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'rf__n_estimators' : [10 * i for i in range(10,15)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST SEARCH: ngram 14-18, alpha 0-6, RESULT: alpha = 0.5, ngram = 14, norm = l2, accuracy = 87\n",
    "\n",
    "#SECOND SEARCH: \n",
    "mnbtResult = gridSearch(mnbtPipeline, mnbtParams)\n",
    "reportScores(mnbtResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnbtResult = gridSearch(cnbtPipeline, cnbtParams)\n",
    "reportScores(cnbtResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvctResult = gridSearch(lsvctPipeline, lsvctParams)\n",
    "reportScores(lsvctResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 39.7min\n"
     ]
    }
   ],
   "source": [
    "#FIRST SEARCH: \n",
    "rftResult = gridSearch(rftPipeline, rftParams)\n",
    "reportScores(rftResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
