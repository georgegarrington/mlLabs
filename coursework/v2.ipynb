{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "#import emoji\n",
    "\n",
    "#make the columns as wide as possible so we can see all the text\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>So touching! RT @DreamCameTrue_: RT @Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/yBaVo3FZ /via @CarrieFairygirl</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>Thank goodness for people who are kind #sandy http://t.co/Pc25SgSy /via @CarrieFairygirl</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>RT “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wfMtqwjl /via @CarrieFairygirl”\\n\\nComplete fire hazard</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>RT “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wfMtqwjl /via @CarrieFairygirl”\\n\\nComplete fire hazard</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>Thank goodness for people who are kind  #sandy http://t.co/ghz8E5je /via @CarrieFairygirl</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>@Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/WE72RVCE /via @CarrieFairygirl LOVE THIS!</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>RT @alyssa_milano: Thank goodness for people who are kind  #sandy http://t.co/45y8vlMQ /via @CarrieFairygirl</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>“@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wuZ5PyNk /via @CarrieFairygirl” Awesome.</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>RT @Alyssa_Milano Thank goodness for people who are kind  #sandy http://t.co/3g21wFvG /via @CarrieFairygirl</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>É, a humanidade ainda tem jeito. “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/xwu5jSIg /via @CarrieFairygirl”</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>Thank goodness for people who are kind  #sandy http://t.co/Do9M8Jag /via @CarrieFairygirl</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>Thank goodness for people who are kind @Alyssa_Milano  #sandy http://t.co/4KVRkeqi</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>THIS is amazing! RT @Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/xIzaj5UN /via @CarrieFairygirl</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>Plug in Pals RT @Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/gXB0pHGX /via @CarrieFairygirl</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>: RT@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/IrwVfQXc”\\nAngoisse et lumières!”</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10015</th>\n",
       "      <td>Splendide ! “@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/U4whKtFD” cc @NYMag</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>RT @confidentiels La une du New-York magazine sur Sandy : \"The City and the Storm\" http://t.co/IiC0brts</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10017</th>\n",
       "      <td>: RT@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/IrwVfQXc”\\nAngoisse et lumières!”</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>Magnifique !! “@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/Ylo9TvOy”</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>NY: The City and the Storm. The Look of Post-Sandy New York.... http://t.co/AL5UZXkm</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          tweetText  \\\n",
       "10000  So touching! RT @DreamCameTrue_: RT @Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/yBaVo3FZ /via @CarrieFairygirl   \n",
       "10001                                                      Thank goodness for people who are kind #sandy http://t.co/Pc25SgSy /via @CarrieFairygirl   \n",
       "10002        RT “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wfMtqwjl /via @CarrieFairygirl”\\n\\nComplete fire hazard   \n",
       "10003        RT “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wfMtqwjl /via @CarrieFairygirl”\\n\\nComplete fire hazard   \n",
       "10004                                                     Thank goodness for people who are kind  #sandy http://t.co/ghz8E5je /via @CarrieFairygirl   \n",
       "10005                           @Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/WE72RVCE /via @CarrieFairygirl LOVE THIS!   \n",
       "10006                                  RT @alyssa_milano: Thank goodness for people who are kind  #sandy http://t.co/45y8vlMQ /via @CarrieFairygirl   \n",
       "10007                          “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wuZ5PyNk /via @CarrieFairygirl” Awesome.   \n",
       "10008                                   RT @Alyssa_Milano Thank goodness for people who are kind  #sandy http://t.co/3g21wFvG /via @CarrieFairygirl   \n",
       "10009  É, a humanidade ainda tem jeito. “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/xwu5jSIg /via @CarrieFairygirl”   \n",
       "10010                                                     Thank goodness for people who are kind  #sandy http://t.co/Do9M8Jag /via @CarrieFairygirl   \n",
       "10011                                                            Thank goodness for people who are kind @Alyssa_Milano  #sandy http://t.co/4KVRkeqi   \n",
       "10012                  THIS is amazing! RT @Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/xIzaj5UN /via @CarrieFairygirl   \n",
       "10013                     Plug in Pals RT @Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/gXB0pHGX /via @CarrieFairygirl   \n",
       "10014         : RT@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/IrwVfQXc”\\nAngoisse et lumières!”   \n",
       "10015              Splendide ! “@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/U4whKtFD” cc @NYMag   \n",
       "10016                                       RT @confidentiels La une du New-York magazine sur Sandy : \"The City and the Storm\" http://t.co/IiC0brts   \n",
       "10017         : RT@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/IrwVfQXc”\\nAngoisse et lumières!”   \n",
       "10018                      Magnifique !! “@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/Ylo9TvOy”   \n",
       "10019                                                          NY: The City and the Storm. The Look of Post-Sandy New York.... http://t.co/AL5UZXkm   \n",
       "\n",
       "      label  lang  \n",
       "10000  real   NaN  \n",
       "10001  real   NaN  \n",
       "10002  real   NaN  \n",
       "10003  real   NaN  \n",
       "10004  real   NaN  \n",
       "10005  real   NaN  \n",
       "10006  real   NaN  \n",
       "10007  real   NaN  \n",
       "10008  real   NaN  \n",
       "10009  real   NaN  \n",
       "10010  real   NaN  \n",
       "10011  real   NaN  \n",
       "10012  real   NaN  \n",
       "10013  real   NaN  \n",
       "10014  real   NaN  \n",
       "10015  real   NaN  \n",
       "10016  real   NaN  \n",
       "10017  real   NaN  \n",
       "10018  real   NaN  \n",
       "10019  real   NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_training = pd.read_csv(\"mediaeval-2015-trainingset.txt\", delimiter = \"\\t\")\n",
    "\n",
    "#drop all columns apart from the text and the label as none of the other data appears to be useful\n",
    "original_training = original_training.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "\n",
    "#add a column to store the language\n",
    "original_training[\"lang\"] = np.nan\n",
    "\n",
    "#view a specific range, uncomment to view only those tweets from certain classes\n",
    "original_training[10000:10020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat the same process for the testing dataset\n",
    "original_testing = pd.read_csv(\"mediaeval-2015-testset.txt\", delimiter = \"\\t\")\n",
    "original_testing = original_testing.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "original_testing[\"lang\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake     6742\n",
       "real     4921\n",
       "humor    2614\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can see that the dataset is skewed towards fake and humor tweets\n",
    "original_training.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14277 entries, 0 to 14276\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   tweetText  14277 non-null  object \n",
      " 1   label      14277 non-null  object \n",
      " 2   lang       0 non-null      float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 334.7+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3755 entries, 0 to 3754\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   tweetText  3755 non-null   object \n",
      " 1   label      3755 non-null   object \n",
      " 2   lang       0 non-null      float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 88.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#there are no non null values to being with we can see\n",
    "original_training.info()\n",
    "print(\"\\n\")\n",
    "original_testing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\George\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\George\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import langdetect as l\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import nltk.stem as st\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the languages supported by the stemming algorithm\n",
    "st.SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#responsible for parsing tweets\n",
    "class TweetHandler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        snowball_langs = list(st.SnowballStemmer.languages)\n",
    "        self.tokenizer_langs = {\"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"it\", \"pt\", \"ru\", \"es\", \"sv\"}\n",
    "        langdetect_langs = [\"ar\", \"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"hu\", \"it\", \"no\", \"illegal\", \"pt\", \"ro\", \"ru\", \"es\", \"sv\"]\n",
    "        #a dictionary to map the corresponding snowball and langdetect properties\n",
    "        self.lang_dict = dict(zip(langdetect_langs, snowball_langs))\n",
    "        #declare some custom stop words\n",
    "        self.custom_stops = [\"http\",\"nhttp\",\"https\"]\n",
    "\n",
    "    #takes a tweet, detects its language, removes any stop words in the language,\n",
    "    #stems tokens in language and returns the simplified tokens paired with the language\n",
    "    def parse_tweet(self, tweet):\n",
    "        \n",
    "        try:\n",
    "            lang_prediction = l.detect(tweet)\n",
    "            #the nltk name for the predicted language\n",
    "            nltkprop = self.lang_dict[lang_prediction]\n",
    "        except:\n",
    "            #assume english stopwords and stemming if the language cannot be detected\n",
    "            lang_prediction = \"unknown\"\n",
    "            nltkprop = \"english\"\n",
    "            \n",
    "        #if the language is not supported by the tokenizer (including unkown) then assume tokenizing in English, however stemming\n",
    "        #and stopwords may still be supported in the language e.g. arabic, hungarian, romanian so tokenize with the english\n",
    "        #version of the algorithm if this is the case and use the stemming and stopwords specific to the language if this\n",
    "        #is available even if the tokenization algorithm isnt\n",
    "        tokens = nltk.word_tokenize(tweet, language = nltkprop if lang_prediction in self.tokenizer_langs else \"english\")\n",
    "        \n",
    "        #stop words specific to the language\n",
    "        stop_words = set(stopwords.words(nltkprop))\n",
    "        \n",
    "        #stemming algorithm specific to the language detected\n",
    "        stemmer = st.SnowballStemmer(nltkprop)\n",
    "        \n",
    "        #store all tokens to be output here, filter out any unwanted tokens and don't add them\n",
    "        #store the tokens in a space seperated string so that it can be fed into a count vectorizer\n",
    "        filtered_tokens = \"\"\n",
    "        \n",
    "        for tok in tokens:\n",
    "            \n",
    "            #remove any hashtags\n",
    "            if tok[0] == '#':\n",
    "                tok = tok[1:]\n",
    "                \n",
    "            #discard non alphanumeric strings containing symbols or pure digits, or stop words\n",
    "            if (not tok.isalnum()) or tok.isdigit() or (tok in stop_words) or tok in self.custom_stops:\n",
    "                continue;\n",
    "            \n",
    "            filtered_tokens += \" \" + stemmer.stem(tok)\n",
    "            #carry out stemming specific to the language detected\n",
    "            #filtered_tokens.add(stemmer.stem(tok))            \n",
    "        \n",
    "        #comment these out when you do not need to check if it works anymore\n",
    "        #print(\"original tokens:\", tokens,\"\\n\")\n",
    "        #print(\"filtered tokens:\", filtered_tokens,\"\\n\")\n",
    "        \n",
    "        #don't think having a column of the languages is really necessary but let's see\n",
    "        return filtered_tokens, lang_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the name of the property to tokens which is more appropriate\n",
    "#training_set = training_set.rename(columns = {\"tweetText\" : \"tokens\"})\n",
    "\n",
    "#make a new dataset from the original dataset transforming the tweet text into stemmed, simplified and filtered \n",
    "#tokens as handled by the TweetHandler class\n",
    "def transform_data(arg):\n",
    "\n",
    "    #copy the instance given so we don't change the original instance and can keep it in memory and reuse it \n",
    "    #if necessary\n",
    "    dataset = copy.deepcopy(arg)\n",
    "    th = TweetHandler()\n",
    "    num_rows = dataset.label.size\n",
    "    \n",
    "    #the tweet text will be transformed into tokens so rename the column appropriately\n",
    "    dataset = dataset.rename(columns = {\"tweetText\" : \"tokens\"})\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "\n",
    "        tweet = dataset[\"tokens\"][i]\n",
    "        label = dataset[\"label\"][i]\n",
    "\n",
    "        #disregard the humour information for now, simplify all the classes to two different classes\n",
    "        if (\"humor\" in label) or (\"fake\" in label):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "            \n",
    "        #for testing\n",
    "        #print(\"The old value of the row is:\",dataset.loc[i],\"\\n\")\n",
    "        \n",
    "        tokens, lang = th.parse_tweet(tweet)\n",
    "        \n",
    "        #change the type of tweetText from a string to a set of strings of the stemmed tokenized words\n",
    "        dataset.loc[i] = tokens, label, lang\n",
    "        \n",
    "        #for testing\n",
    "        #print(\"The new value of the row is:\",dataset.loc[i],\"\\n\\n\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data\n",
    "simplified_training = transform_data(original_training)\n",
    "simplified_testing = transform_data(original_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acuerd pelicul el dia despues mañan me recuerd pas huracan sandy</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>milenagimon mir sandy ny tremend imag huracan parec dia independent real rt</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buen fot huracan sandy recuerd pelicul dia independent id4 sandy</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scari shit hurrican ny</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my fave place world nyc hurrican sandi statueofliberti</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14272</th>\n",
       "      <td>bobombdom slap tweetdeck pigfish</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14273</th>\n",
       "      <td>new speci fish found brazil realli good photoshop what you think</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14274</th>\n",
       "      <td>what call pigfish</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14275</th>\n",
       "      <td>pigfish e dop pescecan pesc maial</td>\n",
       "      <td>1</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14276</th>\n",
       "      <td>for ca decid fish meat pigfish</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14277 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             tokens  \\\n",
       "0                  acuerd pelicul el dia despues mañan me recuerd pas huracan sandy   \n",
       "1       milenagimon mir sandy ny tremend imag huracan parec dia independent real rt   \n",
       "2                  buen fot huracan sandy recuerd pelicul dia independent id4 sandy   \n",
       "3                                                            scari shit hurrican ny   \n",
       "4                            my fave place world nyc hurrican sandi statueofliberti   \n",
       "...                                                                             ...   \n",
       "14272                                              bobombdom slap tweetdeck pigfish   \n",
       "14273              new speci fish found brazil realli good photoshop what you think   \n",
       "14274                                                             what call pigfish   \n",
       "14275                                             pigfish e dop pescecan pesc maial   \n",
       "14276                                                for ca decid fish meat pigfish   \n",
       "\n",
       "      label lang  \n",
       "0         1   es  \n",
       "1         1   es  \n",
       "2         1   es  \n",
       "3         1   en  \n",
       "4         1   en  \n",
       "...     ...  ...  \n",
       "14272     1   en  \n",
       "14273     1   en  \n",
       "14274     1   en  \n",
       "14275     1   it  \n",
       "14276     1   en  \n",
       "\n",
       "[14277 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#have a quick look to see if the tokens have been simplified how we want them to, which they have\n",
    "simplified_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en         76.598725\n",
       "es          9.084542\n",
       "unknown     7.508580\n",
       "fr          1.505919\n",
       "pt          1.155705\n",
       "de          0.847517\n",
       "it          0.707432\n",
       "nl          0.644393\n",
       "ar          0.560342\n",
       "ru          0.420256\n",
       "sv          0.294179\n",
       "no          0.273167\n",
       "da          0.217132\n",
       "fi          0.091056\n",
       "ro          0.049030\n",
       "hu          0.042026\n",
       "Name: lang, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get an idea of how many of each language there are, we can see that it is predominantly english\n",
    "simplified_training.lang.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9356\n",
       "0    4921\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_training.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kereeen rt shyman33 eclips iss</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absolut beauti rt shyman33 eclips iss</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shyman33 eclips iss 우주에서본 일식 wow amaz</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eclips iss</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dit dus rt the solar eclips seen intern space station solareclips iss space</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3693</th>\n",
       "      <td>syria syrian hero boy rescu girl shootout</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3695</th>\n",
       "      <td>böhmermann zdf neo varoufaki fake finger varoufak neo magazin royal mit jan b via youtub</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>top stori jan böhmermann ist laut zdf fake see</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>zdf neo lay jauch varoufaki stinkefing worth watch</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>sorri yanisvaroufaki english subtitl full english greek subtitl avail soon varoufak</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2778 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         tokens  \\\n",
       "0                                                                kereeen rt shyman33 eclips iss   \n",
       "1                                                         absolut beauti rt shyman33 eclips iss   \n",
       "2                                                         shyman33 eclips iss 우주에서본 일식 wow amaz   \n",
       "3                                                                                    eclips iss   \n",
       "8                   dit dus rt the solar eclips seen intern space station solareclips iss space   \n",
       "...                                                                                         ...   \n",
       "3693                                                  syria syrian hero boy rescu girl shootout   \n",
       "3695   böhmermann zdf neo varoufaki fake finger varoufak neo magazin royal mit jan b via youtub   \n",
       "3743                                             top stori jan böhmermann ist laut zdf fake see   \n",
       "3748                                         zdf neo lay jauch varoufaki stinkefing worth watch   \n",
       "3754        sorri yanisvaroufaki english subtitl full english greek subtitl avail soon varoufak   \n",
       "\n",
       "     label lang  \n",
       "0        1   en  \n",
       "1        1   en  \n",
       "2        1   en  \n",
       "3        1   en  \n",
       "8        1   en  \n",
       "...    ...  ...  \n",
       "3693     1   en  \n",
       "3695     1   en  \n",
       "3743     1   en  \n",
       "3748     1   en  \n",
       "3754     1   en  \n",
       "\n",
       "[2778 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_testing[simplified_testing.lang == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en         73.981358\n",
       "unknown    14.993342\n",
       "ar          5.166445\n",
       "es          1.677763\n",
       "de          1.091877\n",
       "pt          0.958722\n",
       "fr          0.852197\n",
       "nl          0.612517\n",
       "it          0.452730\n",
       "fi          0.106525\n",
       "sv          0.053262\n",
       "ru          0.026631\n",
       "ro          0.026631\n",
       "Name: lang, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the percentages of each language\n",
    "simplified_testing.lang.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14277 entries, 0 to 14276\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tokens  14277 non-null  object\n",
      " 1   label   14277 non-null  object\n",
      " 2   lang    14277 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 334.7+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3755 entries, 0 to 3754\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tokens  3755 non-null   object\n",
      " 1   label   3755 non-null   object\n",
      " 2   lang    3755 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 88.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#check if there are any null values after simplifying, there aren't any\n",
    "simplified_training.info()\n",
    "print(\"\\n\")\n",
    "simplified_testing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to convert the sparse matrix produced by the count vectorizer into a dense matrix in order for it \n",
    "#to be able to be used with different algorithms in a pipeline that require a dense matrix and not a sparse matrix\n",
    "class Densifier():\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **kwarge):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNaiveBayes():\n",
    "    \n",
    "    #pipeline which always starts with a countvectorizer\n",
    "    pipeline = Pipeline([\n",
    "        ('cv', CountVectorizer()), \n",
    "        #('dt', Densifier()), \n",
    "        ('nb', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    #carry out a grid search for optimal parameter selection. Due to constraints of using my laptop this\n",
    "    #can't be done that extensively, I will leave it to run overnight a few times\n",
    "    clf = GridSearchCV(pipeline, {\n",
    "        'cv__ngram_range': [(1,x) for x in range(9,10)],\n",
    "        'cv__max_features' : [1000* i for i in range(8,12)]\n",
    "        #,'nb__alpha' : [0.5 * i for i in range (5,7)]\n",
    "    }, scoring='f1', verbose = 4) \n",
    "    #make sure we are focusing on maximizing the f1 score and not a different metric, add some verbosity so we can\n",
    "    #see the progress of the grid search to get an idea of how much time it is taking\n",
    "    \n",
    "    #make sure the labels are treated as ints and not objects\n",
    "    clf.fit(simplified_training.tokens,simplified_training.label.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(pipeline, params):\n",
    "    \n",
    "    #carry out a grid search for optimal parameter selection. Due to constraints of using my laptop this\n",
    "    #can't be done that extensively, I will leave it to run overnight a few times\n",
    "    \n",
    "    #make n_jobs -1 so that all cores that are available are used which should hopefully make it quicker\n",
    "    \n",
    "    clf = GridSearchCV(pipeline, params, scoring = \"f1\", verbose = 6, n_jobs = -1)\n",
    "    \n",
    "    #make sure we are focusing on maximizing the f1 score and not a different metric, add some verbosity so we can\n",
    "    #see the progress of the grid search to get an idea of how much time it is taking\n",
    "    \n",
    "    #make sure the labels are treated as ints and not objects\n",
    "    clf.fit(simplified_training.tokens, simplified_training.label.astype(\"int\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline which always starts with a countvectorizer\n",
    "mNaiveBayesPipeline = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    #('dt', Densifier()), #used for converting from sparse matrix to dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#the second lot of parameters to try\n",
    "mNaiveBayesParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'cv__max_features' : [500 * i for i in range(2 * 12,2 * 17)],\n",
    "    'nb__alpha' : [0.5 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "#ignore this one for now\n",
    "cNBpipeline = Pipeline([('cv', CountVectorizer()), ('cnb', ComplementNB())])\n",
    "cNBparams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'cv__max_features' : [1000* i for i in range(11,18)],\n",
    "    'cnb__alpha' : [1 * i for i in range (0,9)]\n",
    "}\n",
    "\n",
    "#add: ('pca', PCA()),\n",
    "lsvcPipeline = Pipeline([('cv', CountVectorizer()), ('df', Densifier()), ('pca', PCA()), ('lsvc', LinearSVC())])\n",
    "lsvcParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(10,15)],\n",
    "    'cv__max_features' : [1000* i for i in range(14,18)],\n",
    "    'lsvc__C' : [1 * i for i in range(0,3)]\n",
    "}\n",
    "\n",
    "rForestPipeline = Pipeline([('cv', CountVectorizer()), ('rf', RandomForestClassifier())])\n",
    "rForestParams = {\n",
    "    'cv__ngram_range': [(1,x) for x in range(13,19)],\n",
    "    'cv__max_features' : [1000* i for i in range(10,17)],\n",
    "    'rf__n_estimators' : [10 * i for i in range(10,15)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportScores(clf):\n",
    "    \n",
    "    print(\"---- RESULTS ----\",\"\\n\")\n",
    "    print(\"The algorithm being optimised was:\",clf.estimator.steps[-1])\n",
    "    print(\"The best parameters found were:\", clf.best_params_)\n",
    "    y_test_true = simplified_testing.label\n",
    "    y_test_predictions = clf.predict(simplified_testing.tokens)\n",
    "    print(\"Score report:\\n\")\n",
    "    #prevent the chance of any of the lists being treated as objects\n",
    "    print(classification_report(y_test_true.astype('int'), y_test_predictions.astype('int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST SEARCH: ngram 3-16, max features 5-16, alpha 0-9, RESULT: ngram: (1,15), features: 15000, alpha: 0\n",
    "#so now try making the features and ngram even LARGER! got an accuracy of 81\n",
    "\n",
    "#SECOND SEARCH: similar not much change, ngram 14-18, features 13-19, alpha 0-2\n",
    "#RESULTS: features 14000, ngram 15, alpha 0\n",
    "mNBResult = gridSearch(mNaiveBayesPipeline, mNaiveBayesParams)\n",
    "reportScores(mNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters found from alpha 0-9, max features 5k-16k and ngram (3,16): \n",
    "# alpha 2, max features 15000, ngram (1,15) with accuracy of 66. Not very good\n",
    "cNBResult = gridSearch(cNBpipeline, cNBparams) \n",
    "reportScores(cNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 66 was okay... tried ngram 13-17, max features 13-17 and estimators 80-110 and best was 13000 features, ngram 1,15 and n estimators 110\n",
    "#so try again with everything a bit higher\n",
    "rForestResult = gridSearch(rForestPipeline, rForestParams)\n",
    "reportScores(rForestResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters searched: ngram 10-12, max features 10-12, C 0-2, found: ngram 11, max features 12000\n",
    "#introducing PCA made the computer die so don't use it again\n",
    "lsvcResult = gridSearch(lsvcPipeline, lsvcParams)\n",
    "reportScores(lsvcResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA ONES TO DO AFTERWARDS, KEEP ADDING THEM BELOW THEN COMMENT THE RESULTS UP ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 210 candidates, totalling 1050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 30.7min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 40.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed: 42.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('rf', RandomForestClassifier())\n",
      "The best parameters found were: {'cv__max_features': 12000, 'cv__ngram_range': (1, 15), 'rf__n_estimators': 100}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.05      0.08      1209\n",
      "           1       0.68      0.94      0.79      2546\n",
      "\n",
      "    accuracy                           0.65      3755\n",
      "   macro avg       0.48      0.49      0.43      3755\n",
      "weighted avg       0.55      0.65      0.56      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rForestResult = gridSearch(rForestPipeline, rForestParams)\n",
    "reportScores(rForestResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS ---- \n",
      "\n",
      "The algorithm being optimised was: ('lsvc', LinearSVC())\n",
      "The best parameters found were: {'cv__max_features': 15000, 'cv__ngram_range': (1, 13), 'lsvc__C': 1}\n",
      "Score report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.80      0.71      1209\n",
      "           1       0.89      0.78      0.84      2546\n",
      "\n",
      "    accuracy                           0.79      3755\n",
      "   macro avg       0.77      0.79      0.77      3755\n",
      "weighted avg       0.81      0.79      0.79      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lsvcResult = gridSearch(lsvcPipeline, lsvcParams)\n",
    "reportScores(lsvcResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mNBResult = gridSearch(mNaiveBayesPipeline, mNaiveBayesParams)\n",
    "reportScores(mNBResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW TRY DOING THE SAME BUT USING TFIDF INSTEAD\n",
    "\n",
    "mnbtPipeline = Pipeline([\n",
    "    ('tf', TfidfVectorizer()), \n",
    "    #('dt', Densifier()), #used for converting from sparse matrix to dense matrix\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#the second lot of parameters to try\n",
    "mnbtParams = {\n",
    "    'tf__ngram_range': [(1,0.5 * x) for x in range(2 * 12,2 *17)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'nb__alpha' : [0.25 * i for i in range (0,6)]\n",
    "}\n",
    "\n",
    "#ignore this one for now\n",
    "cnbtPipeline = Pipeline([('tf', TfidfVectorizer()), ('cnb', ComplementNB())])\n",
    "cnbtParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(14,18)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'cnb__alpha' : [1 * i for i in range (0,9)]\n",
    "}\n",
    "\n",
    "#add: ('pca', PCA()),\n",
    "lsvctPipeline = Pipeline([('tf', TfidfVectorizer()), ('df', Densifier()), ('lsvc', LinearSVC())])\n",
    "lsvctParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(10,15)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'lsvc__C' : [1 * i for i in range(0,3)]\n",
    "}\n",
    "\n",
    "rftPipeline = Pipeline([('tf', TfidfVectorizer()), ('rf', RandomForestClassifier())])\n",
    "rftParams = {\n",
    "    'tf__ngram_range': [(1,x) for x in range(13,19)],\n",
    "    'tf__norm' : [\"l1\", \"l2\"],\n",
    "    'rf__n_estimators' : [10 * i for i in range(10,15)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST SEARCH: ngram 14-18, alpha 0-6, RESULT: alpha = 0.5, ngram = 14, norm = l2, accuracy = 87\n",
    "\n",
    "#SECOND SEARCH: \n",
    "mnbtResult = gridSearch(mnbtPipeline, mnbtParams)\n",
    "reportScores(mnbtResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnbtResult = gridSearch(cnbtPipeline, cnbtParams)\n",
    "reportScores(cnbtResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvctResult = gridSearch(lsvctPipeline, lsvctParams)\n",
    "reportScores(lsvctResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 39.7min\n"
     ]
    }
   ],
   "source": [
    "#FIRST SEARCH: \n",
    "rftResult = gridSearch(rftPipeline, rftParams)\n",
    "reportScores(rftResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
