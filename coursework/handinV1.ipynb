{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the path to the training and test directories on your device here\n",
    "TRAINING_PATH = \"mediaeval-2015-trainingset.txt\"\n",
    "TESTING_PATH = \"mediaeval-2015-testset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_training = pd.read_csv(TRAINING_PATH, delimiter = \"\\t\")\n",
    "original_testing = pd.read_csv(TESTING_PATH, delimiter = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns apart from the text and the label as none of the other data appears to be useful\n",
    "original_training = original_training.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "#Do the same for the testing set\n",
    "original_testing = original_testing.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column to store the language, initially empty before langdetect populates it\n",
    "original_training[\"lang\"] = np.nan\n",
    "original_testing[\"lang\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Responsible for parsing tweets\n",
    "class TweetHandler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        snowball_langs = list(SnowballStemmer.languages)\n",
    "        #some languages are supported by stemming but NOT supported by language specific tokenizing,\n",
    "        #only the tokens that are in this set are supported by language specific tokenizing\n",
    "        self.tokenizer_langs = {\"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"it\", \"pt\", \"ru\", \"es\", \"sv\"}\n",
    "        langdetect_langs = [\"ar\", \"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"hu\", \"it\", \"no\", \"illegal\", \"pt\", \"ro\", \"ru\", \"es\", \"sv\"]\n",
    "        #a dictionary to map the corresponding snowball and langdetect properties\n",
    "        self.lang_dict = dict(zip(langdetect_langs, snowball_langs))\n",
    "        #declare some custom stop words\n",
    "        self.custom_stops = [\"http\",\"nhttp\",\"https\"]\n",
    "\n",
    "    #takes a tweet, detects its language, removes any stop words in the language, tokenizes and stems\n",
    "    #specific to the detected language and returns the simplified tokens paired with the language\n",
    "    def parse_tweet(self, tweet):\n",
    "        \n",
    "        try:\n",
    "            lang_prediction = detect(tweet)\n",
    "            #the nltk name for the predicted language\n",
    "            nltkprop = self.lang_dict[lang_prediction]\n",
    "        except:\n",
    "            #assume english stopwords and stemming if the language cannot be detected\n",
    "            lang_prediction = \"unknown\"\n",
    "            nltkprop = \"english\"\n",
    "            \n",
    "        # if the language is not supported by the tokenizer (including unkown) then assume tokenizing in English, however stemming\n",
    "        # and stopwords may still be supported in the language that does not support language specific tokenization\n",
    "        # e.g. arabic, hungarian, romanian so tokenize with the english\n",
    "        # version of the algorithm if this is the case and use the stemming and stopwords specific to \n",
    "        # the language if this is available even if the tokenization algorithm isnt\n",
    "        # use a python ternary expression to do this\n",
    "        tokens = word_tokenize(tweet, language = nltkprop if lang_prediction in self.tokenizer_langs else \"english\")\n",
    "        \n",
    "        #stop words specific to the language\n",
    "        stop_words = set(stopwords.words(nltkprop))\n",
    "        \n",
    "        #stemming algorithm specific to the language detected\n",
    "        stemmer = SnowballStemmer(nltkprop)\n",
    "        \n",
    "        # store all tokens to be output as a concatenated string here so that this string\n",
    "        # can later be fed to a CountVectorizer or TfIDFVectorizer , filter out any unwanted tokens \n",
    "        # and don't add them \n",
    "        filtered_tokens = \"\"\n",
    "        \n",
    "        for tok in tokens:\n",
    "            \n",
    "            #remove any hashtags\n",
    "            if tok[0] == '#':\n",
    "                tok = tok[1:]\n",
    "                \n",
    "            #discard non alphanumeric strings containing symbols or pure digits, or stop words\n",
    "            if (not tok.isalnum()) or tok.isdigit() or (tok in stop_words) or tok in self.custom_stops:\n",
    "                continue;\n",
    "            \n",
    "            #carry out stemming specific to the language detected\n",
    "            filtered_tokens += \" \" + stemmer.stem(tok)\n",
    "        \n",
    "        return filtered_tokens, lang_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataset from a dataset of tweets into a dataset of labelled tokens in concatenated\n",
    "# string form, along with the detected language\n",
    "\n",
    "def transform_data(arg):\n",
    "\n",
    "    #copy the argument given so we don't change the original instance and can keep it in memory and reuse it \n",
    "    #if necessary\n",
    "    dataset = deepcopy(arg)\n",
    "    \n",
    "    th = TweetHandler()\n",
    "    num_rows = dataset.label.size\n",
    "    \n",
    "    #the tweet text will be transformed into tokens so rename the column appropriately\n",
    "    dataset = dataset.rename(columns = {\"tweetText\" : \"tokens\"})\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "\n",
    "        tweet = dataset.tokens[i]\n",
    "        label = dataset.label[i]\n",
    "\n",
    "        #disregard the humour information for now, map humor and fake to a single class\n",
    "        if (\"humor\" in label) or (\"fake\" in label):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        tokens, lang = th.parse_tweet(tweet)\n",
    "        \n",
    "        #replace the row with the simplified tokens, the mapped labels and the detected language\n",
    "        dataset.loc[i] = tokens, label, lang\n",
    "    \n",
    "    #make sure the label column is converted into a column of integers and not objects\n",
    "    dataset.label = dataset.label.astype(\"int\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data and populate language column\n",
    "simplified_training = transform_data(original_training)\n",
    "simplified_testing = transform_data(original_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('bnb',\n",
       "                              Pipeline(steps=[('cv',\n",
       "                                               CountVectorizer(max_features=5000,\n",
       "                                                               ngram_range=(1,\n",
       "                                                                            2))),\n",
       "                                              ('bnb', BernoulliNB(alpha=2))])),\n",
       "                             ('mnb',\n",
       "                              Pipeline(steps=[('tf',\n",
       "                                               TfidfVectorizer(ngram_range=(1,\n",
       "                                                                            9))),\n",
       "                                              ('mnb',\n",
       "                                               MultinomialNB(alpha=0.25))])),\n",
       "                             ('rf',\n",
       "                              Pipeline(steps=[('cv',\n",
       "                                               CountVectorizer(max_features=18000)),\n",
       "                                              ('rf',\n",
       "                                               RandomForestClassifier(n_estimators=800,\n",
       "                                                                      n_jobs=-1,\n",
       "                                                                      random_state=1))]))],\n",
       "                 n_jobs=-1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,7), max_features = 23000)),\n",
    "    (\"sgd\", SGDClassifier(alpha = 0.0001, l1_ratio = 0.6, penalty = \"elasticnet\", random_state = 1, n_jobs = -1))\n",
    "])\n",
    "\n",
    "bnb_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,2), max_features = 5000)),\n",
    "    (\"bnb\", BernoulliNB(alpha = 2))\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"cv\", CountVectorizer(ngram_range = (1,1), max_features = 18000)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators = 800, max_depth = None, max_features = \"auto\", random_state = 1, n_jobs = -1))\n",
    "])\n",
    "\n",
    "mnb_pipe = Pipeline([\n",
    "    (\"tf\", TfidfVectorizer(ngram_range = (1,9))),\n",
    "    (\"mnb\", MultinomialNB(alpha = 0.25))\n",
    "])\n",
    "\n",
    "vcf = VotingClassifier([\n",
    "    #(\"sgd\", sgd_pipe),\n",
    "    (\"bnb\", bnb_pipe),\n",
    "    (\"mnb\", mnb_pipe),\n",
    "    (\"rf\", rf_pipe)\n",
    "], voting = \"hard\", n_jobs = -1)\n",
    "\n",
    "vcf.fit(simplified_training.tokens, simplified_training.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = simplified_testing.label\n",
    "predictions = vcf.predict(simplified_testing.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(true, predictions):\n",
    "    \n",
    "    print(\"-------------------- REPORT --------------------\\n\")\n",
    "    \n",
    "    #Format the scores to 2 decimal places\n",
    "    print(\"F1 score:\", \"%0.2f\" % f1_score(true, predictions))\n",
    "    print(\"\\nPrecision score:\", \"%0.2f\" % precision_score(true, predictions))\n",
    "    print(\"\\nRecall score:\", \"%0.2f\" % recall_score(true, predictions))\n",
    "        \n",
    "    print(\"\\nConfusion matrix:\\n\\n\", confusion_matrix(true,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- REPORT --------------------\n",
      "\n",
      "F1 score: 0.90\n",
      "\n",
      "Precision score: 0.87\n",
      "\n",
      "Recall score: 0.94\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      " [[ 850  359]\n",
      " [ 164 2382]]\n"
     ]
    }
   ],
   "source": [
    "report(true, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
