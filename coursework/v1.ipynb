{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import emoji\n",
    "\n",
    "#make the columns as wide as possible so we can see all the text\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>264117164670345217</td>\n",
       "      <td>So touching! RT @DreamCameTrue_: RT @Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/yBaVo3FZ /via @CarrieFairygirl</td>\n",
       "      <td>38027705</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>getdodge</td>\n",
       "      <td>Thu Nov 01 21:30:13 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>264077086992515072</td>\n",
       "      <td>Thank goodness for people who are kind #sandy http://t.co/Pc25SgSy /via @CarrieFairygirl</td>\n",
       "      <td>446472003</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>V_Heritier</td>\n",
       "      <td>Thu Nov 01 18:50:58 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>264072942894333953</td>\n",
       "      <td>RT “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wfMtqwjl /via @CarrieFairygirl”\\n\\nComplete fire hazard</td>\n",
       "      <td>24610808</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>rhianbeak</td>\n",
       "      <td>Thu Nov 01 18:34:30 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>264066811618799616</td>\n",
       "      <td>RT “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wfMtqwjl /via @CarrieFairygirl”\\n\\nComplete fire hazard</td>\n",
       "      <td>245520280</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>anthonylmorris</td>\n",
       "      <td>Thu Nov 01 18:10:08 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>264121176094277633</td>\n",
       "      <td>Thank goodness for people who are kind  #sandy http://t.co/ghz8E5je /via @CarrieFairygirl</td>\n",
       "      <td>876643928</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>cobiani6</td>\n",
       "      <td>Thu Nov 01 21:46:10 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>264061037748297728</td>\n",
       "      <td>@Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/WE72RVCE /via @CarrieFairygirl LOVE THIS!</td>\n",
       "      <td>82540644</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>JaclynPetrina</td>\n",
       "      <td>Thu Nov 01 17:47:12 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>264096394330136576</td>\n",
       "      <td>RT @alyssa_milano: Thank goodness for people who are kind  #sandy http://t.co/45y8vlMQ /via @CarrieFairygirl</td>\n",
       "      <td>28058204</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>dawnmitchellat9</td>\n",
       "      <td>Thu Nov 01 20:07:41 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>264208541357858816</td>\n",
       "      <td>“@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wuZ5PyNk /via @CarrieFairygirl” Awesome.</td>\n",
       "      <td>457207664</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>EDaSilva_BJJ</td>\n",
       "      <td>Fri Nov 02 03:33:19 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>264074496212885504</td>\n",
       "      <td>RT @Alyssa_Milano Thank goodness for people who are kind  #sandy http://t.co/3g21wFvG /via @CarrieFairygirl</td>\n",
       "      <td>22082801</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>PrettyPris</td>\n",
       "      <td>Thu Nov 01 18:40:40 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>264062774043348992</td>\n",
       "      <td>É, a humanidade ainda tem jeito. “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/xwu5jSIg /via @CarrieFairygirl”</td>\n",
       "      <td>48806882</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>roddybarros</td>\n",
       "      <td>Thu Nov 01 17:54:06 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>264055715155562497</td>\n",
       "      <td>Thank goodness for people who are kind  #sandy http://t.co/Do9M8Jag /via @CarrieFairygirl</td>\n",
       "      <td>26642006</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>Alyssa_Milano</td>\n",
       "      <td>Thu Nov 01 17:26:03 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>264061192639770624</td>\n",
       "      <td>Thank goodness for people who are kind @Alyssa_Milano  #sandy http://t.co/4KVRkeqi</td>\n",
       "      <td>194833121</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>Turnedsideways</td>\n",
       "      <td>Thu Nov 01 17:47:49 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>264105100216262659</td>\n",
       "      <td>THIS is amazing! RT @Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/xIzaj5UN /via @CarrieFairygirl</td>\n",
       "      <td>131806327</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>Drop_of_Jupiter</td>\n",
       "      <td>Thu Nov 01 20:42:17 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>264059094011367424</td>\n",
       "      <td>Plug in Pals RT @Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/gXB0pHGX /via @CarrieFairygirl</td>\n",
       "      <td>404532969</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>AmyFreeze7</td>\n",
       "      <td>Thu Nov 01 17:39:28 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>264838364807700482</td>\n",
       "      <td>: RT@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/IrwVfQXc”\\nAngoisse et lumières!”</td>\n",
       "      <td>99528569</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>carolinism</td>\n",
       "      <td>Sat Nov 03 21:16:01 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10015</th>\n",
       "      <td>264856558012334080</td>\n",
       "      <td>Splendide ! “@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/U4whKtFD” cc @NYMag</td>\n",
       "      <td>46665201</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>Manget</td>\n",
       "      <td>Sat Nov 03 22:28:18 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>265084063809945601</td>\n",
       "      <td>RT @confidentiels La une du New-York magazine sur Sandy : \"The City and the Storm\" http://t.co/IiC0brts</td>\n",
       "      <td>43174692</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>BlondieJustine</td>\n",
       "      <td>Sun Nov 04 13:32:20 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10017</th>\n",
       "      <td>264832250795917312</td>\n",
       "      <td>: RT@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/IrwVfQXc”\\nAngoisse et lumières!”</td>\n",
       "      <td>535166689</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>AMaddy2</td>\n",
       "      <td>Sat Nov 03 20:51:43 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>264823352202502144</td>\n",
       "      <td>Magnifique !! “@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/Ylo9TvOy”</td>\n",
       "      <td>430630320</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>LefeuvreAnthony</td>\n",
       "      <td>Sat Nov 03 20:16:22 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>265229166184644608</td>\n",
       "      <td>NY: The City and the Storm. The Look of Post-Sandy New York.... http://t.co/AL5UZXkm</td>\n",
       "      <td>815549341</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>LuengoAlberto</td>\n",
       "      <td>Sun Nov 04 23:08:56 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweetId  \\\n",
       "10000  264117164670345217   \n",
       "10001  264077086992515072   \n",
       "10002  264072942894333953   \n",
       "10003  264066811618799616   \n",
       "10004  264121176094277633   \n",
       "10005  264061037748297728   \n",
       "10006  264096394330136576   \n",
       "10007  264208541357858816   \n",
       "10008  264074496212885504   \n",
       "10009  264062774043348992   \n",
       "10010  264055715155562497   \n",
       "10011  264061192639770624   \n",
       "10012  264105100216262659   \n",
       "10013  264059094011367424   \n",
       "10014  264838364807700482   \n",
       "10015  264856558012334080   \n",
       "10016  265084063809945601   \n",
       "10017  264832250795917312   \n",
       "10018  264823352202502144   \n",
       "10019  265229166184644608   \n",
       "\n",
       "                                                                                                                                          tweetText  \\\n",
       "10000  So touching! RT @DreamCameTrue_: RT @Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/yBaVo3FZ /via @CarrieFairygirl   \n",
       "10001                                                      Thank goodness for people who are kind #sandy http://t.co/Pc25SgSy /via @CarrieFairygirl   \n",
       "10002        RT “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wfMtqwjl /via @CarrieFairygirl”\\n\\nComplete fire hazard   \n",
       "10003        RT “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wfMtqwjl /via @CarrieFairygirl”\\n\\nComplete fire hazard   \n",
       "10004                                                     Thank goodness for people who are kind  #sandy http://t.co/ghz8E5je /via @CarrieFairygirl   \n",
       "10005                           @Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/WE72RVCE /via @CarrieFairygirl LOVE THIS!   \n",
       "10006                                  RT @alyssa_milano: Thank goodness for people who are kind  #sandy http://t.co/45y8vlMQ /via @CarrieFairygirl   \n",
       "10007                          “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/wuZ5PyNk /via @CarrieFairygirl” Awesome.   \n",
       "10008                                   RT @Alyssa_Milano Thank goodness for people who are kind  #sandy http://t.co/3g21wFvG /via @CarrieFairygirl   \n",
       "10009  É, a humanidade ainda tem jeito. “@Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/xwu5jSIg /via @CarrieFairygirl”   \n",
       "10010                                                     Thank goodness for people who are kind  #sandy http://t.co/Do9M8Jag /via @CarrieFairygirl   \n",
       "10011                                                            Thank goodness for people who are kind @Alyssa_Milano  #sandy http://t.co/4KVRkeqi   \n",
       "10012                  THIS is amazing! RT @Alyssa_Milano: Thank goodness for people who are kind #sandy http://t.co/xIzaj5UN /via @CarrieFairygirl   \n",
       "10013                     Plug in Pals RT @Alyssa_Milano: Thank goodness for people who are kind  #sandy http://t.co/gXB0pHGX /via @CarrieFairygirl   \n",
       "10014         : RT@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/IrwVfQXc”\\nAngoisse et lumières!”   \n",
       "10015              Splendide ! “@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/U4whKtFD” cc @NYMag   \n",
       "10016                                       RT @confidentiels La une du New-York magazine sur Sandy : \"The City and the Storm\" http://t.co/IiC0brts   \n",
       "10017         : RT@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/IrwVfQXc”\\nAngoisse et lumières!”   \n",
       "10018                      Magnifique !! “@confidentiels: La une du New-York magazine sur Sandy : \"The City and the Storm\"... http://t.co/Ylo9TvOy”   \n",
       "10019                                                          NY: The City and the Storm. The Look of Post-Sandy New York.... http://t.co/AL5UZXkm   \n",
       "\n",
       "          userId      imageId(s)         username  \\\n",
       "10000   38027705  sandyB_real_71         getdodge   \n",
       "10001  446472003  sandyB_real_71       V_Heritier   \n",
       "10002   24610808  sandyB_real_71        rhianbeak   \n",
       "10003  245520280  sandyB_real_71   anthonylmorris   \n",
       "10004  876643928  sandyB_real_71         cobiani6   \n",
       "10005   82540644  sandyB_real_71    JaclynPetrina   \n",
       "10006   28058204  sandyB_real_71  dawnmitchellat9   \n",
       "10007  457207664  sandyB_real_71     EDaSilva_BJJ   \n",
       "10008   22082801  sandyB_real_71       PrettyPris   \n",
       "10009   48806882  sandyB_real_71      roddybarros   \n",
       "10010   26642006  sandyB_real_71    Alyssa_Milano   \n",
       "10011  194833121  sandyB_real_71   Turnedsideways   \n",
       "10012  131806327  sandyB_real_71  Drop_of_Jupiter   \n",
       "10013  404532969  sandyB_real_71       AmyFreeze7   \n",
       "10014   99528569  sandyB_real_59       carolinism   \n",
       "10015   46665201  sandyB_real_59           Manget   \n",
       "10016   43174692  sandyB_real_59   BlondieJustine   \n",
       "10017  535166689  sandyB_real_59          AMaddy2   \n",
       "10018  430630320  sandyB_real_59  LefeuvreAnthony   \n",
       "10019  815549341  sandyB_real_59    LuengoAlberto   \n",
       "\n",
       "                            timestamp label  lang  \n",
       "10000  Thu Nov 01 21:30:13 +0000 2012  real   NaN  \n",
       "10001  Thu Nov 01 18:50:58 +0000 2012  real   NaN  \n",
       "10002  Thu Nov 01 18:34:30 +0000 2012  real   NaN  \n",
       "10003  Thu Nov 01 18:10:08 +0000 2012  real   NaN  \n",
       "10004  Thu Nov 01 21:46:10 +0000 2012  real   NaN  \n",
       "10005  Thu Nov 01 17:47:12 +0000 2012  real   NaN  \n",
       "10006  Thu Nov 01 20:07:41 +0000 2012  real   NaN  \n",
       "10007  Fri Nov 02 03:33:19 +0000 2012  real   NaN  \n",
       "10008  Thu Nov 01 18:40:40 +0000 2012  real   NaN  \n",
       "10009  Thu Nov 01 17:54:06 +0000 2012  real   NaN  \n",
       "10010  Thu Nov 01 17:26:03 +0000 2012  real   NaN  \n",
       "10011  Thu Nov 01 17:47:49 +0000 2012  real   NaN  \n",
       "10012  Thu Nov 01 20:42:17 +0000 2012  real   NaN  \n",
       "10013  Thu Nov 01 17:39:28 +0000 2012  real   NaN  \n",
       "10014  Sat Nov 03 21:16:01 +0000 2012  real   NaN  \n",
       "10015  Sat Nov 03 22:28:18 +0000 2012  real   NaN  \n",
       "10016  Sun Nov 04 13:32:20 +0000 2012  real   NaN  \n",
       "10017  Sat Nov 03 20:51:43 +0000 2012  real   NaN  \n",
       "10018  Sat Nov 03 20:16:22 +0000 2012  real   NaN  \n",
       "10019  Sun Nov 04 23:08:56 +0000 2012  real   NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_training = pd.read_csv(\"mediaeval-2015-trainingset.txt\", delimiter = \"\\t\")\n",
    "#original_training = original_training.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "\n",
    "#add a column to store the language\n",
    "original_training[\"lang\"] = np.nan\n",
    "original_training[10000:10020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Se acuerdan de la película: “El día después de mañana”? Me recuerda a lo que está pasando con el huracán #Sandy. http://t.co/JQQeRPwN</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del huracán. Parece el \"Día de la Independencia 2\" http://t.co/41jUweux REAL! RT.</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Huracán Sandy, me recuerda a la película Día de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty 🗽 http://t.co/Ex61doZk</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14272</th>\n",
       "      <td>@BobombDom *slaps TweetDeck with the PigFish http: \\/\\/t.co\\/pyHcJn0jwA</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14273</th>\n",
       "      <td>New Species of Fish found in Brazil or just Really good Photoshop??? What You Think? ������������ http: \\/\\/t.co\\/a6bG8mWcUs</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14274</th>\n",
       "      <td>What do we call this? #pigFISH http: \\/\\/t.co\\/4Bml62OD15</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14275</th>\n",
       "      <td>Pigfish ? E dopo il pescecane c'è il pesce maiale ???? http: \\/\\/t.co\\/hQzWGhyDef</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14276</th>\n",
       "      <td>For those who can't decide between fish or meat.....#Pigfish http: \\/\\/t.co\\/5JBtF54cmg</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14277 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    tweetText  \\\n",
       "0      ¿Se acuerdan de la película: “El día después de mañana”? Me recuerda a lo que está pasando con el huracán #Sandy. http://t.co/JQQeRPwN   \n",
       "1       @milenagimon: Miren a Sandy en NY!  Tremenda imagen del huracán. Parece el \"Día de la Independencia 2\" http://t.co/41jUweux REAL! RT.   \n",
       "2                        Buena la foto del Huracán Sandy, me recuerda a la película Día de la Independencia #ID4 #Sandy  http://t.co/PTdAXABZ   \n",
       "3                                                                                              Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4                                                   My fave place in the world #nyc #hurricane #sandy #statueofliberty 🗽 http://t.co/Ex61doZk   \n",
       "...                                                                                                                                       ...   \n",
       "14272                                                                 @BobombDom *slaps TweetDeck with the PigFish http: \\/\\/t.co\\/pyHcJn0jwA   \n",
       "14273            New Species of Fish found in Brazil or just Really good Photoshop??? What You Think? ������������ http: \\/\\/t.co\\/a6bG8mWcUs   \n",
       "14274                                                                               What do we call this? #pigFISH http: \\/\\/t.co\\/4Bml62OD15   \n",
       "14275                                                       Pigfish ? E dopo il pescecane c'è il pesce maiale ???? http: \\/\\/t.co\\/hQzWGhyDef   \n",
       "14276                                                 For those who can't decide between fish or meat.....#Pigfish http: \\/\\/t.co\\/5JBtF54cmg   \n",
       "\n",
       "      label  lang  \n",
       "0      fake   NaN  \n",
       "1      fake   NaN  \n",
       "2      fake   NaN  \n",
       "3      fake   NaN  \n",
       "4      fake   NaN  \n",
       "...     ...   ...  \n",
       "14272  fake   NaN  \n",
       "14273  fake   NaN  \n",
       "14274  fake   NaN  \n",
       "14275  fake   NaN  \n",
       "14276  fake   NaN  \n",
       "\n",
       "[14277 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop all columns apart from the text and the label as none of the other data appears to be useful\n",
    "original_training = original_training.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "original_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>@Cafeinomania acojonaica #sandy http://t.co/gpXOAXsk</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3616</th>\n",
       "      <td>#uragano #hurricane #sandy #newyork la statua della libertà è al sicuro! http://t.co/IlsG52G3</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>#Sandy : ça fait peur!!! http://t.co/sTFW6wQI</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3642</th>\n",
       "      <td>Haha RT @TorstenBeeck Ich bin mir nicht sicher, ob das geshopped ist, sieht ziemlich echt aus ... #sandy #Hurricane http://t.co/wwApNeeJ</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3676</th>\n",
       "      <td>Here comes #sandy http://t.co/V43y2tnP</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704</th>\n",
       "      <td>#Sandy en #NewYork \\n\\n#NewJersey #Hurricane #HuracánSandy #Mexico #Puebla http://t.co/jZBv94cy</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>هاا خلص الإعصار ولا لاء ؟ XD #إعصار_ساندي #sandy http://t.co/XpAKOzif</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>вносим позитивную волну в ленту по поводу США . Итак статуя свободы после урагана  #Sandy .RT)) http://t.co/cRdLgP8d</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>Mientras tanto en New Jersey... http://t.co/uNg2OvMI #Sandy de #Naranja #humor</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>Ouragan #Sandy 2012.\\nN'oublions jamais. http://t.co/6L33BQnn</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>Ich bin mir nicht sicher, ob das geshopped ist, sieht ziemlich echt aus ... #sandy #Hurricane http://t.co/bm3wtHEC</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>Be scared be very scared! :p #sandy http://t.co/sy2Jl2BB</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>#Sandy ???? http://t.co/yLkbBoOH</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3744</th>\n",
       "      <td>что-то ужасное творится в NewJersey\\n#NewJersey #PrayForAmerica #Sandy http://t.co/2ok9Cgbu</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>El huracán #sandy choca fuertemente contra NY. #bruuuuutal http://t.co/8CYevt7G</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>#sandy was so scary !! don't expect me to go out for #halloween http://t.co/VVNWCdfg</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>#Alerta #urgente asi fue el paso de #Sandy por #LaEstatuadelaLibertad http://t.co/sdZv6KPq</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>“@m1mmo Ora che Sandy è passato, sono tanti i cambiamenti a cui dovrà abituarcisi... http://t.co/6K06CtJY”</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>rip sandy http://t.co/05VEd8Nm</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>Sandy Kasırgası, New York kıyılarına ulaştı... http://t.co/amA31ZJJ</td>\n",
       "      <td>humor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     tweetText  \\\n",
       "3606                                                                                      @Cafeinomania acojonaica #sandy http://t.co/gpXOAXsk   \n",
       "3616                                             #uragano #hurricane #sandy #newyork la statua della libertà è al sicuro! http://t.co/IlsG52G3   \n",
       "3623                                                                                             #Sandy : ça fait peur!!! http://t.co/sTFW6wQI   \n",
       "3642  Haha RT @TorstenBeeck Ich bin mir nicht sicher, ob das geshopped ist, sieht ziemlich echt aus ... #sandy #Hurricane http://t.co/wwApNeeJ   \n",
       "3676                                                                                                    Here comes #sandy http://t.co/V43y2tnP   \n",
       "3704                                           #Sandy en #NewYork \\n\\n#NewJersey #Hurricane #HuracánSandy #Mexico #Puebla http://t.co/jZBv94cy   \n",
       "3724                                                                     هاا خلص الإعصار ولا لاء ؟ XD #إعصار_ساندي #sandy http://t.co/XpAKOzif   \n",
       "3731                      вносим позитивную волну в ленту по поводу США . Итак статуя свободы после урагана  #Sandy .RT)) http://t.co/cRdLgP8d   \n",
       "3732                                                            Mientras tanto en New Jersey... http://t.co/uNg2OvMI #Sandy de #Naranja #humor   \n",
       "3736                                                                             Ouragan #Sandy 2012.\\nN'oublions jamais. http://t.co/6L33BQnn   \n",
       "3738                        Ich bin mir nicht sicher, ob das geshopped ist, sieht ziemlich echt aus ... #sandy #Hurricane http://t.co/bm3wtHEC   \n",
       "3741                                                                                  Be scared be very scared! :p #sandy http://t.co/sy2Jl2BB   \n",
       "3743                                                                                                          #Sandy ???? http://t.co/yLkbBoOH   \n",
       "3744                                               что-то ужасное творится в NewJersey\\n#NewJersey #PrayForAmerica #Sandy http://t.co/2ok9Cgbu   \n",
       "3745                                                           El huracán #sandy choca fuertemente contra NY. #bruuuuutal http://t.co/8CYevt7G   \n",
       "3749                                                      #sandy was so scary !! don't expect me to go out for #halloween http://t.co/VVNWCdfg   \n",
       "3750                                                #Alerta #urgente asi fue el paso de #Sandy por #LaEstatuadelaLibertad http://t.co/sdZv6KPq   \n",
       "3753                                “@m1mmo Ora che Sandy è passato, sono tanti i cambiamenti a cui dovrà abituarcisi... http://t.co/6K06CtJY”   \n",
       "3754                                                                                                            rip sandy http://t.co/05VEd8Nm   \n",
       "3755                                                                       Sandy Kasırgası, New York kıyılarına ulaştı... http://t.co/amA31ZJJ   \n",
       "\n",
       "      label  lang  \n",
       "3606  humor   NaN  \n",
       "3616  humor   NaN  \n",
       "3623  humor   NaN  \n",
       "3642  humor   NaN  \n",
       "3676  humor   NaN  \n",
       "3704  humor   NaN  \n",
       "3724  humor   NaN  \n",
       "3731  humor   NaN  \n",
       "3732  humor   NaN  \n",
       "3736  humor   NaN  \n",
       "3738  humor   NaN  \n",
       "3741  humor   NaN  \n",
       "3743  humor   NaN  \n",
       "3744  humor   NaN  \n",
       "3745  humor   NaN  \n",
       "3749  humor   NaN  \n",
       "3750  humor   NaN  \n",
       "3753  humor   NaN  \n",
       "3754  humor   NaN  \n",
       "3755  humor   NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View some of the fake/real/humor tweets to see their general characteristics\n",
    "original_training[original_training[\"label\"]==\"humor\"][300:320]\n",
    "\n",
    "#original_training[original_training[\"label\"]==\"real\"][100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake     6742\n",
       "real     4921\n",
       "humor    2614\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_training.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS.... http://t.co/je2hcFpVfN</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse from ISS.... http://t.co/oqwtTL0ThS</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“@Shyman33: Eclipse from ISS.... http://t.co/C0VfboScRj” 우주에서본 3.20 일식 Wow! amazing!</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eclipse from ISS.... http://t.co/En87OtvsU6</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ebonfigli: Éclipse vue de l'ISS... Autre chose... http://t.co/yNBN7c4O51\\n\\nLa création divine n'a pas de limite 😍</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>Un présentateur de la ZDF confesse avoir truqué la vidéo du doigt de Varoufakis à l'Allemagne #satire http://t.co/FTVpMv6IUK via @lalibrebe</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>Oh les kleine menteurs \"@CorineBarella: Un présentateur ZDF confesse avoir truqué vidéo du doigt de Varoufakis  http://t.co/7lEkEZgjOY\"</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>Este es el programa de ZDF en el que confirman que el video de Varoufakis es un montaje https://t.co/MJa3aoRBIy</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>11.34 - wir haben FAST Mittag ▶ Riesen Verwirrung um Varoufakis-Video ZDF-Comedian Jan Böhmermann ▶ http://t.co/rRLu2cfWcB</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>Sorry, @yanisvaroufakis! https://t.co/BSkYrbIIle\\n\\n(now with ENGLISH subtitles, full English and Greek subtitles available soon) #varoufake</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3755 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         tweetText  \\\n",
       "0                                                                                kereeen RT @Shyman33: Eclipse from ISS.... http://t.co/je2hcFpVfN   \n",
       "1                                                                  Absolutely beautiful! RT @Shyman33: Eclipse from ISS.... http://t.co/oqwtTL0ThS   \n",
       "2                                                             “@Shyman33: Eclipse from ISS.... http://t.co/C0VfboScRj” 우주에서본 3.20 일식 Wow! amazing!   \n",
       "3                                                                                                      Eclipse from ISS.... http://t.co/En87OtvsU6   \n",
       "4                              @ebonfigli: Éclipse vue de l'ISS... Autre chose... http://t.co/yNBN7c4O51\\n\\nLa création divine n'a pas de limite 😍   \n",
       "...                                                                                                                                            ...   \n",
       "3750   Un présentateur de la ZDF confesse avoir truqué la vidéo du doigt de Varoufakis à l'Allemagne #satire http://t.co/FTVpMv6IUK via @lalibrebe   \n",
       "3751       Oh les kleine menteurs \"@CorineBarella: Un présentateur ZDF confesse avoir truqué vidéo du doigt de Varoufakis  http://t.co/7lEkEZgjOY\"   \n",
       "3752                               Este es el programa de ZDF en el que confirman que el video de Varoufakis es un montaje https://t.co/MJa3aoRBIy   \n",
       "3753                    11.34 - wir haben FAST Mittag ▶ Riesen Verwirrung um Varoufakis-Video ZDF-Comedian Jan Böhmermann ▶ http://t.co/rRLu2cfWcB   \n",
       "3754  Sorry, @yanisvaroufakis! https://t.co/BSkYrbIIle\\n\\n(now with ENGLISH subtitles, full English and Greek subtitles available soon) #varoufake   \n",
       "\n",
       "     label  lang  \n",
       "0     fake   NaN  \n",
       "1     fake   NaN  \n",
       "2     fake   NaN  \n",
       "3     fake   NaN  \n",
       "4     fake   NaN  \n",
       "...    ...   ...  \n",
       "3750  fake   NaN  \n",
       "3751  fake   NaN  \n",
       "3752  fake   NaN  \n",
       "3753  fake   NaN  \n",
       "3754  fake   NaN  \n",
       "\n",
       "[3755 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_testing = pd.read_csv(\"mediaeval-2015-testset.txt\", delimiter = \"\\t\")\n",
    "original_testing = original_testing.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "original_testing[\"lang\"] = np.nan\n",
    "original_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14277 entries, 0 to 14276\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   tweetText  14277 non-null  object \n",
      " 1   label      14277 non-null  object \n",
      " 2   lang       0 non-null      float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 334.7+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3755 entries, 0 to 3754\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   tweetText  3755 non-null   object \n",
      " 1   label      3755 non-null   object \n",
      " 2   lang       0 non-null      float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 88.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#there are no non null values to being with we can see\n",
    "original_training.info()\n",
    "print(\"\\n\")\n",
    "original_testing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/georgegarrington/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import langdetect as l\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk.stem as st\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#responsible for parsing tweetws\n",
    "class TweetHandler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        snowball_langs = list(st.SnowballStemmer.languages)\n",
    "        self.tokenizer_langs = {\"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"it\", \"pt\", \"ru\", \"es\", \"sv\"}\n",
    "        langdetect_langs = [\"ar\", \"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"hu\", \"it\", \"no\", \"illegal\", \"pt\", \"ro\", \"ru\", \"es\", \"sv\"]\n",
    "        #a dictionary to map the corresponding snowball and langdetect properties\n",
    "        self.lang_dict = dict(zip(langdetect_langs, snowball_langs))\n",
    "        #declare some custom stop words\n",
    "        self.custom_stops = [\"http\",\"nhttp\"]\n",
    "\n",
    "    #takes a tweet, detects its language, removes any stop words in the language,\n",
    "    #stems tokens in language and returns the simplified tokens paired with the language\n",
    "    def parse_tweet(self, tweet):\n",
    "        \n",
    "        try:\n",
    "            lang_prediction = l.detect(tweet)\n",
    "            #the nltk name for the predicted language\n",
    "            nltkprop = self.lang_dict[lang_prediction]\n",
    "        except:\n",
    "            #assume english stopwords and stemming if the language cannot be detected\n",
    "            lang_prediction = \"unknown\"\n",
    "            nltkprop = \"english\"\n",
    "            \n",
    "        #if the language is not supported by the tokenizer (including unkown) then assume tokenizing in English, however stemming\n",
    "        #and stopwords may still be supported in the language e.g. arabic, hungarian, romanian\n",
    "        tokens = nltk.word_tokenize(tweet, language = nltkprop if lang_prediction in self.tokenizer_langs else \"english\")\n",
    "        stop_words = set(stopwords.words(nltkprop))\n",
    "        stemmer = st.SnowballStemmer(nltkprop)\n",
    "        filtered_tokens = set()\n",
    "        \n",
    "        for tok in tokens:\n",
    "            #remove any hashtags\n",
    "            if tok[0] == '#':\n",
    "                tok = tok[1:]\n",
    "            #discard non alphanumeric strings containing symbols and digits, or stop words\n",
    "            if (not tok.isalnum()) or tok.isdigit() or (tok in stop_words) or tok in self.custom_stops:\n",
    "                continue;\n",
    "            filtered_tokens.add(stemmer.stem(tok))            \n",
    "        \n",
    "        #comment these out when you do not need to check if it works anymore\n",
    "        #print(\"original tokens:\", tokens,\"\\n\")\n",
    "        #print(\"filtered tokens:\", filtered_tokens,\"\\n\")\n",
    "        \n",
    "        return filtered_tokens, lang_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the name of the property to tokens which is more appropriate\n",
    "#training_set = training_set.rename(columns = {\"tweetText\" : \"tokens\"})\n",
    "\n",
    "#transform the dataset into parsed tokens\n",
    "def transform_data(arg):\n",
    "\n",
    "    #copy the instance given so we don't change the original instance and can keep it in memory and reuse it \n",
    "    #if necessary\n",
    "    dataset = copy.deepcopy(arg)\n",
    "    th = TweetHandler()\n",
    "    num_rows = dataset.label.size\n",
    "    \n",
    "    #the tweet text will be transformed into tokens so rename the column appropriately\n",
    "    dataset = dataset.rename(columns = {\"tweetText\" : \"tokens\"})\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "\n",
    "        tweet = dataset[\"tokens\"][i]\n",
    "        label = dataset[\"label\"][i]\n",
    "\n",
    "        #disregard the humour information for now, simplify the classes to two different classes\n",
    "        if (\"humor\" in label) or (\"fake\" in label):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "            \n",
    "        #for testing\n",
    "        #print(\"The old value of the row is:\",dataset.loc[i],\"\\n\")\n",
    "        \n",
    "        tokens, lang = th.parse_tweet(tweet)\n",
    "        \n",
    "        #change the type of tweetText from a string to a set of strings of the stemmed tokenized words\n",
    "        dataset.loc[i] = tokens, label, lang\n",
    "        \n",
    "        #for testing\n",
    "        #print(\"The new value of the row is:\",dataset.loc[i],\"\\n\\n\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_training = transform_data(original_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_testing = transform_data(original_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{rt, shyman33, eclips, kereeen, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{rt, beauti, shyman33, eclips, absolut, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{우주에서본, 일식, wow, shyman33, eclips, amaz, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{eclips, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{éclips, ebonfigl, chos, création, divin, a, limit, vu, autr}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>{varoufak, avoir, truqu, vidéo, présent, lalibreb, vi, doigt, un, satir, zdf, confess}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>{varoufak, avoir, truqu, vidéo, présent, doigt, zdf, corinebarel, un, menteur, oh, klein, confess}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>{confirm, este, program, varoufakis, montaj, vide, https, zdf}</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>{jan, bohmermann, fast, ries, mittag, verwirr}</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>{varoufak, soon, greek, subtitl, english, sorri, yanisvaroufaki, full, https, avail}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3755 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  tokens  \\\n",
       "0                                                                   {rt, shyman33, eclips, kereeen, iss}   \n",
       "1                                                           {rt, beauti, shyman33, eclips, absolut, iss}   \n",
       "2                                                          {우주에서본, 일식, wow, shyman33, eclips, amaz, iss}   \n",
       "3                                                                                          {eclips, iss}   \n",
       "4                                          {éclips, ebonfigl, chos, création, divin, a, limit, vu, autr}   \n",
       "...                                                                                                  ...   \n",
       "3750              {varoufak, avoir, truqu, vidéo, présent, lalibreb, vi, doigt, un, satir, zdf, confess}   \n",
       "3751  {varoufak, avoir, truqu, vidéo, présent, doigt, zdf, corinebarel, un, menteur, oh, klein, confess}   \n",
       "3752                                      {confirm, este, program, varoufakis, montaj, vide, https, zdf}   \n",
       "3753                                                      {jan, bohmermann, fast, ries, mittag, verwirr}   \n",
       "3754                {varoufak, soon, greek, subtitl, english, sorri, yanisvaroufaki, full, https, avail}   \n",
       "\n",
       "     label lang  \n",
       "0        1   en  \n",
       "1        1   en  \n",
       "2        1   en  \n",
       "3        1   en  \n",
       "4        1   fr  \n",
       "...    ...  ...  \n",
       "3750     1   fr  \n",
       "3751     1   fr  \n",
       "3752     1   es  \n",
       "3753     1   de  \n",
       "3754     1   en  \n",
       "\n",
       "[3755 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README            estonian.pickle   italian.pickle    slovene.pickle\r\n",
      "czech.pickle      finnish.pickle    norwegian.pickle  spanish.pickle\r\n",
      "danish.pickle     french.pickle     polish.pickle     swedish.pickle\r\n",
      "dutch.pickle      german.pickle     portuguese.pickle turkish.pickle\r\n",
      "english.pickle    greek.pickle      russian.pickle\r\n"
     ]
    }
   ],
   "source": [
    "#view the tokenization languages that are supported\n",
    "!ls /Users/georgegarrington/nltk_data/tokenizers/punkt/PY3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9356\n",
       "0    4921\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_training.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(dataset):\n",
    "    dataset[\"sentence\"] = dataset[\"tokens\"].apply(' '.join)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't need this anymore\n",
    "def bool_to_int(dataset):\n",
    "    dataset[\"label\"] = dataset[\"label\"].apply(lambda x: (1 if x == True else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_to_int(simplified_training)\n",
    "bool_to_int(simplified_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9356\n",
       "0    4921\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_training[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_testing = transform_data(original_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{rt, shyman33, eclips, kereeen, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{rt, beauti, shyman33, eclips, absolut, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{우주에서본, 일식, wow, shyman33, eclips, amaz, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{eclips, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{éclips, ebonfigl, chos, création, divin, a, limit, vu, autr}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>{varoufak, avoir, truqu, vidéo, présent, lalibreb, vi, doigt, un, satir, zdf, confess}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>{varoufak, avoir, truqu, vidéo, présent, doigt, zdf, corinebarel, un, menteur, oh, klein, confess}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>{confirm, este, program, varoufakis, montaj, vide, https, zdf}</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>{jan, bohmermann, fast, ries, mittag, verwirr}</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>{varoufak, soon, greek, subtitl, english, sorri, yanisvaroufaki, full, https, avail}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3755 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  tokens  \\\n",
       "0                                                                   {rt, shyman33, eclips, kereeen, iss}   \n",
       "1                                                           {rt, beauti, shyman33, eclips, absolut, iss}   \n",
       "2                                                          {우주에서본, 일식, wow, shyman33, eclips, amaz, iss}   \n",
       "3                                                                                          {eclips, iss}   \n",
       "4                                          {éclips, ebonfigl, chos, création, divin, a, limit, vu, autr}   \n",
       "...                                                                                                  ...   \n",
       "3750              {varoufak, avoir, truqu, vidéo, présent, lalibreb, vi, doigt, un, satir, zdf, confess}   \n",
       "3751  {varoufak, avoir, truqu, vidéo, présent, doigt, zdf, corinebarel, un, menteur, oh, klein, confess}   \n",
       "3752                                      {confirm, este, program, varoufakis, montaj, vide, https, zdf}   \n",
       "3753                                                      {jan, bohmermann, fast, ries, mittag, verwirr}   \n",
       "3754                {varoufak, soon, greek, subtitl, english, sorri, yanisvaroufaki, full, https, avail}   \n",
       "\n",
       "     label lang  \n",
       "0        1   en  \n",
       "1        1   en  \n",
       "2        1   en  \n",
       "3        1   en  \n",
       "4        1   fr  \n",
       "...    ...  ...  \n",
       "3750     1   fr  \n",
       "3751     1   fr  \n",
       "3752     1   es  \n",
       "3753     1   de  \n",
       "3754     1   en  \n",
       "\n",
       "[3755 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{mañan, pelicul, el, recuerd, huracan, sandy, acuerd, me, dia, pas, despues}</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>mañan pelicul el recuerd huracan sandy acuerd me dia pas despues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{independent, real, rt, huracan, milenagimon, sandy, parec, mir, imag, tremend, dia, ny}</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>independent real rt huracan milenagimon sandy parec mir imag tremend dia ny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{independent, pelicul, huracan, recuerd, sandy, id4, dia, buen, fot}</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>independent pelicul huracan recuerd sandy id4 dia buen fot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{hurrican, ny, shit, scari}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>hurrican ny shit scari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{hurrican, place, nyc, fave, world, my, sandi, statueofliberti}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>hurrican place nyc fave world my sandi statueofliberti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14272</th>\n",
       "      <td>{slap, pigfish, bobombdom, tweetdeck}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>slap pigfish bobombdom tweetdeck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14273</th>\n",
       "      <td>{found, photoshop, speci, fish, you, brazil, good, what, think, new, realli}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>found photoshop speci fish you brazil good what think new realli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14274</th>\n",
       "      <td>{what, pigfish, call}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>what pigfish call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14275</th>\n",
       "      <td>{e, pesc, dop, maial, pigfish, pescecan}</td>\n",
       "      <td>1</td>\n",
       "      <td>it</td>\n",
       "      <td>e pesc dop maial pigfish pescecan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14276</th>\n",
       "      <td>{fish, ca, pigfish, for, decid, meat}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>fish ca pigfish for decid meat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14277 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         tokens  \\\n",
       "0                  {mañan, pelicul, el, recuerd, huracan, sandy, acuerd, me, dia, pas, despues}   \n",
       "1      {independent, real, rt, huracan, milenagimon, sandy, parec, mir, imag, tremend, dia, ny}   \n",
       "2                          {independent, pelicul, huracan, recuerd, sandy, id4, dia, buen, fot}   \n",
       "3                                                                   {hurrican, ny, shit, scari}   \n",
       "4                               {hurrican, place, nyc, fave, world, my, sandi, statueofliberti}   \n",
       "...                                                                                         ...   \n",
       "14272                                                     {slap, pigfish, bobombdom, tweetdeck}   \n",
       "14273              {found, photoshop, speci, fish, you, brazil, good, what, think, new, realli}   \n",
       "14274                                                                     {what, pigfish, call}   \n",
       "14275                                                  {e, pesc, dop, maial, pigfish, pescecan}   \n",
       "14276                                                     {fish, ca, pigfish, for, decid, meat}   \n",
       "\n",
       "       label lang  \\\n",
       "0          1   es   \n",
       "1          1   es   \n",
       "2          1   es   \n",
       "3          1   en   \n",
       "4          1   en   \n",
       "...      ...  ...   \n",
       "14272      1   en   \n",
       "14273      1   en   \n",
       "14274      1   en   \n",
       "14275      1   it   \n",
       "14276      1   en   \n",
       "\n",
       "                                                                          sentence  \n",
       "0                 mañan pelicul el recuerd huracan sandy acuerd me dia pas despues  \n",
       "1      independent real rt huracan milenagimon sandy parec mir imag tremend dia ny  \n",
       "2                       independent pelicul huracan recuerd sandy id4 dia buen fot  \n",
       "3                                                           hurrican ny shit scari  \n",
       "4                           hurrican place nyc fave world my sandi statueofliberti  \n",
       "...                                                                            ...  \n",
       "14272                                             slap pigfish bobombdom tweetdeck  \n",
       "14273             found photoshop speci fish you brazil good what think new realli  \n",
       "14274                                                            what pigfish call  \n",
       "14275                                            e pesc dop maial pigfish pescecan  \n",
       "14276                                               fish ca pigfish for decid meat  \n",
       "\n",
       "[14277 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_tokens(simplified_training)\n",
    "simplified_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{rt, shyman33, eclips, kereeen, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>rt shyman33 eclips kereeen iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{rt, beauti, shyman33, eclips, absolut, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>rt beauti shyman33 eclips absolut iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{우주에서본, 일식, wow, shyman33, eclips, amaz, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>우주에서본 일식 wow shyman33 eclips amaz iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{eclips, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>eclips iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{éclips, ebonfigl, chos, création, divin, a, limit, vu, autr}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>éclips ebonfigl chos création divin a limit vu autr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>{varoufak, avoir, truqu, vidéo, présent, lalibreb, vi, doigt, un, satir, zdf, confess}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>varoufak avoir truqu vidéo présent lalibreb vi doigt un satir zdf confess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>{varoufak, avoir, truqu, vidéo, présent, doigt, zdf, corinebarel, un, menteur, oh, klein, confess}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>varoufak avoir truqu vidéo présent doigt zdf corinebarel un menteur oh klein confess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>{confirm, este, program, varoufakis, montaj, vide, https, zdf}</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>confirm este program varoufakis montaj vide https zdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>{jan, bohmermann, fast, ries, mittag, verwirr}</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>jan bohmermann fast ries mittag verwirr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>{varoufak, soon, greek, subtitl, english, sorri, yanisvaroufaki, full, https, avail}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>varoufak soon greek subtitl english sorri yanisvaroufaki full https avail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3755 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  tokens  \\\n",
       "0                                                                   {rt, shyman33, eclips, kereeen, iss}   \n",
       "1                                                           {rt, beauti, shyman33, eclips, absolut, iss}   \n",
       "2                                                          {우주에서본, 일식, wow, shyman33, eclips, amaz, iss}   \n",
       "3                                                                                          {eclips, iss}   \n",
       "4                                          {éclips, ebonfigl, chos, création, divin, a, limit, vu, autr}   \n",
       "...                                                                                                  ...   \n",
       "3750              {varoufak, avoir, truqu, vidéo, présent, lalibreb, vi, doigt, un, satir, zdf, confess}   \n",
       "3751  {varoufak, avoir, truqu, vidéo, présent, doigt, zdf, corinebarel, un, menteur, oh, klein, confess}   \n",
       "3752                                      {confirm, este, program, varoufakis, montaj, vide, https, zdf}   \n",
       "3753                                                      {jan, bohmermann, fast, ries, mittag, verwirr}   \n",
       "3754                {varoufak, soon, greek, subtitl, english, sorri, yanisvaroufaki, full, https, avail}   \n",
       "\n",
       "     label lang  \\\n",
       "0        1   en   \n",
       "1        1   en   \n",
       "2        1   en   \n",
       "3        1   en   \n",
       "4        1   fr   \n",
       "...    ...  ...   \n",
       "3750     1   fr   \n",
       "3751     1   fr   \n",
       "3752     1   es   \n",
       "3753     1   de   \n",
       "3754     1   en   \n",
       "\n",
       "                                                                                  sentence  \n",
       "0                                                           rt shyman33 eclips kereeen iss  \n",
       "1                                                    rt beauti shyman33 eclips absolut iss  \n",
       "2                                                    우주에서본 일식 wow shyman33 eclips amaz iss  \n",
       "3                                                                               eclips iss  \n",
       "4                                      éclips ebonfigl chos création divin a limit vu autr  \n",
       "...                                                                                    ...  \n",
       "3750             varoufak avoir truqu vidéo présent lalibreb vi doigt un satir zdf confess  \n",
       "3751  varoufak avoir truqu vidéo présent doigt zdf corinebarel un menteur oh klein confess  \n",
       "3752                                 confirm este program varoufakis montaj vide https zdf  \n",
       "3753                                               jan bohmermann fast ries mittag verwirr  \n",
       "3754             varoufak soon greek subtitl english sorri yanisvaroufaki full https avail  \n",
       "\n",
       "[3755 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_tokens(simplified_testing)\n",
    "simplified_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{rt, shyman33, eclips, kereeen, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>rt shyman33 eclips kereeen iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{rt, beauti, shyman33, eclips, absolut, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>rt beauti shyman33 eclips absolut iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{우주에서본, 일식, wow, shyman33, eclips, amaz, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>우주에서본 일식 wow shyman33 eclips amaz iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{eclips, iss}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>eclips iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{éclips, ebonfigl, chos, création, divin, a, limit, vu, autr}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>éclips ebonfigl chos création divin a limit vu autr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>{varoufak, avoir, truqu, vidéo, présent, lalibreb, vi, doigt, un, satir, zdf, confess}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>varoufak avoir truqu vidéo présent lalibreb vi doigt un satir zdf confess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>{varoufak, avoir, truqu, vidéo, présent, doigt, zdf, corinebarel, un, menteur, oh, klein, confess}</td>\n",
       "      <td>1</td>\n",
       "      <td>fr</td>\n",
       "      <td>varoufak avoir truqu vidéo présent doigt zdf corinebarel un menteur oh klein confess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>{confirm, este, program, varoufakis, montaj, vide, https, zdf}</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>confirm este program varoufakis montaj vide https zdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>{jan, bohmermann, fast, ries, mittag, verwirr}</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>jan bohmermann fast ries mittag verwirr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>{varoufak, soon, greek, subtitl, english, sorri, yanisvaroufaki, full, https, avail}</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>varoufak soon greek subtitl english sorri yanisvaroufaki full https avail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3755 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  tokens  \\\n",
       "0                                                                   {rt, shyman33, eclips, kereeen, iss}   \n",
       "1                                                           {rt, beauti, shyman33, eclips, absolut, iss}   \n",
       "2                                                          {우주에서본, 일식, wow, shyman33, eclips, amaz, iss}   \n",
       "3                                                                                          {eclips, iss}   \n",
       "4                                          {éclips, ebonfigl, chos, création, divin, a, limit, vu, autr}   \n",
       "...                                                                                                  ...   \n",
       "3750              {varoufak, avoir, truqu, vidéo, présent, lalibreb, vi, doigt, un, satir, zdf, confess}   \n",
       "3751  {varoufak, avoir, truqu, vidéo, présent, doigt, zdf, corinebarel, un, menteur, oh, klein, confess}   \n",
       "3752                                      {confirm, este, program, varoufakis, montaj, vide, https, zdf}   \n",
       "3753                                                      {jan, bohmermann, fast, ries, mittag, verwirr}   \n",
       "3754                {varoufak, soon, greek, subtitl, english, sorri, yanisvaroufaki, full, https, avail}   \n",
       "\n",
       "     label lang  \\\n",
       "0        1   en   \n",
       "1        1   en   \n",
       "2        1   en   \n",
       "3        1   en   \n",
       "4        1   fr   \n",
       "...    ...  ...   \n",
       "3750     1   fr   \n",
       "3751     1   fr   \n",
       "3752     1   es   \n",
       "3753     1   de   \n",
       "3754     1   en   \n",
       "\n",
       "                                                                                  sentence  \n",
       "0                                                           rt shyman33 eclips kereeen iss  \n",
       "1                                                    rt beauti shyman33 eclips absolut iss  \n",
       "2                                                    우주에서본 일식 wow shyman33 eclips amaz iss  \n",
       "3                                                                               eclips iss  \n",
       "4                                      éclips ebonfigl chos création divin a limit vu autr  \n",
       "...                                                                                    ...  \n",
       "3750             varoufak avoir truqu vidéo présent lalibreb vi doigt un satir zdf confess  \n",
       "3751  varoufak avoir truqu vidéo présent doigt zdf corinebarel un menteur oh klein confess  \n",
       "3752                                 confirm este program varoufakis montaj vide https zdf  \n",
       "3753                                               jan bohmermann fast ries mittag verwirr  \n",
       "3754             varoufak soon greek subtitl english sorri yanisvaroufaki full https avail  \n",
       "\n",
       "[3755 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake     6742\n",
       "real     4921\n",
       "humor    2614\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_training[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en         10970\n",
       "es          1287\n",
       "unknown     1046\n",
       "fr           216\n",
       "pt           162\n",
       "de           130\n",
       "it            99\n",
       "nl            86\n",
       "ar            81\n",
       "ru            62\n",
       "sv            48\n",
       "da            33\n",
       "no            32\n",
       "fi            15\n",
       "hu             6\n",
       "ro             4\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_training[\"lang\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14277 entries, 0 to 14276\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tokens    14277 non-null  object\n",
      " 1   label     14277 non-null  int64 \n",
      " 2   lang      14277 non-null  object\n",
      " 3   sentence  14277 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 446.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#we can see that some data has been dropped, presumably this is because no\n",
    "#language could be detected in some tweets. We have only lost 7.5% of the training data\n",
    "#so I will simply drop this for now in the first iteration as I do not think this\n",
    "# should make a significant difference\n",
    "simplified_training.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14277 entries, 0 to 14276\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tokens    14277 non-null  object\n",
      " 1   label     14277 non-null  int64 \n",
      " 2   lang      14277 non-null  object\n",
      " 3   sentence  14277 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 557.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#now we can see there are no more null entries\n",
    "simplified_training = simplified_training.dropna()\n",
    "simplified_training.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9356\n",
       "0    4921\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the ratio of fake and real tweets is still similar after removing null rows\n",
    "simplified_training[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{sandi}                                                                                          243\n",
      "{sandi, hurrican}                                                                                 87\n",
      "{hurricanesandi}                                                                                  56\n",
      "{bringbackourgirl}                                                                                49\n",
      "{abc, littl, statenisland, toy, fli, those, unbeliev, sandi, scene, helicopt, nypd}               42\n",
      "                                                                                                ... \n",
      "{squar, hurrican, nbc, imag, empti, factboook, time, gurl, manhattan, darci, sandi, courtesi}      1\n",
      "{rt, lavsmohan, statu, puneersoda, lol, hide, liberti, sandi, rofl, hahaha}                        1\n",
      "{sandi, damn, huirricanesandi, hurricanesandi}                                                     1\n",
      "{air, univers, fli, jet, spot, low, jungl, miss, malaysia, thai, mh370, student}                   1\n",
      "{sea, seasid, casinopi, hurricanesandi, nj}                                                        1\n",
      "Name: tokens, Length: 11070, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(simplified_training[\"tokens\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_features with 2550 seems to give the best result\n",
    "cv = CountVectorizer(ngram_range = (1,5), max_features = 7000)\n",
    "\n",
    "#ORIGINALLY TRIED (1,1) with max_features set to 1000. Then performed a grid search on ngram_range and max_features\n",
    "#and found optimal to be ngram_range(1,5) and max_features to be 7000\n",
    "\n",
    "X_train = cv.fit_transform(simplified_training[\"sentence\"]).todense()\n",
    "X_test = cv.transform(simplified_testing[\"sentence\"]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "#penalty whether or not to use l1 norm or l2 norm\n",
    "#C is the regularization parameter\n",
    "#lsvc = LinearSVC(penalty = \"l1\", C = 1.0, verbose = 1, random_state = 0, tol=1e-5)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, simplified_training.label)\n",
    "\n",
    "\n",
    "\n",
    "#pipeline = make_pipeline(StandardScaler(), lsvc)\n",
    "#pipeline.fit(X_train, simplified_training[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **kwarge):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 335, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 641, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 763, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1047, in check_non_negative\n",
      "    raise ValueError(\"Negative values in data passed to %s\" % whom)\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 335, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 641, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 763, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1047, in check_non_negative\n",
      "    raise ValueError(\"Negative values in data passed to %s\" % whom)\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 335, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 641, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 763, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1047, in check_non_negative\n",
      "    raise ValueError(\"Negative values in data passed to %s\" % whom)\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 335, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 641, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 763, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1047, in check_non_negative\n",
      "    raise ValueError(\"Negative values in data passed to %s\" % whom)\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('cv', CountVectorizer()), ('dt', Dense()), ('nb', MultinomialNB())])\n",
    "clf = GridSearchCV(pipeline, {\n",
    "    'cv__ngram_range': [(1,x) for x in range(6,7)],\n",
    "    'cv__max_features' : [1000* i for i in range(10,11)],\n",
    "    'nb__alpha' : [0.5 * i for i in range (0,10)]\n",
    "},scoring='f1')\n",
    "clf.fit(simplified_training[\"sentence\"],simplified_training.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true = simplified_testing.label\n",
    "y_test_pred = clf.predict(simplified_testing[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-428f846dde6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2546\n",
       "0    1209\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_true.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_true.values.reshape(-1).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.73      1209\n",
      "           1       0.89      0.84      0.86      2546\n",
      "\n",
      "    accuracy                           0.82      3755\n",
      "   macro avg       0.79      0.81      0.80      3755\n",
      "weighted avg       0.83      0.82      0.82      3755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_true.values.reshape(-1).astype('int32'), y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7835465070503856"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true = simplified_testing.label, y_pred = predictions, average = \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
