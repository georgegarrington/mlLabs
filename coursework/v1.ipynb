{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweetText label\n",
      "0      ¿Se acuerdan de la película: “El día después d...  fake\n",
      "1      @milenagimon: Miren a Sandy en NY!  Tremenda i...  fake\n",
      "2      Buena la foto del Huracán Sandy, me recuerda a...  fake\n",
      "3         Scary shit #hurricane #NY http://t.co/e4JLBUfH  fake\n",
      "4      My fave place in the world #nyc #hurricane #sa...  fake\n",
      "...                                                  ...   ...\n",
      "14272  @BobombDom *slaps TweetDeck with the PigFish h...  fake\n",
      "14273  New Species of Fish found in Brazil or just Re...  fake\n",
      "14274  What do we call this? #pigFISH http: \\/\\/t.co\\...  fake\n",
      "14275  Pigfish ? E dopo il pescecane c'è il pesce mai...  fake\n",
      "14276  For those who can't decide between fish or mea...  fake\n",
      "\n",
      "[14277 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "training_set = pd.read_csv(\"mediaeval-2015-trainingset.txt\", delimiter = \"\\t\")\n",
    "training_set = training_set.drop([\"tweetId\", \"userId\", \"imageId(s)\", \"username\", \"timestamp\"], axis = 1)\n",
    "print(training_set)\n",
    "\n",
    "testing_set = pd.read_csv(\"mediaeval-2015-testset.txt\", delimiter = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect as l\n",
    "import nltk.stem as st\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetHandler:\n",
    "    \n",
    "    snowball_langs = list(st.SnowballStemmer.languages)\n",
    "    langdetect_langs = [\"ar\", \"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"hu\", \"it\", \"no\", \"illegal\", \"pt\", \"ro\", \"ru\", \"es\", \"sv\"]\n",
    "    lang_dict = dict(zip(langdetect_langs, snowball_langs))\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tzr = TweetTokenizer()\n",
    "        pass\n",
    "    \n",
    "    def handle_tweet(self, tweet):\n",
    "    \n",
    "        try:\n",
    "            lang_prediction = l.detect(tweet)\n",
    "        except:\n",
    "            lang_prediction = \"unknown\"\n",
    "            \n",
    "        tokens = self.tzr.tokenize(tweet)\n",
    "        filtered_tokens = set()\n",
    "        \n",
    "        for tok in tokens:\n",
    "            #remove any hashtags\n",
    "            if tok[0] == '#':\n",
    "                tok = tok[1:]\n",
    "            #discard non alphanumeric strings containing symbols and digits\n",
    "            if not tok.isalnum() or tok.isdigit():\n",
    "                continue;\n",
    "            filtered_tokens.add(tok)\n",
    "        \n",
    "        return filtered_tokens, lang_prediction\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'acuerdan', 'que', 'Me', 'recuerda', 'mañana', 'con', 'lo', 'está', 'de', 'El', 'el', 'la', 'a', 'película', 'Se', 'pasando', 'Sandy', 'huracán', 'después', 'día'}, 'es')\n"
     ]
    }
   ],
   "source": [
    "handler = TweetHandler()\n",
    "res = handler.handle_tweet(training_set[\"tweetText\"][0])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#illegal as this is the key corresponding to porter stemmer I think which we are not using\n",
    "snowball_langs = list(SnowballStemmer.languages)\n",
    "langdetect_langs = [\"ar\", \"da\", \"nl\", \"en\", \"fi\", \"fr\", \"de\", \"hu\", \"it\", \"no\", \"illegal\", \"pt\", \"ro\", \"ru\", \"es\", \"sv\"]\n",
    "lang_dict = dict(zip(langdetect_langs, snowball_langs))\n",
    "\n",
    "def handle_tweet(tweet):\n",
    "    \n",
    "    #tokenizer the tweet with a TWEET TOKENIZER\n",
    "    tokens = tzr.tokenize(tweet)\n",
    "    \n",
    "    #Keep track of the number of tokens (possibly) detected in each language. Assume that\n",
    "    #the tweet is in the language with the most tokens seen\n",
    "    langs_seen = {}\n",
    "    \n",
    "    filtered_toks = set()\n",
    "    \n",
    "    #remove punctuation and digits\n",
    "    for tok in tokens:\n",
    "    \n",
    "        #If there is a prefix hashtag remove it\n",
    "        if tok[0] == '#':\n",
    "            tok = tok[1:]\n",
    "        \n",
    "        #discard non alphanumeric strings and digits\n",
    "        if not tok.isalnum() or tok.isdigit():\n",
    "            continue;\n",
    "            \n",
    "        filtered_toks.add(tok)\n",
    "    \n",
    "    #first determine the language\n",
    "    for tok in filtered_toks:\n",
    "        \n",
    "        #if the token language cannot be detectected then label as unknown\n",
    "        try:\n",
    "            #print(\"The token is:\",tok)\n",
    "            guess = detect(tok)\n",
    "            #print(\"My guess is:\",guess)\n",
    "        except:\n",
    "            guess = \"unknown\"\n",
    "            \n",
    "        if guess in langs_seen:\n",
    "            langs_seen[guess] += 1\n",
    "        else:\n",
    "            langs_seen[guess] = 1\n",
    "        \n",
    "    #the most common language guess seen\n",
    "    predicted_language = max(langs_seen, key=langs_seen.get)\n",
    "    print(\"The original tweet was:\", tweet,\"\\n\")\n",
    "    #print(\"The tokens were:\", filtered_toks)\n",
    "    print(\"The language guesses were:\",langs_seen)\n",
    "    print(\"I predicted the language:\",predicted_language,\"\\n\")\n",
    "    output = set()\n",
    "    \n",
    "    #print(\"Does snowball stemmer support it?\")\n",
    "    \n",
    "    #only stem the tokens if snowball stemmer supports the language\n",
    "    if predicted_language in lang_dict.keys():\n",
    "    \n",
    "        #print(\"Yes it does\")\n",
    "    \n",
    "        stop_words = stopwords.words(lang_dict[predicted_language])\n",
    "        stemmer = SnowballStemmer(lang_dict[predicted_language])\n",
    "    \n",
    "        #now remove stopwords, links, hashtags and stem the words\n",
    "        for tok in filtered_toks:\n",
    "            \n",
    "            #discard any non alphanumeric tokens or stopwords\n",
    "            if(tok in stop_words):\n",
    "                continue;\n",
    "\n",
    "            #stem the token with it's language specific stemmer\n",
    "            output.add(stemmer.stem(tok))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"(Unsupported by snowball stem)\")\n",
    "        return None\n",
    "        #output = filtered_toks\n",
    "        #print(\"No it doesn't\")\n",
    "            \n",
    "    # LOOK ABOVE AND BELOW ME, should change this so you stem the language with the stemmer \n",
    "    # for the language it is detected in\n",
    "        \n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
