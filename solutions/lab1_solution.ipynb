{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOguF95OyAOs"
   },
   "source": [
    "# COMP3222/6246 Machine Learning Technologies (2019/20)\n",
    "# Lab 1 – Basics of Machine Learning project\n",
    "\n",
    "In this week, we will have an overview of how a practical Machine Learning project works. We aim to familiarise you with the general procedure of doing Machine Learning, while encouraging you to develop your critical thinking by asking you some questions now and then. \n",
    "\n",
    "In general, a Machine Learning project is not different from a software project, where you might want to go back and forth and tweak something, or roll out the first prototype and improve on it incrementally. Answering the questions will help you understand more, and allow you to come up with an idea for improving the Machine Learning prototype we introduced here.\n",
    "\n",
    "Note that you will not learn by simply executing this notebook without playing with it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_iIy8cEHyAOs"
   },
   "source": [
    "## Defining the problem\n",
    "\n",
    "Say, we are given a task to **predict a house price in California**. Depending on a dataset, this can be either a *supervised learning*, *reinforcement learning*, and so on. Clearly, we need to inspect the dataset first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUGoY8cDyAOt"
   },
   "source": [
    "## Getting the dataset\n",
    "\n",
    "We will use 1990 California consus data which is provided by Aurélien Géron on his Github: https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz. The description of this dataset is provided on https://github.com/ageron/handson-ml/blob/master/datasets/housing/README.md and http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCSZTHAPyAOt"
   },
   "outputs": [],
   "source": [
    "import urllib # This is URL handling module required for downloading files from the Internet\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz\", \"housing.tgz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_4K5n18yAOy"
   },
   "outputs": [],
   "source": [
    "import tarfile # This is used for decompressing .tgz file\n",
    "\n",
    "housing_tgz = tarfile.open(\"housing.tgz\")\n",
    "housing_tgz.extractall()\n",
    "housing_tgz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwXimItuyAO0"
   },
   "source": [
    "The *housing.tgz* file should be decompressed and saved on the current working directory as *housing.csv*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J5ji2f9XyAO1"
   },
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "Now, we should familiarise ourselves with the dataset. For example, you should know what kind of attributes (*numerical* or *categorical*?), how many datapoints, how many missing values, is it raw data or transformed data, and so forth. \n",
    "\n",
    "Note that, usually, a dataset that you will acquire in the real world cannot be used right away. You will need to perform *data cleaning* beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1mfOjZZSyAO1"
   },
   "outputs": [],
   "source": [
    "import pandas # This is a library that is mainly used for data manipulation and some basic analyses\n",
    "\n",
    "housing = pandas.read_csv(\"housing.csv\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHLSJWGoyAO4"
   },
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "czt7hWfEyAO7"
   },
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XLubikX2yAO9"
   },
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "65JMniM4yAPA"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # This is for making a plot similar to one in MATLAB\n",
    "housing.hist(bins=50, figsize=(20,15)) # Do you know why we choose 50 bins? Try playing with the number of bins and observe the difference.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5F4OdFYyAPC"
   },
   "source": [
    "What can we say about our dataset after this quick glance over it? What is the name of the variable that we need to predict with our Machine Learning technique? How many attributes or features do we have? Which attributes are numerical and which are not? Are there any missing values in the dataset? Is there any anomaly or outlier in the distribution of the attributes (shown in histogram)?\n",
    "\n",
    "The more we know about our dataset, the less problems we will have later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UETCxzhPyAPD"
   },
   "source": [
    "## Defining the problem and a performance measure\n",
    "\n",
    "After we have roughly explored the dataset, we now know that we need to solve a supervised-learning multivariate regression task, where we will use a Machine Learning algorithm **to predict *median_house_value* based on other attributes**. With this knowledge, you will be able to select a number of appropriate algorithms later.\n",
    "\n",
    "Moreover, we can pick a performance measure of our Machine Learning algorithm beforehand. There are a number of performance measures for regression task, but we will just use the Root Mean Square Error (RMSE) for now.\n",
    "\n",
    "$$\\text{RMSE} \\left( \\mathbf{Y} , \\mathbf{\\hat{Y}} \\right) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\hat{y_i} \\right)^2 } $$\n",
    "\n",
    "where $\\mathbf{Y}$ and $\\mathbf{\\hat{Y}}$ are an $n$-sized vector of true values and an $n$-sized vector of predicted values which comprises of $y_i$ and $\\hat{y_i}$ respectively for each datapoint $i$. In other words, RMSE is computed from a square root of an average squared error.\n",
    "\n",
    "Another well-known performance measure is the Mean Absolute Error (MAE), which is computed by taking an average of an absolute value of the error.\n",
    "\n",
    "$$\\text{MAE} \\left( \\mathbf{Y} , \\mathbf{\\hat{Y}} \\right) = \\frac{1}{n} \\sum_{i=1}^n | y_i - \\hat{y_i} | $$\n",
    "\n",
    "Both of them are good measures of how well our Machine Learning algorithm will perform. The lower the value is, the better our algorithm performs. However, different measures will pick up different aspects in the error that our Machine Learning produced. Now, what is the difference between the RMSE and the MAE in this regard? Which one would be appropriate for this house price prediction task? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4U4r6lDKyAPE"
   },
   "source": [
    "## Data cleaning: missing values and capped values\n",
    "\n",
    "By exploring our dataset, we are aware of at least 2 problems: namely, missing values and capped values. In particular, the attribute total_bedrooms has a number of values smaller than the other attributes, and there is a peak in the distribution of the attributes housing_median_age and median_house_value, which signifies a limit on the attribute's maximum value. These might be caused by how the data was collected; e.g. the survey's choice 'Prefer not to say', '>= 52', or '>= 500,001'.\n",
    "\n",
    "There are a number of ways to solve these issues: we can either (a) discard the attributes, (b) remove the datapoint that have these issues, or (c) replace the attributes with appropriate values. Different methods affect our Machine Learning algorithm in different ways. For examples, our Machine Learning algorithm might not work well if the discarded attributes are the key attributes for accurately predicting house prices. If we discard too many datapoints, our algorithm might not be able to properly learn. Similarly, filling/replacing attributes with incorrect values will also affect our prediction's accuracy.\n",
    "\n",
    "For the sake of simplicity, let's try removing those corrupted datapoints and see how well our Machine Learning algorithm can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Q6tCiuByAPE"
   },
   "outputs": [],
   "source": [
    "fltr_idx = housing['total_bedrooms'].notna() & (housing['housing_median_age'] < 52) & (housing['median_house_value'] < 500001) # retrieve boolean array where each value corresponds to datapoint we want\n",
    "fltr_housing = housing[fltr_idx].reset_index(drop=True) # select datapoints then reset its index\n",
    "fltr_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMTzSZk-yAPH"
   },
   "outputs": [],
   "source": [
    "fltr_housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtFixD4_yAPJ"
   },
   "outputs": [],
   "source": [
    "fltr_housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoVsJ_ZByAPL"
   },
   "outputs": [],
   "source": [
    "fltr_housing.hist(bins=50, figsize=(20,15)) # Do you know why we choose 50 bins? Try playing with the number of bins and observe the difference.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWkFUzh6yAPO"
   },
   "source": [
    "Now, we have cleaned our dataset by removing those datapoints that have missing values and capped values. Do you think this method is appropriate? Will there be any problem after you have applied Machine Learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HV5SgfC7yAPP"
   },
   "source": [
    "## Dealing with categorical attributes\n",
    "\n",
    "Another issue that should not be ignored is the presence of the categorical attribute 'ocean_proximity'. Since most Machine Learning algorithms work on numerical dataset only, we need to transform the categorical attribute to some numerical value that still represent its original meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lpdf3ij-yAPP"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoded_cat, categories = fltr_housing[\"ocean_proximity\"].factorize() # retrieve the attribute encoded as numbers\n",
    "encoded_cat_arr = OneHotEncoder().fit_transform(encoded_cat.reshape(-1,1)).toarray() # transform sparse matrix to NumPy array\n",
    "enc_fltr_housing = fltr_housing.iloc[:,0:9].copy()\n",
    "for i in range(0, len(categories)):\n",
    "    enc_fltr_housing[categories[i]] = encoded_cat_arr[:,i]\n",
    "enc_fltr_housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqOaun0MyAPT"
   },
   "source": [
    "We have just finished transforming each categorical value to a vector of binary values. As an alternative, we could have only a single numerical attribute that maps to the categories; e.g. 1 for 'NEAR BAY', 2 for '<1H OCEAN', etc. Compared to having a vector of binary values, what are the pros&cons of this approach? Will there be any problem later if we use this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iljMgBpyAPT"
   },
   "source": [
    "## Data partitioning: train set and test set\n",
    "\n",
    "How can we be sure that our Machine Learning algorithm will work in the real world? Since we need to evaluate our algorithm on some datapoints, now it is a good time to set aside a portion of the dataset as a *test set* and the rest as a *training set*. By treating a portion of the dataset as unseen data, you can test how good your Machine Learning algorithm will be likely to perform in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3-1cjv_yAPU"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "rnd_indices = numpy.random.permutation(len(enc_fltr_housing)) # Generate a random sequence of housing's index\n",
    "test_set_size = int(len(enc_fltr_housing) * 0.2) \n",
    "print(test_set_size, \" datapoints for test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpMLELgHyAPX"
   },
   "outputs": [],
   "source": [
    "test_indices = rnd_indices[:test_set_size]\n",
    "train_indices = rnd_indices[test_set_size:]\n",
    "print(test_indices, \" \", len(test_indices))\n",
    "print(train_indices, \" \", len(train_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kwEKlyAIyAPZ"
   },
   "outputs": [],
   "source": [
    "test_set1 = enc_fltr_housing.iloc[test_indices].reset_index(drop=True) # Pick data out of housing according to the test indices\n",
    "test_set1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbhd4pCXyAPc"
   },
   "outputs": [],
   "source": [
    "train_set1 = enc_fltr_housing.iloc[train_indices].reset_index(drop=True)\n",
    "train_set1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7593A57uyAPf"
   },
   "source": [
    "Now, we have just randomly put 20% of total datapoints into a test set and the rest into a training set. Do you think 20% is sufficient? Why should not we have less, so that our Machine Learning algorithm can harness more information from the larger training set? Vice versa, should we have a larger test set, so that we are more confident that our Machine Learning algorithm will perform well in the real world?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GbVIFbzIyAPh"
   },
   "source": [
    "## Choose and apply Machine Learning algorithm\n",
    "\n",
    "After so much work on preparing our dataset, we are ready to try our Machine Learning algorithm. Whilst there are many algorithms or *models* for regression task, let us apply the basic approach first: the Linear Regression algorithm. In many cases, a simple model such as the Linear Regression works perfectly fine. If the simple model is sufficient, then there is no need to apply complex algorithms which could require the tuning of many hyperparameters, larger number of datapoints, or longer time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtqJjV53yAPi"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lnr_regressor1 = LinearRegression()\n",
    "lnr_regressor1.fit(train_set1.iloc[:, [idx for idx in range(len(train_set1.columns)) if idx != 8]], train_set1['median_house_value'])\n",
    "prediction1 = lnr_regressor1.predict(test_set1.iloc[:, [idx for idx in range(len(test_set1.columns)) if idx != 8]])\n",
    "print('RMSE = ', numpy.sqrt(mean_squared_error(test_set1['median_house_value'], prediction1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LE8tfHRMyAPk"
   },
   "source": [
    "We have just trained a linear regression model based on our training set. Then, we have used it to predict the house price on our test set, and we have computed the RMSE to quantify how good our model is. Clearly, the RMSE we have got is very high. That implies that our Machine Learning algorithm is not performing well enough. Will the RMSE change if we redo everything again? What could have gone wrong? What could be done to improve our accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9tE0OtGDyAPl"
   },
   "source": [
    "## Feature engineering\n",
    "\n",
    "We have just finished our first prototype, but it doesn't seem to work well. As someone say: 'Garbage in, Garbage out'! So, it could be the case that our dataset is not comprised of useful attributes that are going to help our Machine Learning algorithm to learn and predict well. \n",
    "\n",
    "This is quite true if we observe carefully: a value of some attributes such as total_rooms and total_bedrooms represents the whole district/block's! We are predicting the price of one house, but our Machine Learning algorithm is working on district-level data. Obviously, a house with many rooms should be more expensive than a house with a smaller number of rooms. Similar reasoning goes for other attributes as well. Therefore, we should *engineer* our dataset so that it has a larger number of useful attributes or *features*. Let's try this out and see how much our Machine Learning algorithm will improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7NU3x12zyAPl"
   },
   "outputs": [],
   "source": [
    "train_set2 = train_set1.copy()\n",
    "train_set2['room_per_house'] = train_set2['total_rooms']/train_set2['households']\n",
    "train_set2['bedroom_per_room'] = train_set2['total_bedrooms']/train_set2['total_rooms']\n",
    "train_set2['pop_per_house'] = train_set2['population']/train_set2['households']\n",
    "train_set2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfISVHRLyAPn"
   },
   "outputs": [],
   "source": [
    "test_set2 = test_set1.copy()\n",
    "test_set2['room_per_house'] = test_set2['total_rooms']/test_set2['households']\n",
    "test_set2['bedroom_per_room'] = test_set2['total_bedrooms']/test_set2['total_rooms']\n",
    "test_set2['pop_per_house'] = test_set2['population']/test_set2['households']\n",
    "test_set2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7KhjZKpwyAPq"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lnr_regressor2 = LinearRegression()\n",
    "lnr_regressor2.fit(train_set2.iloc[:, [idx for idx in range(len(train_set2.columns)) if idx != 8]], train_set2['median_house_value'])\n",
    "prediction2 = lnr_regressor2.predict(test_set2.iloc[:, [idx for idx in range(len(test_set2.columns)) if idx != 8]])\n",
    "print('RMSE = ', numpy.sqrt(mean_squared_error(test_set2['median_house_value'], prediction2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dM6NvGJ9yAPs"
   },
   "source": [
    "Whilst the improvement is not that significant, it has shown that *feature engineering* is very useful. There are more techniques than those we have just shown. Could you name some?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLdMndvsyAPs"
   },
   "source": [
    "## Choose and apply Machine Learning algorithm (again)\n",
    "\n",
    "It is possible that the Linear Regression model is not powerful enough to learn from our dataset. We could try different regression models: say, the Random Forest Regression. With Scikit-Learn, we can try many different algorithms easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NVF9VZO7yAPt"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "frst_regressor1 = RandomForestRegressor()\n",
    "frst_regressor1.fit(train_set1.iloc[:, [idx for idx in range(len(train_set1.columns)) if idx != 8]], train_set1['median_house_value'])\n",
    "prediction3 = frst_regressor1.predict(test_set1.iloc[:, [idx for idx in range(len(test_set1.columns)) if idx != 8]])\n",
    "print('RMSE = ', numpy.sqrt(mean_squared_error(test_set1['median_house_value'], prediction3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQy7BD_vyAPw"
   },
   "source": [
    "With the Forest Regression model, we have achieved a good improvement on the non-engineered dataset. Now, we could also try it on the engineered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGvh15wQyAPw"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "frst_regressor2 = RandomForestRegressor()\n",
    "frst_regressor2.fit(train_set2.iloc[:, [idx for idx in range(len(train_set2.columns)) if idx != 8]], train_set2['median_house_value'])\n",
    "prediction4 = frst_regressor2.predict(test_set2.iloc[:, [idx for idx in range(len(test_set2.columns)) if idx != 8]])\n",
    "print('RMSE = ', numpy.sqrt(mean_squared_error(test_set2['median_house_value'], prediction4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPzgfVbaH-T2"
   },
   "source": [
    "It seems that the engineered dataset degrades the performance of Random forest regression! This is very unlikely since Random forest regression is more powerful than the linear regression. Is it possible to tune the parameter of a Machine Learning algorithm to achieve better accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfjYU7CRyAP1"
   },
   "source": [
    "## Recap\n",
    "\n",
    "We have just demonstrated how to carry out a Machine Learning project on a given dataset. Specifically, a multivariate regression task in a supervised model-based batch learning framework. We have shown that a dataset needs to be properly inspected and some data cleaning techniques performed before applying any Machine Learning algorithm. Significant improvement can be obtained by not only changing the Machine Learning algorithm but combining it with feature engineering. There are a number of things that we have not covered here, but you can learn them by trying our exercises below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apRVdL1UyAP2"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. What is the difference between RMSE and MAE? What can be implied if RMSE is significantly higher than MAE? Is it true for this house prediction problem? (You can try compute the MAE in the cell below.)\n",
    "\n",
    "_Hint_: Since both RMSE and MAE is a measure to quantify error in regression task, we need to apply them in different type of numerical predictions that our Machine Learning model can make in order to see their difference. Try the following.\n",
    "- Create a vector of 1000 or more numerical values. (These can be the same value or different values.) We will treat this vector's as true target values that our Machine Learning model needs to predict as accurate as possible.\n",
    "- Generate a vector of the same size where this new vector's values has a normal (Gaussian) distribution with zero mean and a unit variance. (You can use numpy.random.normal().) Visualise the distribution of these values with a histogram. We will treat them as errors that our Machine Learning model make.\n",
    "- Duplicate the true target values and apply this Guassian errors to them simply by pairwise summing.\n",
    "- Compute RMSE and MAE from the noisy prediction and the true target values.\n",
    "- Repeat the same process by trying different means and/or variances of the Gaussian noise and then compute RMSE & MAE. You can also try a uniformly random noise as well. What did you see when you have errors with a non-zero mean and/or a large variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "pRqJyvDXyAP5",
    "outputId": "04f04225-6f45-47d3-830a-5c3e3d72b12b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 0 : 0.9666750800549541\n",
      "MAE 0 : 0.7578647301312328\n",
      "Difference : 0.20881034992372127\n",
      "\n",
      "RMSE 1 : 99.99341211771801\n",
      "MAE 1 : 99.98840515801382\n",
      "Difference : 0.00500695970418974\n",
      "\n",
      "RMSE 2 : 101.23417709027952\n",
      "MAE 2 : 80.5874129121151\n",
      "Difference : 20.646764178164418\n",
      "\n",
      "RMSE 3 : 144.47236908969282\n",
      "MAE 3 : 121.33247104143985\n",
      "Difference : 23.139898048252974\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution 1: \n",
    "# We can see from figures below that, in general, MAE is always no larger than \n",
    "# RMSE. Moreover, the higher the variance of error, the higher value a \n",
    "# difference between MAE and RMSE is. The source of high variance can come from\n",
    "# either (a) the target values are corrupted with noise, or (b) our model fails\n",
    "# to capture a detail in prediction's trend with respect to some attributes.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# function to compute RMSE\n",
    "def rmse(target, prediction):\n",
    "    return np.sqrt(np.mean(np.square(target - prediction)))\n",
    "\n",
    "# function to compute MAE\n",
    "def mae(target, prediction):\n",
    "    return np.mean(np.absolute(target - prediction))\n",
    "\n",
    "# generate numbers running from 0 to 1, counting up in units of 1/1000\n",
    "target = np.linspace(0, 1, num=1000)\n",
    "\n",
    "# generate prediction error with different means and different variances\n",
    "errors = [np.random.normal(loc=0.0, scale=1.0, size=1000), \n",
    "        np.random.normal(loc=100.0, scale=1.0, size=1000), \n",
    "        np.random.normal(loc=0.0, scale=100.0, size=1000), \n",
    "        np.random.normal(loc=100.0, scale=100.0, size=1000)]\n",
    "\n",
    "# indices for plotting histograms of error\n",
    "idx = [221, 222, 223, 224]\n",
    "\n",
    "\n",
    "#prediction = []\n",
    "for i in range(len(idx)):\n",
    "  \n",
    "    # Plotting histogram of errors\n",
    "    plt.subplot(idx[i])\n",
    "    plt.hist(errors[i])\n",
    "    plt.title(\"Error \" + str(i))\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    # Compute predictions, RMSE, MAE, and their difference\n",
    "    prediction = target + errors[i]\n",
    "    rmse_val = rmse(target, prediction)\n",
    "    mae_val = mae(target, prediction)\n",
    "    print(\"RMSE\", i, \":\", rmse_val)\n",
    "    print(\"MAE\", i, \":\", mae_val)\n",
    "    print(\"Difference :\", rmse_val - mae_val)\n",
    "    print()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.001001  , 0.002002  , 0.003003  , 0.004004  ,\n",
       "       0.00500501, 0.00600601, 0.00700701, 0.00800801, 0.00900901,\n",
       "       0.01001001, 0.01101101, 0.01201201, 0.01301301, 0.01401401,\n",
       "       0.01501502, 0.01601602, 0.01701702, 0.01801802, 0.01901902,\n",
       "       0.02002002, 0.02102102, 0.02202202, 0.02302302, 0.02402402,\n",
       "       0.02502503, 0.02602603, 0.02702703, 0.02802803, 0.02902903,\n",
       "       0.03003003, 0.03103103, 0.03203203, 0.03303303, 0.03403403,\n",
       "       0.03503504, 0.03603604, 0.03703704, 0.03803804, 0.03903904,\n",
       "       0.04004004, 0.04104104, 0.04204204, 0.04304304, 0.04404404,\n",
       "       0.04504505, 0.04604605, 0.04704705, 0.04804805, 0.04904905,\n",
       "       0.05005005, 0.05105105, 0.05205205, 0.05305305, 0.05405405,\n",
       "       0.05505506, 0.05605606, 0.05705706, 0.05805806, 0.05905906,\n",
       "       0.06006006, 0.06106106, 0.06206206, 0.06306306, 0.06406406,\n",
       "       0.06506507, 0.06606607, 0.06706707, 0.06806807, 0.06906907,\n",
       "       0.07007007, 0.07107107, 0.07207207, 0.07307307, 0.07407407,\n",
       "       0.07507508, 0.07607608, 0.07707708, 0.07807808, 0.07907908,\n",
       "       0.08008008, 0.08108108, 0.08208208, 0.08308308, 0.08408408,\n",
       "       0.08508509, 0.08608609, 0.08708709, 0.08808809, 0.08908909,\n",
       "       0.09009009, 0.09109109, 0.09209209, 0.09309309, 0.09409409,\n",
       "       0.0950951 , 0.0960961 , 0.0970971 , 0.0980981 , 0.0990991 ,\n",
       "       0.1001001 , 0.1011011 , 0.1021021 , 0.1031031 , 0.1041041 ,\n",
       "       0.10510511, 0.10610611, 0.10710711, 0.10810811, 0.10910911,\n",
       "       0.11011011, 0.11111111, 0.11211211, 0.11311311, 0.11411411,\n",
       "       0.11511512, 0.11611612, 0.11711712, 0.11811812, 0.11911912,\n",
       "       0.12012012, 0.12112112, 0.12212212, 0.12312312, 0.12412412,\n",
       "       0.12512513, 0.12612613, 0.12712713, 0.12812813, 0.12912913,\n",
       "       0.13013013, 0.13113113, 0.13213213, 0.13313313, 0.13413413,\n",
       "       0.13513514, 0.13613614, 0.13713714, 0.13813814, 0.13913914,\n",
       "       0.14014014, 0.14114114, 0.14214214, 0.14314314, 0.14414414,\n",
       "       0.14514515, 0.14614615, 0.14714715, 0.14814815, 0.14914915,\n",
       "       0.15015015, 0.15115115, 0.15215215, 0.15315315, 0.15415415,\n",
       "       0.15515516, 0.15615616, 0.15715716, 0.15815816, 0.15915916,\n",
       "       0.16016016, 0.16116116, 0.16216216, 0.16316316, 0.16416416,\n",
       "       0.16516517, 0.16616617, 0.16716717, 0.16816817, 0.16916917,\n",
       "       0.17017017, 0.17117117, 0.17217217, 0.17317317, 0.17417417,\n",
       "       0.17517518, 0.17617618, 0.17717718, 0.17817818, 0.17917918,\n",
       "       0.18018018, 0.18118118, 0.18218218, 0.18318318, 0.18418418,\n",
       "       0.18518519, 0.18618619, 0.18718719, 0.18818819, 0.18918919,\n",
       "       0.19019019, 0.19119119, 0.19219219, 0.19319319, 0.19419419,\n",
       "       0.1951952 , 0.1961962 , 0.1971972 , 0.1981982 , 0.1991992 ,\n",
       "       0.2002002 , 0.2012012 , 0.2022022 , 0.2032032 , 0.2042042 ,\n",
       "       0.20520521, 0.20620621, 0.20720721, 0.20820821, 0.20920921,\n",
       "       0.21021021, 0.21121121, 0.21221221, 0.21321321, 0.21421421,\n",
       "       0.21521522, 0.21621622, 0.21721722, 0.21821822, 0.21921922,\n",
       "       0.22022022, 0.22122122, 0.22222222, 0.22322322, 0.22422422,\n",
       "       0.22522523, 0.22622623, 0.22722723, 0.22822823, 0.22922923,\n",
       "       0.23023023, 0.23123123, 0.23223223, 0.23323323, 0.23423423,\n",
       "       0.23523524, 0.23623624, 0.23723724, 0.23823824, 0.23923924,\n",
       "       0.24024024, 0.24124124, 0.24224224, 0.24324324, 0.24424424,\n",
       "       0.24524525, 0.24624625, 0.24724725, 0.24824825, 0.24924925,\n",
       "       0.25025025, 0.25125125, 0.25225225, 0.25325325, 0.25425425,\n",
       "       0.25525526, 0.25625626, 0.25725726, 0.25825826, 0.25925926,\n",
       "       0.26026026, 0.26126126, 0.26226226, 0.26326326, 0.26426426,\n",
       "       0.26526527, 0.26626627, 0.26726727, 0.26826827, 0.26926927,\n",
       "       0.27027027, 0.27127127, 0.27227227, 0.27327327, 0.27427427,\n",
       "       0.27527528, 0.27627628, 0.27727728, 0.27827828, 0.27927928,\n",
       "       0.28028028, 0.28128128, 0.28228228, 0.28328328, 0.28428428,\n",
       "       0.28528529, 0.28628629, 0.28728729, 0.28828829, 0.28928929,\n",
       "       0.29029029, 0.29129129, 0.29229229, 0.29329329, 0.29429429,\n",
       "       0.2952953 , 0.2962963 , 0.2972973 , 0.2982983 , 0.2992993 ,\n",
       "       0.3003003 , 0.3013013 , 0.3023023 , 0.3033033 , 0.3043043 ,\n",
       "       0.30530531, 0.30630631, 0.30730731, 0.30830831, 0.30930931,\n",
       "       0.31031031, 0.31131131, 0.31231231, 0.31331331, 0.31431431,\n",
       "       0.31531532, 0.31631632, 0.31731732, 0.31831832, 0.31931932,\n",
       "       0.32032032, 0.32132132, 0.32232232, 0.32332332, 0.32432432,\n",
       "       0.32532533, 0.32632633, 0.32732733, 0.32832833, 0.32932933,\n",
       "       0.33033033, 0.33133133, 0.33233233, 0.33333333, 0.33433433,\n",
       "       0.33533534, 0.33633634, 0.33733734, 0.33833834, 0.33933934,\n",
       "       0.34034034, 0.34134134, 0.34234234, 0.34334334, 0.34434434,\n",
       "       0.34534535, 0.34634635, 0.34734735, 0.34834835, 0.34934935,\n",
       "       0.35035035, 0.35135135, 0.35235235, 0.35335335, 0.35435435,\n",
       "       0.35535536, 0.35635636, 0.35735736, 0.35835836, 0.35935936,\n",
       "       0.36036036, 0.36136136, 0.36236236, 0.36336336, 0.36436436,\n",
       "       0.36536537, 0.36636637, 0.36736737, 0.36836837, 0.36936937,\n",
       "       0.37037037, 0.37137137, 0.37237237, 0.37337337, 0.37437437,\n",
       "       0.37537538, 0.37637638, 0.37737738, 0.37837838, 0.37937938,\n",
       "       0.38038038, 0.38138138, 0.38238238, 0.38338338, 0.38438438,\n",
       "       0.38538539, 0.38638639, 0.38738739, 0.38838839, 0.38938939,\n",
       "       0.39039039, 0.39139139, 0.39239239, 0.39339339, 0.39439439,\n",
       "       0.3953954 , 0.3963964 , 0.3973974 , 0.3983984 , 0.3993994 ,\n",
       "       0.4004004 , 0.4014014 , 0.4024024 , 0.4034034 , 0.4044044 ,\n",
       "       0.40540541, 0.40640641, 0.40740741, 0.40840841, 0.40940941,\n",
       "       0.41041041, 0.41141141, 0.41241241, 0.41341341, 0.41441441,\n",
       "       0.41541542, 0.41641642, 0.41741742, 0.41841842, 0.41941942,\n",
       "       0.42042042, 0.42142142, 0.42242242, 0.42342342, 0.42442442,\n",
       "       0.42542543, 0.42642643, 0.42742743, 0.42842843, 0.42942943,\n",
       "       0.43043043, 0.43143143, 0.43243243, 0.43343343, 0.43443443,\n",
       "       0.43543544, 0.43643644, 0.43743744, 0.43843844, 0.43943944,\n",
       "       0.44044044, 0.44144144, 0.44244244, 0.44344344, 0.44444444,\n",
       "       0.44544545, 0.44644645, 0.44744745, 0.44844845, 0.44944945,\n",
       "       0.45045045, 0.45145145, 0.45245245, 0.45345345, 0.45445445,\n",
       "       0.45545546, 0.45645646, 0.45745746, 0.45845846, 0.45945946,\n",
       "       0.46046046, 0.46146146, 0.46246246, 0.46346346, 0.46446446,\n",
       "       0.46546547, 0.46646647, 0.46746747, 0.46846847, 0.46946947,\n",
       "       0.47047047, 0.47147147, 0.47247247, 0.47347347, 0.47447447,\n",
       "       0.47547548, 0.47647648, 0.47747748, 0.47847848, 0.47947948,\n",
       "       0.48048048, 0.48148148, 0.48248248, 0.48348348, 0.48448448,\n",
       "       0.48548549, 0.48648649, 0.48748749, 0.48848849, 0.48948949,\n",
       "       0.49049049, 0.49149149, 0.49249249, 0.49349349, 0.49449449,\n",
       "       0.4954955 , 0.4964965 , 0.4974975 , 0.4984985 , 0.4994995 ,\n",
       "       0.5005005 , 0.5015015 , 0.5025025 , 0.5035035 , 0.5045045 ,\n",
       "       0.50550551, 0.50650651, 0.50750751, 0.50850851, 0.50950951,\n",
       "       0.51051051, 0.51151151, 0.51251251, 0.51351351, 0.51451451,\n",
       "       0.51551552, 0.51651652, 0.51751752, 0.51851852, 0.51951952,\n",
       "       0.52052052, 0.52152152, 0.52252252, 0.52352352, 0.52452452,\n",
       "       0.52552553, 0.52652653, 0.52752753, 0.52852853, 0.52952953,\n",
       "       0.53053053, 0.53153153, 0.53253253, 0.53353353, 0.53453453,\n",
       "       0.53553554, 0.53653654, 0.53753754, 0.53853854, 0.53953954,\n",
       "       0.54054054, 0.54154154, 0.54254254, 0.54354354, 0.54454454,\n",
       "       0.54554555, 0.54654655, 0.54754755, 0.54854855, 0.54954955,\n",
       "       0.55055055, 0.55155155, 0.55255255, 0.55355355, 0.55455455,\n",
       "       0.55555556, 0.55655656, 0.55755756, 0.55855856, 0.55955956,\n",
       "       0.56056056, 0.56156156, 0.56256256, 0.56356356, 0.56456456,\n",
       "       0.56556557, 0.56656657, 0.56756757, 0.56856857, 0.56956957,\n",
       "       0.57057057, 0.57157157, 0.57257257, 0.57357357, 0.57457457,\n",
       "       0.57557558, 0.57657658, 0.57757758, 0.57857858, 0.57957958,\n",
       "       0.58058058, 0.58158158, 0.58258258, 0.58358358, 0.58458458,\n",
       "       0.58558559, 0.58658659, 0.58758759, 0.58858859, 0.58958959,\n",
       "       0.59059059, 0.59159159, 0.59259259, 0.59359359, 0.59459459,\n",
       "       0.5955956 , 0.5965966 , 0.5975976 , 0.5985986 , 0.5995996 ,\n",
       "       0.6006006 , 0.6016016 , 0.6026026 , 0.6036036 , 0.6046046 ,\n",
       "       0.60560561, 0.60660661, 0.60760761, 0.60860861, 0.60960961,\n",
       "       0.61061061, 0.61161161, 0.61261261, 0.61361361, 0.61461461,\n",
       "       0.61561562, 0.61661662, 0.61761762, 0.61861862, 0.61961962,\n",
       "       0.62062062, 0.62162162, 0.62262262, 0.62362362, 0.62462462,\n",
       "       0.62562563, 0.62662663, 0.62762763, 0.62862863, 0.62962963,\n",
       "       0.63063063, 0.63163163, 0.63263263, 0.63363363, 0.63463463,\n",
       "       0.63563564, 0.63663664, 0.63763764, 0.63863864, 0.63963964,\n",
       "       0.64064064, 0.64164164, 0.64264264, 0.64364364, 0.64464464,\n",
       "       0.64564565, 0.64664665, 0.64764765, 0.64864865, 0.64964965,\n",
       "       0.65065065, 0.65165165, 0.65265265, 0.65365365, 0.65465465,\n",
       "       0.65565566, 0.65665666, 0.65765766, 0.65865866, 0.65965966,\n",
       "       0.66066066, 0.66166166, 0.66266266, 0.66366366, 0.66466466,\n",
       "       0.66566567, 0.66666667, 0.66766767, 0.66866867, 0.66966967,\n",
       "       0.67067067, 0.67167167, 0.67267267, 0.67367367, 0.67467467,\n",
       "       0.67567568, 0.67667668, 0.67767768, 0.67867868, 0.67967968,\n",
       "       0.68068068, 0.68168168, 0.68268268, 0.68368368, 0.68468468,\n",
       "       0.68568569, 0.68668669, 0.68768769, 0.68868869, 0.68968969,\n",
       "       0.69069069, 0.69169169, 0.69269269, 0.69369369, 0.69469469,\n",
       "       0.6956957 , 0.6966967 , 0.6976977 , 0.6986987 , 0.6996997 ,\n",
       "       0.7007007 , 0.7017017 , 0.7027027 , 0.7037037 , 0.7047047 ,\n",
       "       0.70570571, 0.70670671, 0.70770771, 0.70870871, 0.70970971,\n",
       "       0.71071071, 0.71171171, 0.71271271, 0.71371371, 0.71471471,\n",
       "       0.71571572, 0.71671672, 0.71771772, 0.71871872, 0.71971972,\n",
       "       0.72072072, 0.72172172, 0.72272272, 0.72372372, 0.72472472,\n",
       "       0.72572573, 0.72672673, 0.72772773, 0.72872873, 0.72972973,\n",
       "       0.73073073, 0.73173173, 0.73273273, 0.73373373, 0.73473473,\n",
       "       0.73573574, 0.73673674, 0.73773774, 0.73873874, 0.73973974,\n",
       "       0.74074074, 0.74174174, 0.74274274, 0.74374374, 0.74474474,\n",
       "       0.74574575, 0.74674675, 0.74774775, 0.74874875, 0.74974975,\n",
       "       0.75075075, 0.75175175, 0.75275275, 0.75375375, 0.75475475,\n",
       "       0.75575576, 0.75675676, 0.75775776, 0.75875876, 0.75975976,\n",
       "       0.76076076, 0.76176176, 0.76276276, 0.76376376, 0.76476476,\n",
       "       0.76576577, 0.76676677, 0.76776777, 0.76876877, 0.76976977,\n",
       "       0.77077077, 0.77177177, 0.77277277, 0.77377377, 0.77477477,\n",
       "       0.77577578, 0.77677678, 0.77777778, 0.77877878, 0.77977978,\n",
       "       0.78078078, 0.78178178, 0.78278278, 0.78378378, 0.78478478,\n",
       "       0.78578579, 0.78678679, 0.78778779, 0.78878879, 0.78978979,\n",
       "       0.79079079, 0.79179179, 0.79279279, 0.79379379, 0.79479479,\n",
       "       0.7957958 , 0.7967968 , 0.7977978 , 0.7987988 , 0.7997998 ,\n",
       "       0.8008008 , 0.8018018 , 0.8028028 , 0.8038038 , 0.8048048 ,\n",
       "       0.80580581, 0.80680681, 0.80780781, 0.80880881, 0.80980981,\n",
       "       0.81081081, 0.81181181, 0.81281281, 0.81381381, 0.81481481,\n",
       "       0.81581582, 0.81681682, 0.81781782, 0.81881882, 0.81981982,\n",
       "       0.82082082, 0.82182182, 0.82282282, 0.82382382, 0.82482482,\n",
       "       0.82582583, 0.82682683, 0.82782783, 0.82882883, 0.82982983,\n",
       "       0.83083083, 0.83183183, 0.83283283, 0.83383383, 0.83483483,\n",
       "       0.83583584, 0.83683684, 0.83783784, 0.83883884, 0.83983984,\n",
       "       0.84084084, 0.84184184, 0.84284284, 0.84384384, 0.84484484,\n",
       "       0.84584585, 0.84684685, 0.84784785, 0.84884885, 0.84984985,\n",
       "       0.85085085, 0.85185185, 0.85285285, 0.85385385, 0.85485485,\n",
       "       0.85585586, 0.85685686, 0.85785786, 0.85885886, 0.85985986,\n",
       "       0.86086086, 0.86186186, 0.86286286, 0.86386386, 0.86486486,\n",
       "       0.86586587, 0.86686687, 0.86786787, 0.86886887, 0.86986987,\n",
       "       0.87087087, 0.87187187, 0.87287287, 0.87387387, 0.87487487,\n",
       "       0.87587588, 0.87687688, 0.87787788, 0.87887888, 0.87987988,\n",
       "       0.88088088, 0.88188188, 0.88288288, 0.88388388, 0.88488488,\n",
       "       0.88588589, 0.88688689, 0.88788789, 0.88888889, 0.88988989,\n",
       "       0.89089089, 0.89189189, 0.89289289, 0.89389389, 0.89489489,\n",
       "       0.8958959 , 0.8968969 , 0.8978979 , 0.8988989 , 0.8998999 ,\n",
       "       0.9009009 , 0.9019019 , 0.9029029 , 0.9039039 , 0.9049049 ,\n",
       "       0.90590591, 0.90690691, 0.90790791, 0.90890891, 0.90990991,\n",
       "       0.91091091, 0.91191191, 0.91291291, 0.91391391, 0.91491491,\n",
       "       0.91591592, 0.91691692, 0.91791792, 0.91891892, 0.91991992,\n",
       "       0.92092092, 0.92192192, 0.92292292, 0.92392392, 0.92492492,\n",
       "       0.92592593, 0.92692693, 0.92792793, 0.92892893, 0.92992993,\n",
       "       0.93093093, 0.93193193, 0.93293293, 0.93393393, 0.93493493,\n",
       "       0.93593594, 0.93693694, 0.93793794, 0.93893894, 0.93993994,\n",
       "       0.94094094, 0.94194194, 0.94294294, 0.94394394, 0.94494494,\n",
       "       0.94594595, 0.94694695, 0.94794795, 0.94894895, 0.94994995,\n",
       "       0.95095095, 0.95195195, 0.95295295, 0.95395395, 0.95495495,\n",
       "       0.95595596, 0.95695696, 0.95795796, 0.95895896, 0.95995996,\n",
       "       0.96096096, 0.96196196, 0.96296296, 0.96396396, 0.96496496,\n",
       "       0.96596597, 0.96696697, 0.96796797, 0.96896897, 0.96996997,\n",
       "       0.97097097, 0.97197197, 0.97297297, 0.97397397, 0.97497497,\n",
       "       0.97597598, 0.97697698, 0.97797798, 0.97897898, 0.97997998,\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GB6aU4AWyAP7"
   },
   "source": [
    "# 2. Instead of dropping some datapoints that have missing values, we can try and fill them with a median of that attributes. Will the performance measure increase?\n",
    "\n",
    "_Hint_: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\n",
    "\n",
    "_Extra hint_: Do you think that a median is always an appropriate value to use? What's about a mean or a mode? Which one would be appropriate for normally (Gaussian) distributed attribute? Which one would be appropriate for log-normal distribution or long-tailed distribution? (Search for a figure of these distributions on Google. It's useful for you to know at least what the shape of these distributions are and how frequent we can expect for each value in the distribution's range.)\n",
    "\n",
    "_Extra extra hint_: Will our Machine Learning model always benefit from having these datapoints with filled attributes? How? What if the proportion of these datapoints is more than 50%? Also, what if the attribute is one of the main factors that affects the target value (e.g. number of rooms and a house price)? Will our Machine Learning model still be able to use and learn from such filled attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fy008uUlyAP8"
   },
   "outputs": [],
   "source": [
    "Solution 2: \n",
    "Given that the attribute with missing values is important to make a prediction, \n",
    "filling the attribute is better than dropping the attribute, and consequently \n",
    "the performance measure will generally improve. Note that dropping datapoints \n",
    "results in less data for Machine Learning algorithm to train.\n",
    "\n",
    "Additionally, other values such as a mean or a mode may even give a better \n",
    "performance than the median. A number of techniques can also be used to get \n",
    "values to fill. For further detail, have a quick look at this paper\n",
    "\n",
    "Kotsiantis, S.B., Kanellopoulos, D. and Pintelas, P.E., 2006. Data preprocessing\n",
    "for supervised leaning. International Journal of Computer Science, 1(2), \n",
    "pp.111-117."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quJ6hGfpyAP-"
   },
   "source": [
    "3. What is the consequence of ignoring the datapoints with a capped value in our dataset?\n",
    "\n",
    "_Hint_: By discarding those datapoints, you won't have them to let the Machine Learning model learn. When you roll out your Machine Learning project, what will then happen with a prediciton for inputs with those capped values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1KUH4z_MyAP_"
   },
   "outputs": [],
   "source": [
    "Solution 3: \n",
    "Our prediction that bases on data with capped value is not reliable. Such a \n",
    "prediction is an extrapolation which must be treated carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bo8JgGVKyAQC"
   },
   "source": [
    "4. Instead of encoding a categorical attribute into a number of binary attributes, will our performance measure increase if we encode it into one attribute with each value representing one category? What could be a reason for such improvement?\n",
    "\n",
    "_Hint_: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.factorize.html\n",
    "\n",
    "_Extra hint_: What is the difference between ordinal data and categorical data? Can categorical data always be ordered according to some metrics? What will then be a consequence of coding categorical data into a numerical value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "docA6JuCyAQD"
   },
   "outputs": [],
   "source": [
    "Solution 4: \n",
    "Typically, such encoding is not appropriate for categorical attributes and \n",
    "consequently should not improve the performance of Machine Learning models. \n",
    "In particular, encoding such attributes into a numerical attribute will also \n",
    "embed the quantitative difference among each pair of categories which is usually\n",
    "not true. Even if there is such a difference, the encoding has to preserve the \n",
    "original quantitative value as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbTAl-rNyAQF"
   },
   "source": [
    "5. In many dataset including ours, different attributes have different ranges of value. Whilst our Machine Learning algorithm can cope with this issue to certain degree, it is widely known that either standardisation or normalisation should be applied. Try them separately on some attributes in our dataset, and observe any change in the performance measure.\n",
    "\n",
    "_Hint_: http://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTiw2C3RyAQF"
   },
   "outputs": [],
   "source": [
    "# Solution 5\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaled_train_features = preprocessing.scale(train_features)\n",
    "# Other types of preprocessing can be looked in the hint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAZSrTvdyAQG"
   },
   "source": [
    "6. We had randomly partitioned the dataset into the train set and the test set. It might be the case that we were lucky and randomly chose a test set that yielded a very low RMSE. To properly evaluate performance of our Machine Learning algorithm, you should try using all datapoints in your dataset as a test set and make sure that the RMSE is significatly low. This is called 'Cross-Validation.' Try it with our dataset and one Machine Learning model.\n",
    "\n",
    "_Hint_: http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhvTCQh1yAQI"
   },
   "outputs": [],
   "source": [
    "# Solution 6\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "reg = RandomForestRegressor()\n",
    "scores = cross_val_score(reg, features, target, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gN6Y-ZY3yAQJ"
   },
   "source": [
    "7. Many Machine Learning algorithms including the Random Forest Regression have a number of parameters to tune. Try tuning our Random Forest Regressor so that it achieves the lowest RMSE.\n",
    "\n",
    "_Hint_: Instead of manually tuning these parameters, you can try a search such as a grid search (http://scikit-learn.org/stable/modules/grid_search.html) to find a good combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTY0_46PyAQL"
   },
   "outputs": [],
   "source": [
    "# Solution 7\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = [{'n_estimators': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19], 'max_depth': [10, 20, 30, 40, 50]}]\n",
    "\n",
    "cv = GridSearchCV(RandomForestRegressor(), param_grid)\n",
    "cv.fit(train_features, train_label)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)\n",
    "\n",
    "# Detailed example of using GridSearchCV is accessible from \n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Uo1eaOzyAQO"
   },
   "source": [
    "8. In practice, after training your first prototype, you are likely to acquire new datapoints or update your existing datapoints. How can you utilise them to improve your Machine Learning algorithm?\n",
    "\n",
    "_Hint_: Do those new datapoints significantly different to the previous one? If they do, you need to make your Machine Learning algorithm adapt to these new datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-y4DJdPyAQO"
   },
   "outputs": [],
   "source": [
    "Solution 8: \n",
    "Given that a training of the Machine Learning model is not computationally \n",
    "expensive and not time-consuming, the new data can be incorporated into the \n",
    "dataset and the model can be trained freshly from the beginning. \n",
    "\n",
    "Contrarily, an online learning is required if the model is not easy to train. \n",
    "The key idea of an online learning is that an online training method of the \n",
    "Machine Learning model will learn from one datapoint or a small batch of data \n",
    "at each training epoch. Such a training method is different to the offline \n",
    "method and generally guarantees a well trained model given some conditions. \n",
    "\n",
    "For further information, have a quick look on the following sources:\n",
    "\n",
    "Wikipedia contributors, 'Online machine learning', Wikipedia, The Free \n",
    "Encyclopedia, 27 September 2018, 05:30 UTC, <https://en.wikipedia.org/w/index.php?title=Online_machine_learning&oldid=861407129> \n",
    "\n",
    "Shalev-Shwartz, S., 2012. Online learning and online convex optimization. \n",
    "Foundations and Trends® in Machine Learning, 4(2), pp.107-194. <http://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab1-solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
